{"Title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning", "Doi": "10.1109/TPAMI.2022.3148210", "Authors": ["m. stefanini", "m. cornia", "l. baraldi", "s. cascianelli", "g. fiameni", "r. cucchiara"], "Key Words": ["image captioning", "vision-and-language", "deep learning", "survey"], "Abstract": "connecting vision and language plays an essential role in generative intelligence. for this reason large research efforts have been devoted to image captioning i.e. describing images with syntactically and semantically meaningful sentences. starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. during these years both components have evolved considerably through the exploitation of object regions attributes the introduction of multi modal connections fully attentive approaches and bert like early fusion strategies. however regardless of the impressive results research in image captioning has not reached a conclusive answer yet. this work aims at providing a comprehensive overview of image captioning approaches from visual encoding and text generation to training strategies datasets and evaluation metrics. in this respect we quantitatively compare many relevant state of the art approaches to identify the most impactful technical innovations in architectures and training strategies. moreover many variants of the problem and its open challenges are discussed. the final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where computer vision and natural language processing can find an optimal synergy.", "Pub Date": "2022-12-05"}