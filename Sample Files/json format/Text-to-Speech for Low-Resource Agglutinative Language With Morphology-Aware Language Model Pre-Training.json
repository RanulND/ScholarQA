{"Title": "Text-to-Speech for Low-Resource Agglutinative Language With Morphology-Aware Language Model Pre-Training", "Doi": "10.1109/TASLP.2023.3348762", "Authors": ["r. liu", "y. hu", "h. zuo", "z. luo", "l. wang", "g. gao"], "Key Words": ["text-to-speech (tts)", "agglutinative", "morphology", "language modeling", "pre-training"], "Abstract": "text to speech  tts  aims to convert the input text to a human like voice. with the development of deep learning encoder decoder based tts models perform superior performance in terms of naturalness in mainstream languages such as chinese english etc. note that the linguistic information learning capability of the text encoder is the key. however for tts of low resource agglutinative languages the scale of the $< $text speech$>$ paired data is limited. therefore how to extract rich linguistic information from small scale text data to enhance the naturalness of the synthesized speech is an urgent issue that needs to be addressed. in this paper we first collect a large unsupervised text data for bert like language model pre training and then adopt the trained language model to extract deep linguistic information for the input text of the tts model to improve the naturalness of the final synthesized speech. it should be emphasized that in order to fully exploit the prosody related linguistic information in agglutinative languages we incorporated morphological information into the language model training and constructed a morphology aware masking based bert model  mam bert . experimental results based on various advanced tts models validate the effectiveness of our approach. further comparison of the various data scales also validates the effectiveness of our approach in low resource scenarios.", "Pub Date": "2024-01-19"}