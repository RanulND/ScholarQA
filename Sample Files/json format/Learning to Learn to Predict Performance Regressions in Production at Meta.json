{"Title": "Learning to Learn to Predict Performance Regressions in Production at Meta", "Doi": "10.1109/AST58925.2023.00010", "Authors": ["m. beller", "h. li", "v. nair", "v. murali", "i. ahmad", "j. cito", "d. carlson", "a. aye", "w. dyer"], "Key Words": ["machine learning", "performance regressions", "prediction", "continuous integration"], "Abstract": "catching and attributing code change induced performance regressions in production is hard  predicting them beforehand even harder. a primer on automatically learning to predict performance regressions in software this article gives an account of the experiences we gained when researching and deploying an ml based regression prediction pipeline at meta.in this paper we report on a comparative study with four ml models of increasing complexity from  1  code opaque over  2  bag of words  3  off the shelve transformer based to  4  a bespoke transformer based model coined superperforator. our investigation shows the inherent difficulty of the performance prediction problem which is characterized by a large imbalance of benign onto regressing changes. our results also call into question the general applicability of transformer based architectures for performance prediction  an off the shelve codebert based approach had surprisingly poor performance  even the highly customized superperforator architecture achieved offline results that were on par with simpler bag of words models  it only started to significantly outperform it for down stream use cases in an online setting. to gain further insight into superperforator we explored it via a series of experiments computing counterfactual explanations. these highlight which parts of a code change the model deems important thereby validating it.the ability of superperforator to transfer to an application with few learning examples afforded an opportunity to deploy it in practice at meta  it can act as a pre filter to sort out changes that are unlikely to introduce a regression truncating the space of changes to search a regression in by up to 43% a 45x improvement over a random baseline.", "Pub Date": "2023-07-12"}