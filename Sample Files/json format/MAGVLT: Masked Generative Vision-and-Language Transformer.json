{"Title": "MAGVLT: Masked Generative Vision-and-Language Transformer", "Doi": "10.1109/CVPR52729.2023.02235", "Authors": ["s. kim", "d. jo", "d. lee", "j. kim"], "Key Words": ["multi-modal learning"], "Abstract": "while generative modeling on multimodal image text data has been actively developed with large scale paired datasets there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. in this paper we explore a unified generative vision and language  vl  model that can produce both images and text sequences. especially we propose a generative vl transformer based on the non autoregressive mask prediction named magvlt and compare it with an autoregressive generative vl transformer  argvlt . in comparison to argvlt the proposed magvlt enables bidirectional context encoding fast decoding by parallel token predictions in an iterative refinement and extended editing capabilities such as image and text infilling. for rigorous training of our magvlt with image text pairs from scratch we combine the image to text text to image and joint image and text mask prediction tasks. moreover we devise two additional tasks based on the step unrolled mask prediction and the selective prediction on the mixture of two image text pairs. experimental results on various downstream generation tasks of vl benchmarks show that our magvlt outperforms argvlt by a large margin even with significant inference speedup. particularly magvlt achieves competitive results on both zero shot image to text and text to image generation tasks from ms coco by one moderate sized model  fewer than 500m parameters  even without the use of monomodal data and networks.", "Pub Date": "2023-08-22"}