{"Title": "Pipeline Chain-of-Thought: A Prompt Method for Large Language Model Relation Extraction", "Doi": "10.1109/IALP61005.2023.10337264", "Authors": ["h. zhao", "h. yilahun", "a. hamdulla"], "Key Words": ["large language models", "relation extraction", "triple", "prompt fine-tuning", "pipeline chain-of- thought"], "Abstract": "the development of language models has been influencing approaches to relation extraction  re  problems. although large language models  large language model  have demonstrated breakthrough potential in certain aspects they are still in the exploratory stage for re tasks. currently the mainstream approach to improving the re performance of large language model is through prompt fine tuning but most methods require providing entity information in the prompt which effectively only allows the large language model to perform relationship classification tasks. we propose the pipeline chain of thought  pipeline cot  which breaks down the re task into steps and transforms it into reasoning tasks that have flat scaling curves thereby enabling the use of chain of thought  cot  to enhance model inference. in addition our method utilizes n shot samples to provide signals for the bayesian inference of the model by prompting the large language model to focus on specific concepts to generate answers. we evaluated pipeline cot on the chinese dataset duie2.0 and compared with baseline methods that require including entity information in the prompt our method still shows competitive performance.", "Pub Date": "2023-12-12"}