{"Title": "Real-Time Offloading for Dependent and Parallel Tasks in Cloud-Edge Environments Using Deep Reinforcement Learning", "Doi": "10.1109/TPDS.2023.3349177", "Authors": ["x. chen", "s. hu", "c. yu", "z. chen", "g. min"], "Key Words": ["cloud-edge computing", "deep reinforcement learning", "dependent and parallel tasks", "real-time offloading"], "Abstract": "as an effective technique to relieve the problem of resource constraints on mobile devices  mds  the computation offloading utilizes powerful cloud and edge resources to process the computation intensive tasks of mobile applications uploaded from mds. in cloud edge computing the resources  e.g. cloud and edge servers  that can be accessed by mobile applications may change dynamically. meanwhile the parallel tasks in mobile applications may lead to the huge solution space of offloading decisions. therefore it is challenging to determine proper offloading plans in response to such high dynamics and complexity in cloud edge environments. the existing studies often preset the priority of parallel tasks to simplify the solution space of offloading decisions and thus the proper offloading plans cannot be found in many cases. to address this challenge we propose a novel real time and dependency aware task offloading method with deep q networks  dodq  in cloud edge computing. in dodq mobile applications are first modeled as directed acyclic graphs  dags . next the deep q networks  dqn  is customized to train the decision making model of task offloading aiming to quickly complete the decision making process and generate new offloading plans when the environments change which considers the parallelism of tasks without presetting the task priority when scheduling tasks. simulation results show that the dodq can well adapt to different environments and efficiently make offloading decisions. moreover the dodq outperforms the state of art methods and quickly reaches the optimal near optimal performance.", "Pub Date": "2024-01-22"}