{"Title": "Rethinking Learning Rate Tuning in the Era of Large Language Models", "Doi": "10.1109/CogMI58952.2023.00025", "Authors": ["h. jin", "w. wei", "x. wang", "w. zhang", "y. wu"], "Key Words": ["learning rate", "hyperparameter tuning", "deep learning", "large language model"], "Abstract": "large language models  large language model  represent the recent success of deep learning in achieving remarkable human like predictive performance. it has become a mainstream strategy to leverage fine tuning to adapt large language model for various real world applications due to the prohibitive expenses associated with large language model training. the learning rate is one of the most important hyper parameters in large language model fine tuning with direct impacts on both fine tuning efficiency and fine tuned large language model quality. existing learning rate policies are primarily designed for training traditional deep neural networks  dnns  which may not work well for large language model fine tuning. we reassess the research challenges and opportunities of learning rate tuning in the coming era of large language models. this paper makes three original contributions. first we revisit existing learning rate policies to analyze the critical challenges of learning rate tuning in the era of large language model. second we present lrbench++ to benchmark learning rate policies and facilitate learning rate tuning for both traditional dnns and large language model. third our experimental evaluation with lrbench++ demonstrates the key differences between large language model fine tuning and traditional dnn training and validates our analysis.", "Pub Date": "2024-02-19"}