{"Title": "MEDIA: An Incremental DNN Based Computation Offloading for Collaborative Cloud-Edge Computing", "Doi": "10.1109/TNSE.2023.3335345", "Authors": ["l. zhao", "y. han", "a. hawbani", "s. wan", "z. guo", "m. guizani"], "Key Words": ["mobile cloud computing", "mobile edge computing", "computation offloading", "deep learning", "incremental learning"], "Abstract": "mobilecloud computing  mcc  provides computing storage and other fruitful services to end users. offloading such tasks to cloud servers can help to fulfill the demands of extensive computing resources but may also lead to network congestion and high latency. mobile edge computing  mec  places the computing nodes near the end users to enable low latency services whereas it cannot execute too many computing tasks due to limited computing resources. therefore mcc and mec are highly complementary. for computing offloading problems in a collaborative cloud edge environment traditional optimization algorithms require multiple iterations to obtain results which leads to excessive time spent to obtain offloading strategies. deep neural network  dnn  based offloading algorithms can provide low latency offloading strategies but training data is difficult to be obtained and the cost of retraining is too high. therefore in this article we adopt an incremental training method to overcome the problem of insufficient training data and high retraining costs in dnn based offloading algorithms. an incremental dnn based computation offloading  media  algorithm is proposed to derive near optimal offloading strategies for collaborative cloud edge computing. the task information on the real scenarios is sent to the central cloud to generate training data and the powerful computing resources of the central cloud improve the efficiency of training model. the continuous incremental training can maintain a high accuracy of the dnn model and reduce the time for training the model. the evaluation results demonstrate that the proposed algorithm substantially reduces the cost for updating the model without loss of performance.", "Pub Date": "2024-02-22"}