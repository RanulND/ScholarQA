{"Title": "Scenario-Based AI Benchmark Evaluation of Distributed Cloud/Edge Computing Systems", "Doi": "10.1109/TC.2022.3176803", "Authors": ["t. hao", "k. hwang", "j. zhan", "y. li", "y. cao"], "Key Words": ["computer benchmarks", "cloud/edge computing", "machine learning", "and artificial intelligence"], "Abstract": "distributed cloud edge  dce  platform has become popular in recent years. this paper proposes a new ai benchmark suite for assessing the performance of dce platforms in machine learning  ml  and cognitive science applications. the benchmark suite is custom designed to satisfy scenario based performance requirements namely the model training time inference speed model accuracy job response time quality of service and system reliability. these metrics are substantiated by intensive experiments with real life ai workloads. our work is specially tailored for supporting massive ai multitasking across distributed resources in the networking environment. our benchmark experiments were conducted on an ai oriented airs cloud built at the chinese university of hong kong shenzhen. we have tested a large number of ml dl programs to narrow down the inclusion of ten representative ai kernel codes in the benchmark suite. our benchmark results reveal the advantages of using the dce systems cost effectively in smart cities healthcare community surveillance and transportation services. our technical contributions are in the airs cloud architecture benchmark design testing and distributed ai computing requirements. our work will benefit computer system designers and ai application developers on clouds edge and mobile devices that are supported by 5g mobile networks and aiot resources.", "Pub Date": "2023-02-09"}