{"Title": "The Impact of Adversarial Attacks on Interpretable Semantic Segmentation in Cyber\u201a\u00c4\u00ecPhysical Systems", "Authors": ["r. gipi\u2248\u00b0kis", "d. chiaro", "m. preziosi", "e. prezioso", "f. piccialli"], "Pub Date": "2023-12-12", "Abstract": "the widespread adoption of deep learning  dl  models raises concerns about their trustworthiness and reliability. adversarial attacks are cyber related attacks that target the dl network prediction by adding imperceptible perturbations to its input. their deployment against critical artificial intelligence based systems such as industrial cyber\u201a\u00e4\u00ecphysical systems  icpss  can result in substantial damage. research on their scope and limitations can provide information that would help with their detection and prevention. in this article the interconnection of adversarial attacks and interpretable semantic segmentation is investigated for potential applications in the icps in order to contribute to the safe use of future intelligent systems. we first explore gradient based interpretability extensions to semantic segmentation on two industry related cyber\u201a\u00e4\u00ecphysical system datasets. then two types of attacks on semantic segmentation networks are discussed. first we apply the dense adversary generation attack on different segmentation outputs and evaluate its influence on the corresponding saliency maps. we then introduce a way to visualize the similarity of attacked saliency maps to the original with respect to the targeted attack direction. finally we extend the application of adversarial attacks on saliency maps to semantic segmentation.", "Doi": "10.1109/JSYST.2023.3281079", "Key Words": ["adversarial attacks", "cyber\u201a\u00e4\u00ecphysical systems (cpss)", "explainable artificial intelligence (xai)", "industrial control", "interpretability", "semantic segmentation"]}