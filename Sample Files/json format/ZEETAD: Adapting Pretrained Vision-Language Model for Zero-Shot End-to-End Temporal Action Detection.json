{"Title": "ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection", "Doi": "10.1109/WACV57701.2024.00689", "Authors": ["t. phan", "k. vo", "d. le", "g. doretto", "d. adjeroh", "n. le"], "Key Words": ["algorithms", "video recognition and understanding", "algorithms", "vision + language and/or other modalities"], "Abstract": "temporal action detection  tad  involves the localization and classification of action instances within untrimmed videos. while standard tad follows fully supervised learning with closed set setting on large training data recent zero shot tad methods showcase the promising open set setting by leveraging large scale contrastive visual language  vil  pretrained models. however existing zero shot tad methods have limitations on how to properly construct the strong relationship between two interdependent tasks of localization and classification and adapt vil model to video understanding. in this work we present zee tad featuring two modules  dual localization and zero shot proposal classification. the former is a transformer based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. the latter one clip based module generates semantic embeddings from text and frame inputs for each temporal unit. additionally we enhance discriminative capability on unseen classes by minimally updating the frozen clip encoder with lightweight adapters. extensive experiments on thumos14 and activitynet 1.3 datasets demonstrate our approach\u201a\u00e4\u00f4s superior performance in zero shot tad and effective knowledge transfer from vil models to unseen action categories. code is available at https  //github.com uark aicv zeetad.", "Pub Date": "2024-04-09"}