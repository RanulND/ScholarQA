{"Title": "Combining the Predictions of Out-of-Domain Classifiers Using Etcetera Abduction", "Doi": "10.1109/CISS59072.2024.10480194", "Authors": ["a. s. gordon", "a. feng"], "Key Words": ["i.2.6.g machine learning", "i.2.3.d inference engines", "i.2.4 knowledge representation formalisms and methods"], "Abstract": "research in machine learning on domain adaptation has led to numerous methods for re purposing highperformance pre trained models for novel tasks e.g. via finetuning a model with out of domain training data. when model weights are unavailable or otherwise fixed there are fewer options available for exploiting its predictive power. in this paper we investigate whether the predictions of ensembles of fixed pre trained out of domain image classification models can be used to improve the performance of an in domain classifier or replace it outright with comparable performance. our approach involves computing the conditional probabilities from the confusion matrixes of out of domain predictions for in domain training samples then combining this information with prior probabilities and classification confidence using probability ordered logical abduction etcetera abduction to select the most likely label for an in domain test sample. we evaluate this approach using four image classification models in highly disparate domains. results indicate that this method may be well suited to applications where insufficient training data is available to train an accurate model on a novel task.", "Pub Date": "2024-04-02"}