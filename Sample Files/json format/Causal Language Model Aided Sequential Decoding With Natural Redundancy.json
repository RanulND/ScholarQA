{"Title": "Causal Language Model Aided Sequential Decoding With Natural Redundancy", "Doi": "10.1109/TCOMM.2023.3247733", "Authors": ["z. zhu", "h. yu", "c. shen", "j. du", "z. shen", "z. wang"], "Key Words": ["deep learning", "error correction", "sequential decoding", "causal language model", "natural redundancy"], "Abstract": "a high performance communication receiver desires a sufficient and accurate recognition of transmitted sources. in this paper we propose a sequential decoding algorithm for the robust reception of sources with natural redundancy  nr  over the awgn channel. to fully exploit the abundant nr in the exemplified english text sources a causal language modeling  clm  with a powerful transformer decoder neural network  nn  structure is adopted for modeling the joint probability distribution. for versatility of byte level tokenization the utf-8 encoding scheme is considered for the wikipedia corpus namely the english edition of the wiki 40b dataset. the tree search based m algorithm  ma  is integrated with clm denoted as clm ma algorithm to synthesize the a priori probability of information sequence and likelihood values of polluted symbols. both simulation experiments and hardware platform evaluations are conducted to analyze the performance of the clm ma algorithm. with an adequately large m value tremendous performance profit is achievable especially for high efficiency modulation levels and extremely noisy conditions. this mechanism of adopting the pre trained nn model merely desires self supervised learning architecture and eliminates the requirement of high cost and accurate labels which paves a new way for communication receiver design.", "Pub Date": "2023-05-17"}