{"Title": "Should Learning Analytics Models Include Sensitive Attributes? Explaining the Why", "Doi": "10.1109/TLT.2022.3226474", "Authors": ["o. b. deho", "s. joksimovic", "j. li", "c. zhan", "j. liu", "l. liu"], "Key Words": ["dropout prediction", "explainable ai", "fairness in education", "learning analytics (la)"], "Abstract": "many educational institutions are using predictive models to leverage actionable insights using student data and drive student success. a common task has been predicting students at risk of dropping out for the necessary interventions to be made. however issues of discrimination by these predictive models based on protected attributes of students have recently been raised. an important question that is constantly asked is  should the protected attributes be excluded from the learning analytics  la  models in order to ensure fairness? in this article we aimed at answering questions that if we exclude the protected attributes from the la models does the exclusion ensure fairness as it supposedly should? does the exclusion affect the performance of the la model? if so why? we found answers to these questions and went further to explain why. we built machine learning models and performed empirical evaluations using a three year dropout data for a particular program in a large australian university. we found that excluding or including the protected attributes had marginal effect on predictive performance and fairness. perhaps not surprisingly our findings suggest that the effect of including or excluding protected attributes is a function of how they relate with the prediction outcome. more specifically if a protected attribute is correlated with the target label and proves to be an important feature then their inclusion or exclusion would have effect on the performance and fairness and vice versa. our findings provide insightful information that can be used by relevant stakeholders to make well informed decisions.", "Pub Date": "2023-08-15"}