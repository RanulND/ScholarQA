{"Title": "A case study of fairness in generated images of Large Language Models for Software Engineering tasks", "Doi": "10.1109/ICSME58846.2023.00051", "Authors": ["m. sami", "a. sami", "p. barclay"], "Key Words": ["large language models", "bias", "gender diversity", "generative images", "dall-e-2"], "Abstract": "bias in large language models  large language model  has significant implications. since they have revolutionized content creation on the web they can lead to more unfair outcomes lack of inclusivity reinforcement of stereotypes and ethical and legal concerns. notably openai has recently made claims they have introduced a new technique to ensure that dall e 2 generates images of people accurately reflect the diversity of the world\u201a\u00e4\u00f4s population. in order to investigate bias within the field of software engineering the study utilized dall e 2 image generation to assess 56 tasks related to software engineering. another objective was to determine the impact of openai\u201a\u00e4\u00f4s new measures on the generated images for these specific tasks. two sets of experiments were conducted. in one set the tasks were prefixed with the clause \"as a software engineer\" while in the other set only the tasks themselves were used. the tasks were presented in a gender neutral manner and the artificial intelliegence was instructed to generate images for each task 20 times. for a female dominant task of doing administrative tasks 40 more images were generated. the study revealed a large gender bias in the 2280 images generated. for instance in the subset of experiments with prompts explicitly incorporating the phrase \"as a software engineer\" only 2% of the generated images portrayed female protagonists. in all the images in this setting male protagonists were dominant and in 45 tasks 100% of the protagonists were male. notably images generated without the prefixed clause only had more female protagonists in \u201a\u00e4\u00f2provide comments on project milestones\u201a\u00e4\u00f4 and \u201a\u00e4\u00f2provide enhancements\u201a\u00e4\u00f4 while other tasks did not exhibit a similar pattern. the findings emphasize unsuitability of implemented guardrails and the importance of further research on large language model assessments. further research is needed in large language model to find out where their guardrails fail so companies can address them properly.", "Pub Date": "2023-12-11"}