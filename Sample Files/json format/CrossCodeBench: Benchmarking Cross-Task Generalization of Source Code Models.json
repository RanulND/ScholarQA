{"Title": "CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models", "Doi": "10.1109/ICSE48619.2023.00055", "Authors": ["c. niu", "c. li", "v. ng", "b. luo"], "Key Words": ["pre-training of source code", "cross-task transfer learning", "few-shot learning", "ai for se"], "Abstract": "despite the recent advances showing that a model pre trained on large scale source code data is able to gain appreciable generalization capability it still requires a sizeable amount of data on the target task for fine tuning. and the effectiveness of the model generalization is largely affected by the size and quality of the fine tuning data which is detrimental for target tasks with limited or unavailable resources. therefore cross task generalization with the goal of improving the generalization of the model to unseen tasks that have not been seen before is of strong research and application value. in this paper we propose a large scale benchmark that includes 216 existing code related tasks. then we annotate each task with the corresponding meta information such as task description and instruction which contains detailed information about the task and a solution guide. this also helps us to easily create a wide variety of \u201a\u00e4\u00fatraining evaluation\u201a\u00e4\u00f9 task splits to evaluate the various cross task generalization capabilities of the model. then we perform some preliminary experiments to demonstrate that the cross task generalization of models can be largely improved by in context learning methods such as few shot learning and learning from task instructions which shows the promising prospects of conducting cross task learning research on our benchmark. we hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross task generalization.", "Pub Date": "2023-07-14"}