{"Title": "A Sound Approach: Using Large Language Models to Generate Audio Descriptions for Egocentric Text-Audio Retrieval", "Doi": "10.1109/ICASSP48485.2024.10448486", "Authors": ["a. -m. oncescu", "j. f. henriques", "a. zisserman", "s. albanie", "a. s. koepke"], "Key Words": ["text-audio retrieval", "large language models", "generated audio descriptions", "egocentric data"], "Abstract": "video databases from the internet are a valuable source of text audio retrieval datasets. however given that sound and vision streams represent different \"views\" of the data treating visual descriptions as audio descriptions is far from optimal. even if audio class labels are present they commonly are not very detailed making them unsuited for text audio retrieval. to exploit relevant audio information from video text datasets we introduce a methodology for generating audio centric descriptions using large language models  llms . in this work we consider the egocentric video setting and propose three new text audio retrieval benchmarks based on the epicmir and egomcq tasks and on the epicsounds dataset. our approach for obtaining audio centric descriptions gives significantly higher zero shot performance than using the original visual centric descriptions. furthermore we show that using the same prompts we can successfully employ llms to improve the retrieval on epicsounds compared to using the original audio class labels of the dataset. finally we confirm that llms can be used to determine the difficulty of identifying the action associated with a sound.", "Pub Date": "2024-03-18"}