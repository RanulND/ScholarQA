{"Title": "Sculpting DistilBERT: Enhancing Efficiency in Resource-Constrained Scenarios", "Doi": "10.1109/SMART59791.2023.10428568", "Authors": ["v. prema", "v. elavazhahan"], "Key Words": ["sentiment analysis", "distilbert"], "Abstract": "fine tuning large language models  large language model  such as bert for natural language processing  nlp  tasks can be challenging in resource constrained environments. distilbert is a smaller more efficient version of bert but it can still be too large for deployment on devices with limited memory and computational resources. model compression techniques can be used to reduce the size and improve the inference time of large language model but this can often lead to a decrease in model performance. in this paper we propose a novel approach to fine tuning distilbert with model compression techniques while maintaining or even improving model performance. we use a combination of pruning and quantization methods to reduce the model size and improve inference time. we also introduce a new training regime that is specifically designed for fine tuning compressed distilbert models. we evaluated our approach on sentiment analysis. our results show that we can achieve significant model compression and inference time improvements without sacrificing model performance. for example on the youtube comment sentiment analysis task we were able to reduce the model size by 50% and improve the inference time by 25% while maintaining the same accuracy as the original distilbert model. our findings suggest that our approach is a promising way to develop nlp models that are suitable for resource constrained environments.", "Pub Date": "2024-02-19"}