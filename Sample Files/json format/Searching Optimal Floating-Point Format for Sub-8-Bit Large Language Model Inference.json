{"Title": "Searching Optimal Floating-Point Format for Sub-8-Bit Large Language Model Inference", "Doi": "10.1109/ICEIC61013.2024.10457111", "Authors": ["y. hwang", "j. lee", "j. park", "j. lim", "j. choi"], "Key Words": ["large language model", "floating-point", "post-training quantization", "mixed-format"], "Abstract": "large language models  large language model  have shown remarkable success in various natural language processing tasks. however their extensive parameter count leads to significant memory and computational demands. to tackle these challenges there is growing interest in employing post training quantization  ptq  with reduced precision floating point  fp  operations. yet the optimal fp configuration remains a topic of debate. existing studies often overlook a thorough analysis of the diverse data distributions found in large language model and the crucial design choice denormal. in this paper we conduct a comprehensive examination of the various data distributions within large language model and the significance of denormal representation presenting a mixed format floating point framework. our proposed framework allows for sub 8 bit inference with minimal performance degradation in language modeling and reasoning tasks across a broad spectrum of large language model.", "Pub Date": "2024-03-19"}