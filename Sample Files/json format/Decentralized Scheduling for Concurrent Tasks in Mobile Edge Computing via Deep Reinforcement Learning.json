{"Title": "Decentralized Scheduling for Concurrent Tasks in Mobile Edge Computing via Deep Reinforcement Learning", "Doi": "10.1109/TMC.2023.3266226", "Authors": ["y. fan", "j. ge", "s. zhang", "j. wu", "b. luo"], "Key Words": ["deep reinforcement learning", "deep q-learning", "mobile edge computing", "resource allocation", "task offloading"], "Abstract": "mobile edge computing  mec  is a promising solution to enhance the computing capability of resource limited networks. a fundamental problem in mec is efficiently offloading tasks from user devices to edge servers. however there still exists a gap to deploy in real world environments  1  traditional centralized approaches needs complete information of edge network ignoring the communication costs generated by synchronization 2  previous works do not consider concurrent computation on edge servers which may cause dynamic changes in the environment and 3  the scheduling algorithm should deliver individualized decisions for different users independently and with high efficiency to solve this mismatch we studied a multi user task offloading problem where user devices make offloading decisions independently. we consider the concurrent execution of tasks and formulate a non divisible and delay aware task offloading problem to jointly minimize the dropped task ratio and long term latency. we propose a decentralized task scheduling algorithm based on drl that makes offloading decisions without knowing the information of other user devices. we employ double dqn dueling dqn prioritized replay memory and recurrent neural network  rnn  techniques to improve the algorithm performance. the results of simulation experiments show that our method can significantly reduce the long term latency and dropped task ratio compared to the baseline algorithms.", "Pub Date": "2024-03-07"}