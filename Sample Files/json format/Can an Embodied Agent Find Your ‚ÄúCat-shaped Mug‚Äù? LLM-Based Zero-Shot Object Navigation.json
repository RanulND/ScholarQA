{"Title": "Can an Embodied Agent Find Your \u201a\u00c4\u00faCat-shaped Mug\u201a\u00c4\u00f9? LLM-Based Zero-Shot Object Navigation", "Doi": "10.1109/LRA.2023.3346800", "Authors": ["v. s. dorbala", "j. f. mullen", "d. manocha"], "Key Words": ["ai-enabled robotics", "autonomous agents", "domestic robotics", "human-centered robotics"], "Abstract": "we present language guided exploration  lgx  a novel algorithm for language driven zero shot object goal navigation  l zson  where an embodied agent navigates to an uniquely described target object in a previously unseen environment. our approach makes use of large language models  llms  for this task by leveraging the llm commonsense reasoning capabilities for making sequential navigational decisions. simultaneously we perform generalized target object detection using a pre trained vision language grounding model. we achieve state of the art zero shot object navigation results on robothor with a success rate  sr  improvement of over 27% over the current baseline of the owl vit clip on wheels  owl cow . furthermore we study the usage of llms for robot navigation and present an analysis of various prompting strategies affecting the model output. finally we showcase the benefits of our approach via real world experiments that indicate the superior performance of lgx in detecting and navigating to visually unique objects.", "Pub Date": "2024-03-22"}