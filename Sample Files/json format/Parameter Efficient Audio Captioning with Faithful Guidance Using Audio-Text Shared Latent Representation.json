{"Title": "Parameter Efficient Audio Captioning with Faithful Guidance Using Audio-Text Shared Latent Representation", "Doi": "10.1109/ICASSP48485.2024.10448154", "Authors": ["a. k. sridhar", "y. guo", "e. visser", "r. mahfuz"], "Key Words": ["audio captioning", "hallucination", "clap"], "Abstract": "there has been significant research on developing pretrained transformer architectures for multimodal to text generation tasks. albeit performance improvements such models frequently suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. in this paper we address both these issues for the application of automated audio captioning. first we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio text shared latent space is suitable for detecting hallucination. then we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. during the beam decoding step the smaller model utilizes an audio text shared latent representation to semantically align the generated text with corresponding input audio. faithful guidance is introduced into the beam probability by incorporating the cosine similarity between latent representation projections of greedy rolled out intermediate beams and audio clip. we show the efficacy of our algorithm on benchmark datasets and evaluate the proposed scheme against baselines using conventional audio captioning and semantic similarity metrics while illustrating tradeoffs between performance and complexity.", "Pub Date": "2024-03-18"}