{"Title": "Biglog: Unsupervised Large-scale Pre-training for a Unified Log Representation", "Doi": "10.1109/IWQoS57198.2023.10188759", "Authors": ["s. tao", "y. liu", "w. meng", "z. ren", "h. yang", "x. chen", "l. zhang", "y. xie", "c. su", "x. oiao", "w. tian", "y. zhu", "t. han", "y. qin", "y. li"], "Key Words": ["log analysis", "language model", "log pre-training", "domain adaption"], "Abstract": "automated log analysis has been widely applied in modern data center network performing critical tasks such as log parsing log anomaly detection and log based failure prediction. however existing approaches rely on hand crafted features or domain specific vectors to represent logs which are either laborious in manual efforts or ineffective facing multiple domains in a system. furthermore general purpose word embeddings are not optimized for log data thus are data inefficient in handling complex log analysis tasks. in this paper we present a pre training phase for language models to understand both in sentence and cross sentence features of logs resulting in a unified representation of logs that is well suited for various downstream analysis tasks. the pre training phase is unsupervised utilizing 0.45 billion logs from 16 diverse domains. experiments on 12 publicly available evaluation datasets across 3 tasks indicate superiority of our approach against existing approaches especially in online scenarios with limited historical logs. our approach also exhibits remarkable few shot learning ability and domain adaptiveness which not only outperforms existing approaches using only 0.0025% of their required training data but also adapts into new domains via only a few in domain logs. we release our code and pre trained model.", "Pub Date": "2023-07-27"}