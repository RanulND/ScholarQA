{"Title": "An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention", "Authors": ["y. wang", "y. qin", "d. deng", "j. wei", "y. zhou", "y. fan", "t. chen", "h. sun", "l. liu", "s. wei", "s. yin"], "Pub Date": "2022-12-27", "Abstract": "transformer based models achieve tremendous success in many artificial intelligence  artificial intelliegence  tasks outperforming conventional convolution neural networks  cnns  from natural language processing  nlp  to computer vision  cv . their success relies on the self attention mechanism that provides a global rather than local receptive field as cnns. despite its superiority the global\u201a\u00e4\u00eclevel self attention consumes  $\\sim 100\\times $  more operations than cnns and cannot be effectively handled by the existing cnn processor due to the distinct operations. it inspires an urgent requirement to design a dedicated transformer processor. however global self attention involves massive naturally existent weakly related tokens  wr tokens  due to the redundant contents in human languages or images. these wr tokens generate zero and near zero attention results that introduce energy consumption bottleneck redundant computations and hardware under utilization issues making it challenging to achieve energy efficient self attention computing. this article proposes a transformer processor effectively handling the wr tokens to solve these challenges. first a big exact small approximate processing element  pe  reduces multiply and accumulate  mac  energy for wr tokens by adaptively computing the small values approximately while computing the large values exactly. second a bidirectional asymptotical speculation unit captures and removes redundant computations of zero attention outputs by exploiting the local property of self attention. third an out of order pe line computing scheduler improves hardware utilization for near zero values by reordering the operands to dovetail two operations into one multiplication. fabricated in a 28 nm cmos technology the proposed processor occupies an area of 6.82 mm2. when evaluated with a 90% of approximate computing for the generative pre trained transformer 2  gpt 2  model the peak energy efficiency is 27.56 tops w under 0.56 v at 50 mhz  $17.66\\times $  higher than a100 graphics processing unit  gpu . compared with the state of the art transformer processor it reduces energy by  $4.57\\times $  and offers  $3.73\\times $  speedup.", "Doi": "10.1109/JSSC.2022.3213521", "Key Words": ["approximate computing", "out-of-order computing", "processor", "self-attention", "speculating", "transformer"]}