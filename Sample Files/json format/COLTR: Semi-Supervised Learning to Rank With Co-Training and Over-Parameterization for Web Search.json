{"Title": "COLTR: Semi-Supervised Learning to Rank With Co-Training and Over-Parameterization for Web Search", "Doi": "10.1109/TKDE.2023.3270750", "Authors": ["y. li", "h. xiong", "q. wang", "l. kong", "h. liu", "h. li", "j. bian", "s. wang", "g. chen", "d. dou", "d. yin"], "Key Words": ["learning to rank", "semi-supervised learning", "over-parameterization"], "Abstract": "while learning to rank  ltr  has been widely used in web search to prioritize most relevant webpages among the retrieved contents subject to the input queries the traditional ltr models fail to deliver decent performance due to two main reasons  1  the lack of well annotated query webpage pairs with ranking scores to cover search queries of various popularity and 2  ill trained models based on a limited number of training samples with poor generalization performance. to improve the performance of ltr models tremendous efforts have been done from above two aspects such as enlarging training sets with pseudo labels of ranking scores by self training or refining the features used for ltr through feature extraction and dimension reduction. though ltr performance has been marginally increased we still believe these methods could be further improved in the newly fashioned \u201a\u00e4\u00fainterpolating regime\u201a\u00e4\u00f9. specifically instead of lowering the number of features used for ltr models our work proposes to transform original data with random fourier feature so as to over parameterize the downstream ltr models  e.g. gbrank or lightgbm  with features in ultra high dimensionality and achieve superb generalization performance. furthermore rather than self training with pseudo labels produced by the same ltr model in a \u201a\u00e4\u00faself tuned\u201a\u00e4\u00f9 fashion the proposed method incorporates the diversity of prediction results between the listwise and pointwise ltr models while co training both models with a cyclic labeling prediction pipeline in a \u201a\u00e4\u00faping pong\u201a\u00e4\u00f9 manner. we deploy the proposed co trained and over parameterized ltr system coltr at baidu search and evaluate coltr with a large number of baseline methods. the results show that coltr could achieve $\\delta ndcg {4}$\u0153\u00eendcg4 = 3.64%$\\sim$\u201a\u00e0\u00ba4.92% compared to baselines under various ratios of labeled samples. we also conduct a 7 day a b test using the realistic web traffics of baidu search where we can still observe significant performance improvement around $\\delta ndcg {4}$\u0153\u00eendcg4 = 0.17%$\\sim$\u201a\u00e0\u00ba0.92% in real world applications. coltr performs consistently both in online and offline experiments.", "Pub Date": "2023-11-07"}