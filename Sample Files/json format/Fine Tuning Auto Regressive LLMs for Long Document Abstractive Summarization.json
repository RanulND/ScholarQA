{"Title": "Fine Tuning Auto Regressive LLMs for Long Document Abstractive Summarization", "Doi": "10.1109/ICIDeA59866.2023.10295238", "Authors": ["m. k. rath", "s. banerjee", "t. swain"], "Key Words": ["large language models", "natural language processing", "natural language generations", "wafer-scale cluster", "abstractive summarization"], "Abstract": "generating a short summary from a long document is a challenging task for which new language models are still being designed and trained based on the available data. since deep learning models are used for nlp and nlg applications it requires high computational power to train these models. further fine tuning the weights based on a given context is an important task that needs additional computation space and time. in this paper we have used cerebras\u201a\u00e4\u00f4 wafer scale cluster that aims at providing an efficient software and hardware infrastructure that enhances the capabilities of pre existing models and empowers them to handle lengthy documents well. in addition to analyzing common models along with their pros and cons other factors such as the context lengths and model sizes have also been analyzed to accommodate lengthy documents as much as possible.", "Pub Date": "2023-10-31"}