{"Title": "Large Language Models (LLMs) Inference Offloading and Resource Allocation in Cloud-Edge Networks: An Active Inference Approach", "Doi": "10.1109/VTC2023-Fall60731.2023.10333824", "Authors": ["j. fang", "y. he", "f. r. yu", "j. li", "v. c. leung"], "Key Words": ["active inference", "large language model inference task", "task offloading", "resource allocation", "cloud-edge networks", "reinforcement learning"], "Abstract": "as the research and applications of large language model  llm  become increasingly sophisticated it is difficult for resource limited mobile terminals to run large model inference tasks efficiently. traditional deep reinforcement learning  drl  based approaches have been used to offload llm inference tasks to servers. however existing solutions suffer from data inefficiency insensitivity to latency requirements and non adaptability to task load variations. in this paper we propose an active inference with rewardless guidance algorithm using expected future free energy for offloading decisions and allocating resources for the llm inference task offloading and resource allocation problem of cloud edge networks systems. experimental results show that our proposed method has superior performance over mainstream drls improves in data utilization efficiency and is more adaptable to changing task load scenarios.", "Pub Date": "2023-12-11"}