{"Title": "Chronos: Accelerating Federated Learning With Resource Aware Training Volume Tuning at Network Edges", "Doi": "10.1109/TVT.2022.3218155", "Authors": ["y. liu", "x. zhang", "y. zhao", "y. he", "s. yu", "k. zhu"], "Key Words": ["distributed machine learning", "federated learning", "artificial intelligence", "edge computing", "parallel mechanism"], "Abstract": "due to the limited resources and data privacy issue last decade witnesses the fast development of distributed machine learning  dml  at network edges. among all the existing dml paradigms federated learning  fl  would be a promising one since in fl each client trains its local model without sharing the raw data with others. a community of clients with the same interest can join together to derive a high performance model by periodically synchronizing the parameters of their local models under the help of a coordination server. however fl will encounter the straggler problem at network edges and hence the synchronization among clients becomes inefficient. it slows down the convergence speed of learning process. to alleviate the straggler problem we propose a method named chronos that accelerates fl with training volume tuning in this paper. more specifically chronos is a resource aware method that adaptively adjusts the amount of data used by each client for training  i.e. training volume  in each iteration in order to eliminate the synchronization waiting time caused by the heterogeneous and dynamical computing and communication resources. in addition we theoretically analyze the convergence of chronos in a non convex setting and utilize the results for the algorithm design of chronos in return to guarantee the convergence. extensive experiments show that compared with the benchmark algorithms  i.e. bsp and ssp  chronos significantly improves convergence speed by up to 6.4\u221a\u00f3.", "Pub Date": "2023-03-14"}