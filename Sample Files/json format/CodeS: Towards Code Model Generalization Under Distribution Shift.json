{"Title": "CodeS: Towards Code Model Generalization Under Distribution Shift", "Doi": "10.1109/ICSE-NIER58687.2023.00007", "Authors": ["q. hu", "y. guo", "x. xie", "m. cordy", "m. papadakis", "l. ma", "y. le traon"], "Key Words": ["source code learning", "distribution shift"], "Abstract": "distribution shift has been a longstanding challenge for the reliable deployment of deep learning  dl  models due to unexpected accuracy degradation. although dl has been becoming a driving force for large scale source code analysis in the big code era limited progress has been made on distribution shift analysis and benchmarking for source code tasks. to fill this gap this paper initiates to propose codes a distribution shift benchmark dataset for source code learning. specifically codes supports two programming languages  java and python  and five shift types  task programmer time stamp token and concrete syntax tree . extensive experiments based on codes reveal that 1  out of distribution detectors from other domains  e.g. computer vision  do not generalize to source code 2  all code classification models suffer from distribution shifts 3  representation based shifts have a higher impact on the model than others and 4  pretrained bimodal models are relatively more resistant to distribution shifts.", "Pub Date": "2023-07-12"}