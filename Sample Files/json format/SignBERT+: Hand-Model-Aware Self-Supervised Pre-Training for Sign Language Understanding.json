{"Title": "SignBERT+: Hand-Model-Aware Self-Supervised Pre-Training for Sign Language Understanding", "Doi": "10.1109/TPAMI.2023.3269220", "Authors": ["h. hu", "w. zhao", "w. zhou", "h. li"], "Key Words": ["self-supervised pre-training", "masked modeling strategies", "model-aware hand prior", "sign language understanding"], "Abstract": "hand gesture serves as a crucial role during the expression of sign language. current deep learning based methods for sign language understanding  slu  are prone to over fitting due to insufficient sign data resource and suffer limited interpretability. in this paper we propose the first self supervised pre trainable signbert+ framework with model aware hand prior incorporated. in our framework the hand pose is regarded as a visual token which is derived from an off the shelf detector. each visual token is embedded with gesture state and spatial temporal position encoding. to take full advantage of current sign data resource we first perform self supervised learning to model its statistics. to this end we design multi level masked modeling strategies  joint frame and clip  to mimic common failure detection cases. jointly with these masked modeling strategies we incorporate model aware hand prior to better capture hierarchical context over the sequence. after the pre training we carefully design simple yet effective prediction heads for downstream tasks. to validate the effectiveness of our framework we perform extensive experiments on three main slu tasks involving isolated and continuous sign language recognition  slr  and sign language translation  slt . experimental results demonstrate the effectiveness of our method achieving new state of the art performance with a notable gain.", "Pub Date": "2023-08-07"}