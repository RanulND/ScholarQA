{"Title": "A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of One-Stage Referring Expression Comprehension", "Doi": "10.1109/TMM.2023.3314153", "Authors": ["g. luo", "y. zhou", "j. sun", "x. sun", "r. ji"], "Key Words": ["computer vision", "object recognition"], "Abstract": "one stage referring expression comprehension  rec  is a task that requires accurate alignment between text descriptions and visual content. in recent years numerous efforts have been devoted to cross modal learning for rec while the influence of other factors in this task still lacks a systematic study. to fill this gap we conduct an empirical study in this article. concretely we ablate 42 candidate designs settings based on a common rec framework and these candidates cover the entire process of one stage rec from network design to model training. afterwards we conduct over 100 experimental trials on three rec benchmark datasets. the extensive experimental results reveal the key factors that affect rec performance in addition to multi modal fusion e.g. multi scale features and data augmentation. based on these findings we further propose a simple yet strong model called simrec which achieves new state of the art performance on these benchmarks. in addition to these progresses we also find that with much less training overhead and parameters simrec can achieve better performance than a set of large scale pre trained models e.g. uniter and villa portraying the special role of rec in existing v&l research.", "Pub Date": "2024-02-16"}