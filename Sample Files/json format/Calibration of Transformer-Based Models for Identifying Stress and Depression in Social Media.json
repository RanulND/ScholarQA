{"Title": "Calibration of Transformer-Based Models for Identifying Stress and Depression in Social Media", "Doi": "10.1109/TCSS.2023.3283009", "Authors": ["l. ilias", "s. mouzakitis", "d. askounis"], "Key Words": ["calibration", "depression", "emotion", "mental health", "stress", "transformers"], "Abstract": "in today\u201a\u00e4\u00f4s fast paced world the rates of stress and depression present a surge. people use social media for expressing their thoughts and feelings through posts. therefore social media provide assistance for the early detection of mental health conditions. existing methods mainly introduce feature extraction approaches and train shallow machine learning  ml  classifiers. for addressing the need of creating a large feature set and obtaining better performance other research studies use deep neural networks or language models based on transformers. despite the fact that transformer based models achieve noticeable improvements they cannot often capture rich factual knowledge. although there have been proposed a number of studies aiming to enhance the pretrained transformer based models with extra information or additional modalities no prior work has exploited these modifications for detecting stress and depression through social media. in addition although the reliability of a machine learning  ml  model\u201a\u00e4\u00f4s confidence in its predictions is critical for high risk applications there is no prior work taken into consideration the model calibration. to resolve the above issues we present the first study in the task of depression and stress detection in social media which injects extra linguistic information in transformer based models namely bidirectional encoder representations from transformers  bert  and mentalbert. specifically the proposed approach employs a multimodal adaptation gate for creating the combined embeddings which are given as input to a bert  or mentalbert  model. for taking into account the model calibration we apply label smoothing. we test our proposed approaches in three publicly available datasets and demonstrate that the integration of linguistic features into transformer based models presents a surge in performance. also the usage of label smoothing contributes to both the improvement of the model\u201a\u00e4\u00f4s performance and the calibration of the model. we finally perform a linguistic analysis of the posts and show differences in language between stressful and nonstressful texts as well as depressive and nondepressive posts.", "Pub Date": "2024-04-02"}