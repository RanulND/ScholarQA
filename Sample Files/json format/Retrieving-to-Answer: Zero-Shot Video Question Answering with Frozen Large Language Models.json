{"Title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models", "Doi": "10.1109/ICCVW60793.2023.00035", "Authors": ["j. pan", "z. lin", "y. ge", "x. zhu", "r. zhang", "y. wang", "y. qiao", "h. li"], "Key Words": ["video question answering", "large language models", "multimodal models"], "Abstract": "video question answering  videoqa  has been significantly advanced from the scaling of recent large language models  large language model . the key idea is to convert the visual information into the language feature space so that the capacity of large language model can be fully exploited. existing videoqa methods typically take two paradigms   1  learning cross modal alignment and  2  using an off the shelf captioning model to describe the visual data. however the first design needs costly training on many extra multi modal data whilst the second is further limited by limited domain generalization. to address these limitations a simple yet effective retrieving to answer  r2a  framework is proposed. given an input video r2a first retrieves a set of semantically similar texts from a generic text corpus using a pre trained multi modal model  e.g. clip . with both the question and the retrieved texts a large language model  e.g. deberta  can be directly used to yield a desired answer. without the need for cross modal fine tuning r2a allows for all the key components  e.g. large language model retrieval model and text corpus  to plug and play. extensive experiments on several videoqa benchmarks show that despite with 1.3b parameters and no fine tuning our r2a can outperform the 61\u221a\u00f3 larger flamingo 80b model  even additionally trained on nearly 2.1b multi modal data.", "Pub Date": "2023-12-25"}