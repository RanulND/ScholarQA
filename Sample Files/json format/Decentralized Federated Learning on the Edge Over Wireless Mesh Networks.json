{"Title": "Decentralized Federated Learning on the Edge Over Wireless Mesh Networks", "Doi": "10.1109/ACCESS.2023.3329362", "Authors": ["a. salama", "a. stergioulis", "s. a. r. zaidi", "d. mclernon"], "Key Words": ["the internet of things (iot)", "federated learning", "decentralized federated learning", "edge computing", "data privacy"], "Abstract": "the rapid growth of internet of things  iot  devices has generated vast amounts of data leading to the emergence of federated learning as a novel distributed machine learning paradigm. federated learning enables model training at the edge leveraging the processing capacity of edge devices while preserving privacy and mitigating data transfer bottlenecks. however the conventional centralized federated learning architecture suffers from a single point of failure and susceptibility to malicious attacks. in this study we delve into an alternative approach called decentralized federated learning  dfl  conducted over a wireless mesh network as the communication backbone. we perform a comprehensive network performance analysis using stochastic geometry theory and physical interference models offering fresh insights into the convergence analysis of dfl. additionally we conduct system simulations to assess the proposed decentralized architecture under various network parameters and different aggregator methods such as fedavg krum and median methods. our model is trained on the widely recognized emnist dataset for benchmarking handwritten digit classification. to minimize the model\u201a\u00e4\u00f4s size at the edge and reduce communication overhead we employ a cutting edge compression technique based on genetic algorithms. our simulation results reveal that the compressed decentralized architecture achieves performance comparable to the baseline centralized architecture and traditional dfl in terms of accuracy and average loss for our classification task. moreover it significantly reduces the size of shared models over the wireless channel by compressing participants\u201a\u00e4\u00f4 local model sizes to nearly half of their original size compared to the baselines effectively reducing complexity and communication overhead.", "Pub Date": "2023-11-10"}