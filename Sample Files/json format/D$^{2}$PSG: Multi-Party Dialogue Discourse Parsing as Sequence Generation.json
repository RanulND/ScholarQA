{"Title": "D$^{2}$PSG: Multi-Party Dialogue Discourse Parsing as Sequence Generation", "Doi": "10.1109/TASLP.2023.3313415", "Authors": ["a. wang", "l. song", "l. jin", "j. yao", "h. mi", "c. lin", "j. su", "d. yu"], "Key Words": ["multi-party dialogue discourse parsing", "pretrained language model", "model initialization", "sequence generation"], "Abstract": "conversational discourse analysis aims to extract the interactions between dialogue turns which is crucial for modeling complex multi party dialogues. as the benchmarks are still limited in size and human annotations are costly the current standard approaches apply pretrained language models but they still require randomly initialized classifiers to make predictions. these classifiers usually require massive data to work smoothly with the pretrained encoder causing severe data hunger issue. we propose two convenient strategies to formulate this task as a sequence generation problem where classifier decisions are carefully converted into sequence of tokens. we then adopt a pretrained t5 [c. raffel et al. 2020] model to solve this task so that no parameters are randomly initialized. we also leverage the descriptions of the discourse relations to help model understand their meanings. experiments on two popular benchmarks show that our approach outperforms previous state of the art models by a large margin and it is also more robust in zero shot and few shot settings.1", "Pub Date": "2023-10-23"}