{"Title": "LMIE-BERT: A Learnable Method for Inter-Layer Ensembles to Accelerate Inference of BERT-Style Pre-trained Models", "Doi": "10.1109/BIGCOM61073.2023.00044", "Authors": ["w. qi", "x. guo", "h. du"], "Key Words": ["bert", "model compression", "early exit"], "Abstract": "pre trained models have brought tremendous accuracy improvements to natural language processing nlp  and computer vision tasks but they suffer from slow inference speed due to the heavy model which hinders their deployment in production. the early exit methods have been proposed to accelerate the inference speed of large pre trained models. however these methods will lose control of accuracy at higher speed ratios. in order to balance the trade off between model speed and accuracy better we propose a novel early exit mechanism called lmie bert. to achieve this we introduce a learnable method for inter layer ensemble strategy in the internal classifier it trains the model to fit the information from both the previous and current layers which enables the early exit method to get more robust results. the experimental results demonstrate that lmie bert can maintain over 90% of the accuracy of the original model while achieving a 4\u221a\u00f3 inference speed up in multiple tasks. our method is ahead of other early exit methods in terms of model accuracy for the same speed ratio.", "Pub Date": "2024-02-07"}