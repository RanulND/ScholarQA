{"Title": "Few-shot Domain-Adaptative Visually-fused Event Detection from Text", "Doi": "10.23919/FUSION52260.2023.10224213", "Authors": ["f. moghimifar", "f. shiri", "r. haffari", "y. -f. li", "v. nguyen"], "Key Words": ["event detection", "multimodal event detection", "few-shot learning", "image generation"], "Abstract": "incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. the complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. however current approaches in this area suffer from data scarcity where a large amount of labelled text image pairs are required for model training. furthermore limited access to the visual context at inference time negatively impacts the performance of such models which makes them practically ineffective in real world scenarios. in this paper we present a novel domain adaptive visually fused event detection approach that can be trained on a few labelled image text paired data points. specifically we introduce a visual imaginator method that synthesises images from text in the absence of visual context. moreover the imaginator can be customised to a specific domain. in doing so our model can leverage the capabilities of pre trained vision language models and can be trained in a few shot setting. this also allows for effective inference where only single modality data  i.e. text  is available. the experimental evaluation on the benchmark m2e2 dataset shows that our model outperforms existing state of the art models by up to 11 points.", "Pub Date": "2023-08-25"}