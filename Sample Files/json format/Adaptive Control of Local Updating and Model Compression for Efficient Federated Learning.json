{"Title": "Adaptive Control of Local Updating and Model Compression for Efficient Federated Learning", "Doi": "10.1109/TMC.2022.3186936", "Authors": ["y. xu", "y. liao", "h. xu", "z. ma", "l. wang", "j. liu"], "Key Words": ["edge computing", "federated learning", "local updating", "model compression"], "Abstract": "data generated at the network edge can be processed locally by leveraging the paradigm of edge computing  ec . aided by ec federated learning  fl  has been becoming a practical and popular approach for distributed machine learning over locally distributed data. however fl faces three critical challenges i.e. resource constraint system heterogeneity and context dynamics in ec. to address these challenges we present a training efficient fl method termed fedlamp by optimizing both the local updating frequency and model compression ratio in the resource constrained ec systems. we theoretically analyze the model convergence rate and obtain a convergence upper bound related to the local updating frequency and model compression ratio. upon the convergence bound we propose a control algorithm that adaptively determines diverse and appropriate local updating frequencies and model compression ratios for different edge nodes so as to reduce the waiting time and enhance the training efficiency. we evaluate the performance of fedlamp through extensive simulation and testbed experiments. evaluation results show that fedlamp can reduce the traffic consumption by 63% and the completion time by about 52% for achieving the similar test accuracy compared to the baselines.", "Pub Date": "2023-08-31"}