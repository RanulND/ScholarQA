{"Title": "Arabic Speech Recognition based on Self Supervised Learning", "Doi": "10.1109/DeSE60595.2023.10469031", "Authors": ["h. a. younis", "y. f. mohammad"], "Key Words": ["asr", "ssl", "hubert", "wav2vec2xlsr", "wer", "arabic language", "mms"], "Abstract": "automatic arabic speech recognition  aasr  has gained significant attention in recent years due to its potential applications in various fields such as transcription voice assistants and language learning. however asr systems typically require a large amount of labelled data for training which can be expensive and time consuming to obtain especially for languages with limited resources like arabic. to address this challenge self supervised learning techniques have emerged as a promising approach to improve asr performance by leveraging unlabeled data.in this research the architecture of the hubert model was enhanced by removing the last layer and adding additional layers including  normalization layer two linear layer followed by relu activation function drop out layer  and finally a linear projection layer for alignment with the vocabulary size of arabic language dataset which led to better performance and smaller word error rate wer  as compared to hubert base model. many experiments were also applied in this research for enhancing mono and multi lingual asr models using different combination of hyper parameters through trial and error. results showed that multi lingual wav2vec2xlsr model outperformed monolingual hubert model and massive multilingual speech mms  model fine tuned on arabic common voice dataset. also the proposed modified hubert model with specific type of learning rate scheduler mhlr  gave better results than base hubert model and good results as compared with large mms model. these models were evaluated using wer and wrr metrics and showed 40% for wav2vec2xlsr model and 42% 45% 54% for mms mhlr and hubert base models respectively.", "Pub Date": "2024-03-21"}