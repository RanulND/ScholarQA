{"Title": "Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code", "Doi": "10.1109/ISSREW60843.2023.00060", "Authors": ["c. improta"], "Key Words": ["ai-based code generators", "offensive security", "data poisoning"], "Abstract": "artificial intelliegence based code generators have gained a fundamental role in assisting developers in writing software starting from natural language  nl . however since these large language models are trained on massive volumes of data collected from unreliable online sources  e.g. github hugging face  artificial intelliegence models become an easy target for data poisoning attacks in which an attacker corrupts the training data by injecting a small amount of poison into it i.e. astutely crafted malicious samples. in this position paper we address the security of artificial intelliegence code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. next we devise an extensive evaluation of how these attacks impact state of the art models for code generation. lastly we discuss potential solutions to overcome this threat.", "Pub Date": "2023-11-02"}