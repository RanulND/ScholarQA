{"Title": "Text Labels Classification Model based on BERT Algorithm", "Doi": "10.1109/ICBASE59196.2023.10303262", "Authors": ["s. wu", "z. huang", "h. feng"], "Key Words": ["text labels classification", "bert", "pre-training", "fine-tunning"], "Abstract": "text labels are extracted from the content of texts and is a issue of natural language process which contains multiple labels. the text labels classification aims to divide the multiple labels into only one correct category. traditionally researchers utilize machine learning algorithms to train an one classifier to solve the multiple labels which may cause wrong classifier due to the machine learning can not process the relationship between texts. in this paper we utilize pre training of deep bidirectional transformers  bert  to dispose text understanding issues. initially the pre train process is established to capture the text representations and the initial classification is executed. subsequently the fine tunning function is to optimize the parameters of bert model including learning rate training epochs and batch sizes. from our extensive experimental results we can significantly conclude that our proposed method can achieve the text labels classification with reasonable computation costs and training process.", "Pub Date": "2023-11-03"}