{"Title": "Tree Learning: Towards Promoting Coordination in Scalable Multi-Client Training Acceleration", "Doi": "10.1109/TMC.2023.3259007", "Authors": ["t. guo", "s. guo", "f. wu", "w. xu", "j. zhang", "q. zhou", "q. chen", "w. zhuang"], "Key Words": ["efficient collaborative training", "edge computing", "tree-aggregation scheme"], "Abstract": "iteration based collaborative learning  cl  paradigms such as federated learning  fl  and split learning  sl  faces challenges in training neural models over the rapidly growing yet resource constrained edge devices. such devices have difficulty in accommodating a full size large model for fl or affording an excessive waiting time for the mandatory synchronization step in sl. to deal with such challenge we propose a novel cl framework which adopts an tree aggregation structure with an adaptive partition and ensemble strategy to achieve optimal synchronization and fast convergence at scale. to find the optimal split point for heterogeneous clients we also design a novel partitioning algorithm by minimizing the idleness during communication and achieving the optimal synchronization between clients. in addition a parallelism paradigm is proposed to unleash the potential of optimum synchronization between the clients and server to boost the distributed training process without losing model accuracy for edge devices. furthermore we theoretically prove that our framework can achieve better convergence rate than state of the art cl paradigms. we conduct extensive experiments and show that our framework is 4.6\u221a\u00f3 in training speed as compared with the traditional methods without compromising training accuracy.", "Pub Date": "2024-02-05"}