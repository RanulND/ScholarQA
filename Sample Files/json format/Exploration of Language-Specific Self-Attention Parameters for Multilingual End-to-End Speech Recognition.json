{"Title": "Exploration of Language-Specific Self-Attention Parameters for Multilingual End-to-End Speech Recognition", "Doi": "10.1109/SLT54892.2023.10022937", "Authors": ["b. houston", "k. kirchhoff"], "Key Words": ["multilingual", "ctc", "self-attention"], "Abstract": "in the last several years end to end  e2e  asr models have mostly surpassed the performance of hybrid asr models. e2e is particularly well suited to multilingual approaches because it doesn't require language specific phone alignments for training. recent work has improved multilingual e2e modeling over naive data pooling on up to several dozen languages by using both language specific and language universal model parameters as well as providing information about the language being presented to the network. complementary to previous work we analyze language specific parameters in the attention mechanism of conformer based encoder models. we show that using language specific parameters in the attention mechanism can improve performance across six languages by up to 12% compared to standard multilingual baselines and up to 36% compared to monolingual baselines without requiring any additional parameters during monolingual inference nor fine tuning.", "Pub Date": "2023-01-27"}