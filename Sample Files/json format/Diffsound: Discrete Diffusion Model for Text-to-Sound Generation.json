{"Title": "Diffsound: Discrete Diffusion Model for Text-to-Sound Generation", "Doi": "10.1109/TASLP.2023.3268730", "Authors": ["d. yang", "j. yu", "h. wang", "w. wang", "c. weng", "y. zou", "d. yu"], "Key Words": ["autoregressive model", "diffusion model", "text-to-sound generation", "vocoder"], "Abstract": "generating sound effects that people want is an important topic. however there are limited studies in this area for sound generation. in this study we investigate generating sound conditioned on a text prompt and propose a novel text to sound generation framework that consists of a text encoder a vector quantized variational autoencoder  vq vae  a token decoder and a vocoder. the framework first uses the token decoder to transfer the text features extracted from the text encoder to a mel spectrogram with the help of vq vae and then the vocoder is used to transform the generated mel spectrogram into a waveform. we found that the token decoder significantly influences the generation performance. thus we focus on designing a good token decoder in this study. we begin with the traditional autoregressive  ar  token decoder. however the ar token decoder always predicts the mel spectrogram tokens one by one in order which may introduce the unidirectional bias and accumulation of errors problems. moreover with the ar token decoder the sound generation time increases linearly with the sound duration. to overcome the shortcomings introduced by ar token decoders we propose a non autoregressive token decoder based on the discrete diffusion model named diffsound. specifically the diffsound model predicts all of the mel spectrogram tokens in one step and then refines the predicted tokens in the next step so the best predicted results can be obtained by iteration. our experiments show that our proposed diffsound model not only produces better generation results when compared with the ar token decoder but also has a faster generation speed i.e. mos  3.56 v.s 2.786.", "Pub Date": "2023-05-05"}