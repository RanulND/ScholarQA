{"Title": "UbiNN: A Communication Efficient Framework for Distributed Machine Learning in Edge Computing", "Doi": "10.1109/TNSE.2023.3260566", "Authors": ["k. li", "k. chen", "s. luo", "h. zhang", "p. fan"], "Key Words": ["communication-efficient", "distributed machine learning", "edge computing", "knowledge distillation", "ubinn"], "Abstract": "deployment of distributed machine learning at the edge is conducive to reducing latency and protecting privacy associated with transmitting data back to the cloud. nonetheless as machine learning models scale bandwidth resources are limited and heterogeneity in data collection from heterogeneous edge devices is visible resulting in low model accuracy and high communication overhead. in this paper a novel distributed deep learning framework called ubiquitous neural network  ubinn  is proposed to improve communication efficiency without affecting the accuracy of the local neural network model at the edge or the global neural network model in the cloud. as for the accuracy of the neural network model a common dataset with a small portion of insensitive data is constructed for training the neural network model and its accuracy is enhanced by a new algorithm based on knowledge distillation and covariance computation  kdcc . experimental results demonstrate that the test accuracy of ubinn is extremely close to the centralized machine learning and non federated learning scheme ddnn and up to 18.74% better than that of other classic federated learning schemes when using public datasets such as mnist cifar 10 and reuters 21578. meanwhile communication overheads are substantially reduced in terms of data transmission volume and latency.", "Pub Date": "2023-10-24"}