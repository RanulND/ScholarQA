{"Title": "Improving Vision-and-Language Reasoning via Spatial Relations Modeling", "Doi": "10.1109/WACV57701.2024.00082", "Authors": ["c. yang", "r. xu", "y. guo", "p. huang", "y. chen", "w. ding", "z. wang", "h. zhou"], "Key Words": ["algorithms", "image recognition and understanding", "algorithms", "machine learning architectures", "formulations", "and algorithms", "algorithms", "vision + language and/or other modalities"], "Abstract": "visual commonsense reasoning  vcr  is a challenging multi modal task which requires high level cognition and commonsense reasoning ability about the real world. in recent years large scale pre training approaches have been developed and promoted the state of the art performance of vcr. however the existing approaches almost employ the bert like objectives to learn multi modal representations. these objectives motivated from the text domain are insufficient for the excavation on the complex scenario of visual modality. most importantly the spatial distribution of the visual objects is basically neglected. to address the above issue we propose to construct the spatial relation graph based on the given visual scenario. further we design two pre training tasks named object position regression  opr  and spatial relation classification  src  to learn to reconstruct the spatial relation graph respectively. quantitative analysis suggests that the proposed method can guide the representations to maintain more spatial context and facilitate the attention on the essential visual regions for reasoning. we achieve the state of the art results on vcr and two other vision and language reasoning tasks visual question answering and nlvr2.", "Pub Date": "2024-04-09"}