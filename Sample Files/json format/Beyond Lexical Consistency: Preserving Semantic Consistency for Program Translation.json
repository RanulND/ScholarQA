{"Title": "Beyond Lexical Consistency: Preserving Semantic Consistency for Program Translation", "Doi": "10.1109/ICDM58522.2023.00018", "Authors": ["y. du", "y. -f. ma", "z. xie", "m. li"], "Key Words": ["program translation", "semantic consistency", "regularization", "large language model"], "Abstract": "program translation aims to convert the input programs from one programming language to another. automatic program translation is a prized target of software engineering research which leverages the reusability of projects and improves the efficiency of development. recently thanks to the rapid development of deep learning model architectures and the availability of large scale parallel corpus of programs the performance of program translation has been greatly improved. however the existing program translation models are still far from satisfactory in terms of the quality of translated programs. in this paper we argue that a major limitation of the current approaches is the lack of consideration of semantic consistency. beyond lexical consistency semantic consistency is also critical for the task. to make the program translation model more semantically aware we propose a general framework named preserving semantic consistency for program translation  pscpt  which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder decoder methods with various neural networks  e.g. lstm transformer  as the backbone. we conduct extensive experiments in 7 general programming languages. experimental results show that with codebert as the backbone our approach outperforms not only the state of the art open source models but also the commercial closed large language models  e.g. textdavinci 002 text davinci 003  on the program translation task. our replication package  including code data etc.  is publicly available at https //github.com duyali2000/pscpt.", "Pub Date": "2024-02-05"}