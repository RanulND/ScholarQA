{"Title": "GMML is All You Need", "Doi": "10.1109/ICIP49359.2023.10222150", "Authors": ["s. atito", "m. awais", "s. nandam", "j. kittler"], "Key Words": ["self-supervised learning", "vision transformers", "group masked model learning", "deep learning"], "Abstract": "vision transformers  vits  have generated significant interest in the computer vision community because of their flexibility in exploiting contextual information whether it is sharply confined local or long range global. however they are known to be data hungry and therefore often pretrained on large scale datasets e.g. jft 300m or imagenet. an ideal learning method would perform best regardless of the size of the dataset a property lacked by current learning methods with merely a few existing works studying vits with limited data. we propose group masked model learning  gmml  a self supervised learning  ssl  method that is able to train vits and achieve state of the art  sota  performance when pre trained with limited data. the gmml uses the information conveyed by all concepts in the image. this is achieved by manipulating randomly groups of connected tokens successively covering different meaningful parts of the image content and then recovering the hidden information from the visible part of the concept. unlike most of the existing ssl approaches gmml does not require momentum encoder nor relies on careful implementation details such as large batches and gradient stopping. pretraining finetuning and evaluation codes are available under  https //github.com gmml.", "Pub Date": "2023-09-11"}