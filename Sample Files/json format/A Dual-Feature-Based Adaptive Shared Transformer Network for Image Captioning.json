{"Title": "A Dual-Feature-Based Adaptive Shared Transformer Network for Image Captioning", "Authors": ["y. shi", "j. xia", "m. zhou", "z. cao"], "Pub Date": "2024-02-14", "Abstract": "current models exhibit notable efficacy in image captioning tasks. mainstream research shows that combining dual visual features enhances visual representations and brings a performance boost. however the incorporation of dual visual features complicates computation and expands parameters hindering streamlined model deployment. the selection of region features requires a pretrained object detector neglecting the model\u201a\u00e4\u00f4s ease of use for new scenarios and data. in this article we propose a dual feature adaptive shared transformer network capitalizing on the merits of grid and shallow patch features while circumventing the extra complexity from dual channels. specifically we eschew complex features such as region features to facilitate straightforward dataset compilation and expedite inference. we propose an adaptive shared transformer block  ast  to conserve parameters and diminish the model\u201a\u00e4\u00f4s flops. a gating mechanism is employed to adaptively compute the importance of each feature thereby obtaining stronger visual features. since using flattening grid features before a transformer often leads to a loss of crucial spatial information we incorporate the learning of relative geometric information based on grid features into our proposed method. our analysis of various feature fusion techniques reveals that the ast approach outperforms its counterparts in terms of flops and model size while still achieving high performance. extensive experiments on different datasets indicate that our model demonstrates competitive performance on mscoco and outperforms state of the art models on small scale datasets.", "Doi": "10.1109/TIM.2024.3353830", "Key Words": ["deep learning", "image captioning", "transformer"]}