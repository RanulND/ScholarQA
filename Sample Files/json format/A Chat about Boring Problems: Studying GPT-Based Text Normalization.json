{"Title": "A Chat about Boring Problems: Studying GPT-Based Text Normalization", "Doi": "10.1109/ICASSP48485.2024.10447169", "Authors": ["y. zhang", "t. m. bartley", "m. graterol-fuenmayor", "v. lavrukhin", "e. bakhturina", "b. ginsburg"], "Key Words": ["text-normalization", "gpt", "large-language-models", "in-context learning", "finite state automata", "text-to-speech"], "Abstract": "text normalization   the conversion of text from written to spoken form   is traditionally assumed to be an ill formed task for language modeling. in this work we argue otherwise. we empirically show the capacity of large language models  large language model  for text normalization in few shot scenarios. combining self consistency reasoning with linguistic informed prompt engineering we find large language model based text normalization to achieve error rates approximately 40% lower than production level normalization systems. further upon error analysis we note key limitations in the conventional design of text normalization tasks. we create a new taxonomy of text normalization errors and apply it to results from gpt 3.5 turbo and gpt 4.0. through this new framework we identify strengths and weaknesses of large language model based tn opening opportunities for future work.", "Pub Date": "2024-03-18"}