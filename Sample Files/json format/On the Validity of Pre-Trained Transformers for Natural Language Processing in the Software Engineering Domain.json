{"Title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain", "Doi": "10.1109/TSE.2022.3178469", "Authors": ["j. von der mosel", "a. trautsch", "s. herbold"], "Key Words": ["natural language processing", "software engineering", "transformers"], "Abstract": "transformers are the current state of the art of natural language processing in many domains and are using traction within software engineering research as well. such models are pre trained on large amounts of data usually from the general domain. however we only have a limited understanding regarding the validity of transformers within the software engineering domain i.e. how good such models are at understanding words and sentences within a software engineering context and how this improves the state of the art. within this article we shed light on this complex but crucial issue. we compare bert transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions  their vocabulary their ability to understand which words are missing and their performance in classification tasks. our results show that for tasks that require understanding of the software engineering context pre training with software engineering data is valuable while general domain models are sufficient for general language understanding also within the software engineering domain.", "Pub Date": "2023-04-18"}