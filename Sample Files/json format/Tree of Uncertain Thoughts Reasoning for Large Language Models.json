{"Title": "Tree of Uncertain Thoughts Reasoning for Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10448355", "Authors": ["s. mo", "m. xin"], "Key Words": ["large language models", "tree of thoughts", "uncertainty estimation"], "Abstract": "while the recently introduced tree of thoughts  tot  has heralded advancements in allowing large language models  large language model  to reason through foresight and backtracking for global decision making it has overlooked the inherent local uncertainties in intermediate decision points or \"thoughts\". these local uncertainties intrinsic to large language model given their potential for diverse responses remain a significant concern in the reasoning process. addressing this pivotal gap we introduce the tree of uncertain thoughts  tout  \u201a\u00e4\u00ee a reasoning framework tailored for large language model. our tout effectively leverages monte carlo dropout to quantify uncertainty scores associated with large language model\u201a\u00e4\u00f4 diverse local responses at these intermediate steps. by marrying this local uncertainty quantification with global search algorithms tout enhances the model\u201a\u00e4\u00f4s precision in response generation. we substantiate our approach with rigorous experiments on two demanding planning tasks  game of 24 and mini crosswords. the empirical evidence underscores tout\u201a\u00e4\u00f4s superiority over both tot and chain of thought prompting methods.", "Pub Date": "2024-03-18"}