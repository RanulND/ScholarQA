{"Title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge", "Doi": "10.1109/WACV56688.2023.00121", "Authors": ["s. ravi", "a. chinchure", "l. sigal", "r. liao", "v. shwartz"], "Key Words": ["algorithms: vision + language and/or other modalities", "image recognition and understanding (object detection", "categorization", "segmentation", "scene modeling", "visual reasoning)"], "Abstract": "there has been a growing interest in solving visual question answering  visual question answering  tasks that require the model to reason beyond the content present in the image. in this work we focus on questions that require commonsense reasoning. in contrast to previous methods which inject knowledge from static knowledge bases we investigate the incorporation of contextualized knowledge using commonsense transformer  comet  an existing knowledge model trained on human curated knowledge bases. we propose a method to generate select and encode external commonsense knowledge alongside visual and textual cues in a new pre trained vision language commonsense transformer model vlc bert. through our evaluation on the knowledge intensive ok visual question answering and a okvqa datasets we show that vlc bert is capable of outperforming existing models that utilize static knowledge bases. furthermore through a detailed analysis we explain which questions benefit and which don\u201a\u00e4\u00f4t from contextualized commonsense knowledge from comet. code  https //github.com aditya10/vlc bert", "Pub Date": "2023-02-06"}