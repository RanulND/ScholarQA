{"Title": "Read-All-in-Once (RAiO): Multi-Layer Contextual Architecture for Long-Text Machine Reading Comprehension", "Doi": "10.1109/ACCESS.2023.3298100", "Authors": ["t. -a. phan", "j. j. jung", "k. -h. n. bui"], "Key Words": ["natural language processing", "long-text machine reading comprehension", "question-answering system"], "Abstract": "machine reading comprehension  mrc  is a cutting edge technology in natural language processing  nlp  which focuses on teaching machines to read and understand the meaning of texts based on the emergence of large scale datasets and neural network models. recently with the successful development of pre trained transformer models  e.g. bert  mrc has advanced significantly surpassing human parity in several public datasets and being applied in various nlp tasks  e.g. qa systems . nevertheless long document mrc is still a remain challenge since the transformer based models are limited by the input length. for instance several well known pre trained language models such as bert and roberta are limited by 512 tokens. this study aims to provide a new simple approach for long document mrc. specifically recent state of the art models follow the architecture with two crucial stages for reading long texts in order to enable local and global context representations. in this study we present a new architecture that is able to enrich the global information of the context with one stage by exploiting the interaction of different levels of semantic units of the context  i.e. sentence and word level . therefore we name the proposed model as raio  read all in once  approach. for the experiment we evaluate raio on two benchmark long document mrc datasets such as newsqa and nlquad. accordingly the experiment shows promising results of the proposed approach compared with strong baselines in this research field.", "Pub Date": "2023-08-01"}