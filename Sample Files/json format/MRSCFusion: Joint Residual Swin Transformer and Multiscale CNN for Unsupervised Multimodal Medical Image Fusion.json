{"Title": "MRSCFusion: Joint Residual Swin Transformer and Multiscale CNN for Unsupervised Multimodal Medical Image Fusion", "Authors": ["x. xie", "x. zhang", "s. ye", "d. xiong", "l. ouyang", "b. yang", "h. zhou", "y. wan"], "Pub Date": "2023-10-03", "Abstract": "it is crucial to integrate the complementary information of multimodal medical images for enhancing the image quality in clinical diagnosis. convolutional neural network  cnn  based deep learning methods have been widely used for image fusion due to their strong modeling ability  however cnns fail to build the long range dependencies in an image which limits the fusion performance. to address this issue in this work we develop a new unsupervised multimodal medical image fusion framework that combines the swin transformer and cnn. the proposed model follows a two stage training strategy where an autoencoder is trained to extract multiple deep features and reconstruct fused images. a novel residual swin convolution fusion  rscf  module is designed to fuse the multiscale features. specifically it consists of a global residual swin transformer branch for capturing the global contextual information as well as a local gradient residual dense branch for capturing the local fine grained information. to further effectively integrate more meaningful information and ensure the visual quality of fused images we define a joint loss function including content loss and intensity loss to constrain the rscf fusion module  moreover we introduce an adaptive weight block  awb  to assign learnable weights in the loss function which can control the information preservation degree of source images. in such cases abundant texture features from magnetic resonance imaging  mri  images and appropriate intensity information from functional images can be well preserved simultaneously. extensive comparisons have been conducted between the proposed model and other state of the art fusion methods on ct mri pet mri and spect mri image fusion tasks. both qualitative and quantitative comparisons have demonstrated the superiority of our model.", "Doi": "10.1109/TIM.2023.3317470", "Key Words": ["end-to-end network", "image fusion", "multimodal medical images", "swin transformer", "unsupervised learning"]}