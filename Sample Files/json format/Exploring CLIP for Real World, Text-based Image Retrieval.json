{"Title": "Exploring CLIP for Real World, Text-based Image Retrieval", "Doi": "10.1109/AIPR60534.2023.10440710", "Authors": ["m. sultan", "l. jacobs", "a. stylianou", "r. pless"], "Key Words": ["deep learning", "image retrieval", "human computer interaction"], "Abstract": "we consider the ability of clip features to support text driven image retrieval. traditional image based queries sometimes misalign with user intentions due to their focus on irrelevant image components. to overcome this we explore the potential of text based image retrieval specifically using contrastive language image pretraining  clip  models. clip models trained on large datasets of image caption pairs offer a promising approach by allowing natural language descriptions for more targeted queries. we explore the effectiveness of text driven image retrieval based on clip features by evaluating the image similarity for progressively more detailed queries. we find that there is a sweet spot of detail in the text that gives best results and find that words describing the \"tone\" of a scene  such as messy dingy  are quite important in maximizing text image similarity.", "Pub Date": "2024-02-22"}