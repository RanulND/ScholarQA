{"Title": "Distributed Semi-Supervised Learning With Consensus Consistency on Edge Devices", "Doi": "10.1109/TPDS.2023.3340707", "Authors": ["h. -r. chen", "l. yang", "x. zhang", "j. shen", "j. cao"], "Key Words": ["consistency regularization", "distributed machine learning", "semi-supervised learning"], "Abstract": "distributed learning has been increasingly studied in edge computing enabling edge devices to learn a model collaboratively without exchanging their private data. however existing approaches assume the private data owned by edge devices are all labeled while the reality is that massive private data are unlabeled and remain to be utilized which leads to suboptimal performance. to overcome this limitation we study a new practical problem distributed semi supervised learning  dssl  to learn models collaboratively with mixed private labeled and unlabeled data on each device. we also propose a novel method distmatch that exploits private unlabeled data by self training on each device with the help of models from neighboring devices. distmatch generates pseudo labels for unlabeled data by properly averaging the predictions of these received models. furthermore to avoid self training with wrong pseudo labels distmatch proposes a consensus consistency loss to filter pseudo labels with high consensus and force the output of the trained model to be consistent with these pseudo labels. extensive evaluation results via our self developed testbed indicate the proposed method outperforms all baselines on commonly used image classification benchmark datasets.", "Pub Date": "2024-01-02"}