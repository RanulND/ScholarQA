{"Title": "CLIPPING: Distilling CLIP-Based Models with a Student Base for Video-Language Retrieval", "Doi": "10.1109/CVPR52729.2023.01820", "Authors": ["r. pei", "j. liu", "w. li", "b. shao", "s. xu", "p. dai", "j. lu", "y. yan"], "Key Words": ["multi-modal learning"], "Abstract": "pre training a vision language model and then fine tuning it on downstream tasks have become a popular paradigm. however pre trained vision language models with the transformer architecture usually take long inference time. knowledge distillation has been an efficient technique to transfer the capability of a large model to a small one while maintaining the accuracy which has achieved remarkable success in natural language processing. however it faces many problems when applying kd to the multi modality applications. in this paper we propose a novel knowledge distillation method named clipping11in this paper clipping means cutting something to make it smaller through distilling. where the plentiful knowledge of a large teacher model that has been fine tuned for video language tasks with the powerful pre trained clip can be effectively transferred to a small student only at the fine tuning stage. especially a new layer wise alignment with the student as the base is proposed for knowledge distillation of the intermediate layers in clipping which enables the student layers to be the bases of the teacher and thus allows the student to fully absorb the knowledge of the teacher. clipping with mobilevit v2 as the vision encoder without any vision language pre training achieves 88.1% 95.3% of the performance of its teacher on three video language retrieval benchmarks with its vision encoder being 19.5x smaller. clipping also significantly outperforms a state of the art small baseline  all in one b  on the msr vtt dataset obtaining relatively 7.4% performance gain with 29% fewer parameters and 86.9% fewer flops. moreover clipping is comparable or even superior to many large pre training models.", "Pub Date": "2023-08-22"}