{"Title": "On Data Scaling in Masked Image Modeling", "Doi": "10.1109/CVPR52729.2023.00999", "Authors": ["z. xie", "z. zhang", "y. cao", "y. lin", "y. wei", "q. dai", "h. hu"], "Key Words": ["self-supervised or unsupervised representation learning"], "Abstract": "scaling properties have been one of the central issues in self supervised pre training especially the data scalability which has successfully motivated the large scale self supervised pre trained language models and endowed them with significant modeling capabilities. however scaling properties seem to be unintentionally neglected in the recent trending studies on masked image modeling  mim  and some arguments even suggest that mim cannot benefit from large scale data. in this work we try to break down these preconceptions and systematically study the scaling behaviors of mim through extensive experiments with data ranging from 10% of imagenet 1k to full imagenet 22k model parameters ranging from 49 million to one billion and training length ranging from 125k to 500k iterations. and our main findings can be summarized in two folds  1  masked image modeling remains demanding large scale data in order to scale up computes and model parameters  2  masked image modeling cannot benefit from more data under a non overfitting scenario which diverges from the previous observations in self supervised pre trained language models or supervised pre trained vision models. in addition we reveal several intriguing properties in mim such as high sample efficiency in large mim models and strong correlation between pre training validation loss and transfer performance. we hope that our findings could deepen the understanding of masked image modeling and facilitate future developments on largescale vision models. code and models will be available at https //github.com microsoft/simmim.", "Pub Date": "2023-08-22"}