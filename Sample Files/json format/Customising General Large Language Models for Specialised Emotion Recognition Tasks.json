{"Title": "Customising General Large Language Models for Specialised Emotion Recognition Tasks", "Doi": "10.1109/ICASSP48485.2024.10447044", "Authors": ["l. peng", "z. zhang", "t. pang", "j. han", "h. zhao", "h. chen", "b. w. schuller"], "Key Words": ["emotion recognition", "large language model", "prompt tuning", "low-rank adaptation"], "Abstract": "the advent of large language models  large language model  has gained tremendous attention over the past year. previous studies have shown the astonishing performance of large language model not only in other tasks but also in emotion recognition in terms of accuracy universality explanation robustness few zero shot learning and others. leveraging the capability of large language model inevitably becomes an essential solution for emotion recognition. to this end we further comprehensively investigate how large language model perform in linguistic emotion recognition if we concentrate on this specific task. specifically we exemplify a publicly available and widely used large language model \u201a\u00e4\u00ec chat general language model and customise it for our target by using two different model adaptation techniques i.e. deep prompt tuning and low rank adaptation. the experimental results obtained on six widely used datasets present that the adapted large language model can easily outperform other state of the art but specialised deep models. this indicates the strong transferability and feasibility of large language model in the field of emotion recognition.", "Pub Date": "2024-03-18"}