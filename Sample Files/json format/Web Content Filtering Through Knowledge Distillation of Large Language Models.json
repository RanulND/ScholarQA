{"Title": "Web Content Filtering Through Knowledge Distillation of Large Language Models", "Doi": "10.1109/WI-IAT59888.2023.00058", "Authors": ["t. v\u221a\u2202r\u221a\u2202s", "s. p. bergeron", "k. berlin"], "Key Words": ["web content filtering", "machine learning", "large language models"], "Abstract": "we introduce a state of the art approach for url categorization that leverages the power of large language models  llms  to address the primary objectives of web content filtering  safeguarding organizations from legal and ethical risks limiting access to high risk or suspicious websites and fostering a secure and professional work environment. our method utilizes llms to generate accurate classifications and then employs established knowledge distillation techniques to create smaller more specialized student models tailored for web content filtering. distillation results in a student model with a 9 % accuracy rate improvement in classifying websites sourced from customer telemetry data collected by a large security vendor into 30 distinct content categories based on their urls surpassing the current state of the art approach. our student model matches the performance of the teacher llm with 175 times less parameters allowing the model to be used for in line scanning of large volumes of urls and requires 3 orders of magnitude less manually labeled training data than the current state of the art approach. depending on the specific use case the output generated by our approach can either be directly returned or employed as a pre filter for more resource intensive operations involving website images or html.", "Pub Date": "2023-12-19"}