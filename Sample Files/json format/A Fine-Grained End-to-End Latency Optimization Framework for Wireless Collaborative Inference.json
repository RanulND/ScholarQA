{"Title": "A Fine-Grained End-to-End Latency Optimization Framework for Wireless Collaborative Inference", "Authors": ["l. mu", "z. li", "w. xiao", "r. zhang", "p. wang", "t. liu", "g. min", "k. li"], "Pub Date": "2024-02-05", "Abstract": "mobile devices are becoming increasingly capable of delivering intelligent services by leveraging deep learning architectures such as deep neural networks  dnns . however due to the compute intensive nature of these tasks mobile devices often struggle to handle them independently leading to the exploration of collaborative inference as a promising solution for achieving low latency mobile intelligence. despite its potential benefits many challenges need to be addressed in realizing the full potential of inference acceleration. this article presents a collaborative device edge inference optimization framework as a promising solution to inference acceleration. the framework comprises fundamental modules including the parameters generator  pg  accuracy predictor  ap  delay calculator  dc  and optimizer  op  which are specifically designed to identify the optimal set of parameters for model compression dnn partition and feature compression. to illustrate its implementation an example of a deep cnn network is introduced and the collaborative inference latency optimization is formulated as a mixed integer programming problem. the implementation of a specific algorithm instance using a quantum inspired op within the optimization framework is then presented. a multiple regression based inference accuracy prediction model is proposed to maintain inference accuracy close to that of the original network while significantly reducing the time consumption during the offline phase. through various simulation scenarios involving inference tasks of alexnet and regnet on cifar 10 incorporating diverse hardware computation specifications and wireless communication link conditions the proposed framework demonstrates superior performance in terms of inference acceleration compared to the compared methods.", "Doi": "10.1109/JIOT.2023.3307820", "Key Words": ["collaborative inference", "edge intelligence", "latency optimization"]}