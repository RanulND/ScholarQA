{"Title": "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification", "Doi": "10.1109/CVPR52729.2023.01839", "Authors": ["y. yang", "a. panagopoulou", "s. zhou", "d. jin", "c. callison-burch", "m. yatskar"], "Key Words": ["explainable computer vision"], "Abstract": "concept bottleneck models  cbm  are inherently interpretable models that factor model decisions into humanreadable concepts. they allow people to easily understand why a model is failing a critical feature for high stakes applications. cbms require manually specified concepts and often under perform their black box counterparts preventing their broad adoption. we address these shortcomings and are first to show how to construct high performance cbms without manual specification of similar accuracy to black box models. our approach language guided bottlenecks  labo  leverages a language model gpt 3 to define a large space of possible bottlenecks. given a problem domain labo uses gpt-3 to produce factual sentences about categories to form candidate concepts. labo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. ultimately gpt 3 sentential concepts can be aligned to images using clip to form a bottleneck layer. experiments demonstrate that labo is a highly effective prior for concepts important to visual recognition. in the evaluation with 11 diverse datasets labo bottlenecks excel at few shot classification  they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. overall labo demonstrates that inherently interpretable models can be widely applied at similar or better performance than black box approaches.11code and data are available at https //github.com yueyang1996/labo", "Pub Date": "2023-08-22"}