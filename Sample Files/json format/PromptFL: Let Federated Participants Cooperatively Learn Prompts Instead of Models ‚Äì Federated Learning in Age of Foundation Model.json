{"Title": "PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models \u201a\u00c4\u00ec Federated Learning in Age of Foundation Model", "Doi": "10.1109/TMC.2023.3302410", "Authors": ["t. guo", "s. guo", "j. wang", "x. tang", "w. xu"], "Key Words": ["edge computing", "prompt federated learning", "vision-language model"], "Abstract": "quick global aggregation of effective distributed parameters is crucial to federated learning  fl  which requires adequate bandwidth for parameters communication and sufficient user data for local training. otherwise fl may cost excessive training time for convergence and produce inaccurate models. in this paper we propose a brand new fl framework promptfl that replaces the federated model training with the federated prompt training i.e. let federated participants train prompts instead of a shared model to simultaneously achieve the efficient global aggregation and local training on insufficient data by exploiting the power of foundation models  fm  in a distributed way. promptfl ships an off the shelf fm i.e. clip to distributed clients who would cooperatively train shared soft prompts based on very few local data. since promptfl only needs to update the prompts instead of the whole model both the local training and the global aggregation can be significantly accelerated. and fm trained over large scale data can provide strong adaptation capability to distributed users tasks with the trained soft prompts. we empirically analyze the promptfl via extensive experiments and show its superiority in terms of system feasibility user privacy and performance.", "Pub Date": "2024-04-04"}