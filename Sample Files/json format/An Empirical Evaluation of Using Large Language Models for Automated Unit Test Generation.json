{"Title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation", "Doi": "10.1109/TSE.2023.3334955", "Authors": ["m. sch\u221a\u00a7fer", "s. nadi", "a. eghbali", "f. tip"], "Key Words": ["test generation", "javascript", "language models"], "Abstract": "unit tests play a key role in ensuring the correctness of software. however manually creating unit tests is a laborious task motivating the need for automation. large language models  large language model  have recently been applied to various aspects of software development including their suggested use for automated generation of unit tests but while requiring additional training or few shot learning on examples of existing tests. this paper presents a large scale empirical evaluation on the effectiveness of large language model for automated unit test generation without requiring additional training or manual effort. concretely we consider an approach where the large language model is provided with prompts that include the signature and implementation of a function under test along with usage examples extracted from documentation. furthermore if a generated test fails our approach attempts to generate a new test that fixes the problem by re prompting the model with the failing test and error message. we implement our approach in testpilot an adaptive large language model based test generation tool for javascript that automatically generates unit tests for the methods in a given project application programming interface. we evaluate testpilot using openai gpt3.5 turbo large language model on 25 npm packages with a total of 1684 application programming interface functions. the generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. in contrast the state of the feedback directed javascript test generation technique nessie achieves only 51.3% statement coverage and 25.6% branch coverage. furthermore experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. we also find that 92.8% of testpilot generated tests have $\\leq$\u201a\u00e2\u00a7 50% similarity with existing tests  as measured by normalized edit distance  with none of them being exact copies. finally we run testpilot with two additional large language model openai older code cushman 002 large language model and starcoder an large language model for which the training process is publicly documented. overall we observed similar results with the former  68.2% median statement coverage  and somewhat worse results with the latter  54.0% median statement coverage  suggesting that the effectiveness of the approach is influenced by the size and training set of the large language model but does not fundamentally depend on the specific model.", "Pub Date": "2024-01-08"}