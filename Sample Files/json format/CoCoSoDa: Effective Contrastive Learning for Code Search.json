{"Title": "CoCoSoDa: Effective Contrastive Learning for Code Search", "Doi": "10.1109/ICSE48619.2023.00185", "Authors": ["e. shi", "y. wang", "w. gu", "l. du", "h. zhang", "s. han", "d. zhang", "h. sun"], "Key Words": ["code search", "contrastive learning", "soft data augmentation", "momentum mechanism"], "Abstract": "code search aims to retrieve semantically relevant code snippets for a given natural language query. recently many approaches employing contrastive learning have shown promising results on code representation learning and greatly improved the performance of code search. however there is still a lot of room for improvement in using contrastive learning for code search. in this paper we propose cocosoda to effectively utilize contrastive learning for code search via two key factors in contrastive learning  data augmentation and negative samples. specifically soft data augmentation is to dynamically masking or replacing some tokens with their types for input sequences to generate positive samples. momentum mechanism is used to generate large and consistent representations of negative samples in a mini batch through maintaining a queue and a momentum encoder. in addition multimodal contrastive learning is used to pull together representations of code query pairs and push apart the unpaired code snippets and queries. we conduct extensive experiments to evaluate the effectiveness of our approach on a large scale dataset with six programming languages. experimental results show that   1  cocosoda outperforms 18 baselines and especially exceeds codebert graphcodebert and unixcoder by 13.3% 10.5% and 5.9% on average mrr scores respectively.  2  the ablation studies show the effectiveness of each component of our approach.  3  we adapt our techniques to several different pre trained models such as roberta codebert and graphcodebert and observe a significant boost in their performance in code search.  4  our model performs robustly under different hyper parameters. furthermore we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.", "Pub Date": "2023-07-14"}