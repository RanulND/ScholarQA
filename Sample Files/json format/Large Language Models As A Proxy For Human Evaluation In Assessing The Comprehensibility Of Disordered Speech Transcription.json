{"Title": "Large Language Models As A Proxy For Human Evaluation In Assessing The Comprehensibility Of Disordered Speech Transcription", "Doi": "10.1109/ICASSP48485.2024.10447177", "Authors": ["k. tomanek", "j. tobin", "s. venugopalan", "r. cave", "k. seaver", "j. r. green", "r. heywood"], "Key Words": ["automatic speech recognition", "model quality evaluation", "speech disorders"], "Abstract": "automatic speech recognition  asr  systems despite significant advances in recent years still have much room for improvement particularly in the recognition of disordered speech. even so erroneous transcripts from asr models can help people with disordered speech be better understood especially if the transcription doesn\u201a\u00e4\u00f4t significantly change the intended meaning. evaluating the efficacy of asr for this use case requires a methodology for measuring the impact of transcription errors on the intended meaning and comprehensibility. human evaluation is the gold standard for this but it can be laborious slow and expensive. in this work we tune and evaluate large language models for this task and find them to be a much better proxy for human evaluators than other metrics commonly used. we further present a case study using the presented approach to assess the quality of personalized asr models to make model deployment decisions and correctly set user expectations for model quality as part of our trusted tester program.", "Pub Date": "2024-03-18"}