{"Title": "Memory Efficient with Parameter Efficient Fine-Tuning for Code Generation Using Quantization", "Doi": "10.1109/IMCOM60618.2024.10418267", "Authors": ["purnawansyah", "z. ali", "h. darwis", "l. b. ilmawan", "s. r. jabir", "a. r. manga"], "Key Words": ["fine tuning", "large language models", "quantization", "text generation"], "Abstract": "code large language models  code llms  such as code llama and starcoder have exhibited outstanding proficiency in tasks required for specific tasks like code generation. several conducted research to similar task by utilizing fine tuning techniques from state of the art base models for more specific related task. however due to the cost limitations and limited computing resources performing fine tuning from large language models is excessively high. in this study we utilized low rank adaptation  lora  for base large language models such as llama 2 and phi 1.5 which uses trainable rank decomposition matrices. furthermore we injected quantized lora  qlora  to help reduce memory usage while training the model and analyzed the contribution to gpu usage. notably our findings reveal that employing these techniques for fine tuning on small datasets yields cost effective and viable alternatives for language related tasks showcasing competitive performance compared to state of the art models like codellama 7b substantiated by lower train loss achieved in our experiments.", "Pub Date": "2024-02-12"}