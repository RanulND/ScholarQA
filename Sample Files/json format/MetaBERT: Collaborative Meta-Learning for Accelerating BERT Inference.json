{"Title": "MetaBERT: Collaborative Meta-Learning for Accelerating BERT Inference", "Doi": "10.1109/CSCWD57460.2023.10152853", "Authors": ["y. xu", "f. yuan", "c. cao", "x. zhang", "m. su", "d. wang", "y. liu"], "Key Words": ["inference acceleration", "early exit", "meta-learning", "bert"], "Abstract": "early exit methods are used to accelerate inference in pre trained language models and maintain competitive performance on resource constrained devices. however existing methods for training early exit classifiers suffer from the problem of poor classifier representations in different layers leading to difficulties in adapting to diverse natural language processing tasks. to address this issue we propose metabert  collaborative meta learning for accelerating bert inference. the main goal of metabert is to train early exit classifiers through collaborative meta learning in which case few gradient updates can be quickly adapted to new tasks. moreover this novel meta training approach produces good generalization performance thus achieving an effective balance between the inference result and efficiency. extensive experimental results show that our approach outperforms previous training methods by a large margin and achieves state of the art results compared to other competitive models.", "Pub Date": "2023-06-22"}