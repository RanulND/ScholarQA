{"Title": "VindLU: A Recipe for Effective Video-and-Language Pretraining", "Doi": "10.1109/CVPR52729.2023.01034", "Authors": ["f. cheng", "x. wang", "j. lei", "d. crandall", "m. bansal", "g. bertasius"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "the last several years have witnessed remarkable progress in video and language  vidl  understanding. however most modern vidl approaches use complex and specialized model architectures and sophisticated pretraining protocols making the reproducibility analysis and comparisons of these frameworks difficult. hence instead of proposing yet another new vidl model this paper conducts a thorough empirical study demystifying the most important factors in the vidl model design. among the factors that we investigate are  i  the spatiotemporal architecture design  ii  the multimodal fusion schemes  iii  the pretraining objectives  iv  the choice of pretraining data  v  pretraining and finetuning protocols and  vi  dataset and model scaling. our empirical study reveals that the most important design factors include  temporal modeling video to text multimodal fusion masked modeling objectives and joint training on images and videos. using these empirical insights we then develop a step by step recipe dubbed vindlu for effective vidl pretraining. our final model trained using our recipe achieves comparable or better than state of the art results on several vidl tasks without relying on external clip pretraining. in particular on the text to video retrieval task our approach obtains 61.2% on didemo and 55.0% on activitynet outperforming current sota by 7.8% and 6.1% respectively. furthermore our model also obtains state of the art video question answering results on activitynet qa msrvtt qa msrvtt mc and tvqa. our code and pretrained models are publicly available at  https //github.com klauscc/vindlu.", "Pub Date": "2023-08-22"}