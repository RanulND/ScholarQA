{"Title": "Improved Knowledge Distillation via Teacher Assistants for Sentiment Analysis", "Doi": "10.1109/SSCI52147.2023.10371965", "Authors": ["x. dong", "o. huang", "p. thulasiraman", "a. mahanti"], "Key Words": ["sentiment analysis", "bert", "knowledge distillation", "teacher assistant", "teacher-student network"], "Abstract": "bidirectional encoder representations from transformers  bert  has achieved state of the art results on various nlp tasks. however the size of bert makes application in time sensitive scenarios challenging. there are lines of research compressing bert through different techniques and knowledge distillation  kd  is the most popular. nevertheless more recent studies challenge the effectiveness of kd from an arbitrarily large teacher model. so far research on the negative impact of the teacher student gap on the effectiveness of knowledge transfer has been confined mainly to computer vision. additionally those researches were limited to distillations between teachers and students with similar model architectures. to fill the gap in the literature we implemented a teacher assistant  ta  model lying between a fine tuned bert model and non transformer based machine learning models including cnn and bi lstm for sentiment analysis. we have shown that teaching assistant facilitated kd outperformed traditional kd while maintaining a competitive inference efficiency. in particular a well designed cnn model could retain 97% of bert performance while being 1410x smaller for sentiment analysis. we have also found that bert is not necessarily a better teacher model than non transformer based neural networks.", "Pub Date": "2024-01-01"}