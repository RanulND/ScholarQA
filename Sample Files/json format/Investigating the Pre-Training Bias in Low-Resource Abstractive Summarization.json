{"Title": "Investigating the Pre-Training Bias in Low-Resource Abstractive Summarization", "Doi": "10.1109/ACCESS.2024.3379139", "Authors": ["d. chernyshev", "b. dobrov"], "Key Words": ["abstractive summarization", "attention mechanism", "low-resource text processing", "pre-trained language models", "model probing"], "Abstract": "recent advances in low resource abstractive summarization were largely made through the adoption of specialized pre training pseudo summarization that integrates the content selection knowledge through various centrality based sentence recovery tasks. however despite the substantial results there are several cases where the predecessor general purpose pre trained language model bart outperforms the summarization specialized counterparts in both few shot and fine tuned scenarios. in this work we investigate these performance irregularities and shed some light on the effect of pseudo summarization pre training in low resource settings. we benchmarked five pre trained abstractive summarization models on five datasets of diverse domains and analyzed their behavior in terms of extractive intuition and attention patterns. despite that all models exhibit extractive behavior some lack the prediction confidence to copy longer text fragments and have a misaligned attention distribution with the structure of the real world texts. the latter happens to be the major factor of underperformance in fiction news and scientific articles domains as the better initial attention alignment of bart leads to the best benchmark results in all few shot settings. a further examination reveals that bart summarization capabilities are the side effect of the combination of sentence permutation task and specificities of the pre training dataset. based on the discovery we introduce pegasus sp an improved pre trained abstractive summarization model that unifies pseudo summarization with sentence permutation. the new model outperforms the existing counterparts in low resource settings and demonstrates superior adaptability. additionally we show that all pre trained summarization models benefit from data wise attention correction achieving up to 10% relative rouge improvement on model data pairs with the largest distribution discrepancies.", "Pub Date": "2024-04-04"}