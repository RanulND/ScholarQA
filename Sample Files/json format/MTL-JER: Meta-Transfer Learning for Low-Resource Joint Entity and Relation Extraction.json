{"Title": "MTL-JER: Meta-Transfer Learning for Low-Resource Joint Entity and Relation Extraction", "Doi": "10.1109/NNICE58320.2023.10105766", "Authors": ["d. peng", "z. pei", "d. mo"], "Key Words": ["joint entity and relation extraction", "meta-learning", "low-resource"], "Abstract": "joint entity and relation extraction has achieved impressive advances in nlp such as document understanding and knowledge graph construction. the typical methods for entity and relation extraction typically break down the joint task into several smaller components or stages for ease of implementation but this leads to a loss of the interconnected knowledge in the triple. hence we propose to model the triple in one module jointly. furthermore the labeling of a joint entity and relation extraction tasks is costly and domain specific  therefore it is important to improve its performance on low resource data and domain adaption. to address this issue we suggest using two sources that are rich in information namely pre trained models on large data and multi domain text corpora. pretraining allows us to provide the model with the fundamental ability to perform joint entity and relationship extraction. second through meta learning on multi domain text we can improve the model generalization capabilities enabling it to perform well even with limited data. we present mtl jer a meta transfer learning method for joint entity and relation extraction in low resource settings in this paper. using exhaustive experiments on five datasets we prove that our model obtains optimal results.", "Pub Date": "2023-04-25"}