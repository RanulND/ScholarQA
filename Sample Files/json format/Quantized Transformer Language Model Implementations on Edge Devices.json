{"Title": "Quantized Transformer Language Model Implementations on Edge Devices", "Doi": "10.1109/ICMLA58977.2023.00104", "Authors": ["m. w. ur rahman", "m. m. abrar", "h. g. copening", "s. hariri", "s. shao", "p. satam", "s. salehi"], "Key Words": ["lot", "natural language processing", "machine learning", "bert", "reputation polarity", "social media", "embedded systems", "tinyml", "privacy"], "Abstract": "large scale transformer based models like the bidi rectional encoder representations from transformers  bert  are widely used for natural language processing  nlp  applications wherein these models are initially pre trained with a large corpus with millions of parameters and then fine tuned for a downstream nlp task. one of the major limitations of these large scale models is that they cannot be deployed on resource  constrained devices due to their large model size and increased inference latency. in order to overcome these limitations such large scale models can be converted to an optimized flatbuffer format tailored for deployment on resource constrained edge devices. herein we evaluate the performance of such flatbuffer transformed mobilebert models on three different edge devices fine tuned for reputation analysis of english language tweets in the rep lab 2013 dataset. in addition this study encompassed an evaluation of the deployed models wherein their latency performance and resource efficiency were meticulously assessed. our experiment results show that compared to the original bert large model the converted and quantized mobilebert models have 160x smaller footprints for a 4.1 % drop in accuracy while analyzing at least one tweet per second on edge devices. furthermore our study highlights the privacy preserving aspect of tinyml systems as all data is processed locally within a serverless environment.", "Pub Date": "2024-03-19"}