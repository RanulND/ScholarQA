{"Title": "Quantifying Domain Knowledge in Large Language Models", "Doi": "10.1109/CAI54212.2023.00091", "Authors": ["s. sayenju", "r. aygun", "b. franks", "s. johnston", "g. lee", "h. choi", "g. modgil"], "Key Words": ["natural language processing", "bert", "domain bias", "domain knowledge"], "Abstract": "transformer based large language models such as bert have demonstrated the ability to derive contextual information from the words surrounding it. however when these models are applied in specific domains such as medicine insurance or scientific disciplines publicly available models trained on general knowledge sources such as wikipedia it may not be as effective in inferring the appropriate context compared to domain specific models trained on specialized corpora. given the limited availability of training data for specific domains pre trained models can be fine tuned via transfer learning using relatively small domain specific corpora. however there is currently no standardized method for quantifying the effectiveness of these domain specific models in acquiring the necessary domain knowledge. to address this issue we explore hidden layer embeddings and introduce domain gain a measure to quantify the ability of a model to infer the correct context. in this paper we show how our measure could be utilized to determine whether words with multiple meanings are more likely to be associated with domain related meanings rather than their colloquial meanings.", "Pub Date": "2023-08-02"}