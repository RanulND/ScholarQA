{"Title": "A Fully-Integrated Energy-Scalable Transformer Accelerator Supporting Adaptive Model Configuration and Word Elimination for Language Understanding on Edge Devices", "Doi": "10.1109/ISLPED58423.2023.10244459", "Authors": ["z. ji", "h. wang", "m. wang", "w. -s. khwa", "m. -f. chang", "s. han", "a. p. chandrakasan"], "Key Words": ["hardware accelerators", "machine learning", "natural language processing", "transformers"], "Abstract": "efficient natural language processing on the edge is needed to interpret voice commands which have become a standard way to interact with devices around us. due to the tight power and compute constraints of edge devices it is important to adapt the computation to the hardware conditions. we present a transformer accelerator with a variable depth adder tree to support different model dimensions a supertransformer model from which sub transformers of various sizes can be sampled enabling adaptive model configuration and a dedicated word elimination unit to prune redundant tokens. we achieve up to 6.9\u221a\u00f3 scalability in network latency and energy between the largest and smallest sub transformers under the same operating conditions. word elimination can reduce network energy by 16% with a 14.5% drop in f1 score. at 0.68v and 80mhz processing a 32 length input with our custom 2 layer transformer model for intent detection and slot filling takes 0.61ms and 1.6\u0153\u00baj.", "Pub Date": "2023-09-19"}