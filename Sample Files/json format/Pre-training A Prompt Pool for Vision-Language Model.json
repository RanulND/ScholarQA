{"Title": "Pre-training A Prompt Pool for Vision-Language Model", "Doi": "10.1109/IJCNN54540.2023.10191454", "Authors": ["j. liu", "y. gu", "z. yang", "s. guo", "h. liu", "y. chen"], "Key Words": ["pre-trained vision-language model", "prompt learning", "image caption"], "Abstract": "pre trained vision language vl  model improves the performance in vision language tasks but requires a large amount of labeled data to apply the model to downstream tasks. it is challenging to obtain good results with limited training data. prompt tuning pt  which freezes pre train language models plms  and only tunes soft prompts provides an effective solution for adapting plms to downstream tasks. however pt performs comparably with full model tuning ft  when the data are sufficient and performs much worse in few shot settings primarily due to the initialization of soft prompts. in this paper we propose a new training framework pre train a prompt pool for vision language models called \u201a\u00e4\u00fap3vlm\u201a\u00e4\u00f9 to give better initialization to pt. the objective of p3vlm is to optimize prompts selected by the visual feature allowing the prompt to learn relevant knowledge in different data domains and store it in the prompt pool. then we use diverse prompts in the prompt pool as the initialization of pt instead of a single pre trained prompt. in the pre train stage the training samples are used to train a prompt pool by selecting prompts that best match the visual features. in the downstream datasets the model selects the best matching prompts as the initialization for pt freezes the plm and only tunes the prompts. extensive experiments show that our method obtains strong performance on two image caption datasets in both zero shot and few shot scenarios.", "Pub Date": "2023-08-02"}