{"Title": "A Novel Approach to Efficient Multilabel Text Classification: BERT-Federated Learning Fusion", "Doi": "10.1109/ICCIT60459.2023.10441264", "Authors": ["a. a. i. m. sadot", "m. maliha mehjabin", "a. mahafuz"], "Key Words": ["large language model", "bert", "text classification", "multi-label", "federated learning", "transfer learning"], "Abstract": "large language model  large language model  based transformers such as bidirectional encoder representations from transformers  bert  are currently gaining significant attention for various natural language processing  nlp  tasks such as machine translation classification and auto completion. these transformer models demonstrate substantial performance improvements for text classification tasks. multi label classification problems often require more computation than binary and multi class classification problems. also the computation requirements become more aggressive if large datasets are considered. federated learning  fl  offers a solution to train models in a distributed manner while preserving data privacy. this paper proposes a novel approach for building a machine learning model which deals with a sizeable textual dataset for multi label classification leveraging fl. fl has been used to train a compound model constructed by extending bidirectional encoder representations from transformers  bert  with a \"one dimensional convolutional neural network  1d cnn \". at first the experiment was conducted in a single machine  central  with the entire dataset. then the dataset was split into two groups and the same experiment was performed in a federated learning fashion  bert fl fusion . the fl setup considerably reduced the required computing power to derive an equivalent global model while increasing accuracy precision and f1 score and minimizing hamming loss.", "Pub Date": "2024-02-27"}