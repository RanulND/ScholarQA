{"Title": "Distributed Training of Large Language Models", "Doi": "10.1109/ICPADS60453.2023.00126", "Authors": ["f. zeng", "w. gan", "y. wang", "p. s. yu"], "Key Words": ["big data", "distributed training", "large language models", "data parallelism", "pipeline parallelism"], "Abstract": "the advent of large language models  llms  like chatgpt ushers in revolutionary opportunities that bring a vast variety of applications  such as healthcare law and education  across various disciplines. the research report pointed out that the model showcases excellent performance often closely related to the parameter scale of the model so how to train an llm? this is a question that everyone is more concerned about. at present there are several commonly used distributed training frameworks including megatron lm deepspeed etc. in this paper we first provide a brief introduction which refers to the current development status of llm. second we start from the status introducing the current common parallel strategies of llm distributed training. next we briefly introduce the underlying technologies and frameworks that llm relies on nowadays describing the current popular ones and types of large models. then we introduce the optimization techniques used in the llms. finally we summarize the problems and challenges encountered in the current llm training and describe the possible future development direction of llm.", "Pub Date": "2024-03-26"}