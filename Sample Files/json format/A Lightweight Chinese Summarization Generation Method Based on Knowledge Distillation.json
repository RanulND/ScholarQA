{"Title": "A Lightweight Chinese Summarization Generation Method Based on Knowledge Distillation", "Doi": "10.1109/ISPDS58840.2023.10235472", "Authors": ["s. liu", "y. xiao", "x. wang", "w. huang", "z. deng"], "Key Words": ["summarization generation", "natural language processing", "knowledge distillation", "model compression"], "Abstract": "in recent years large scale pretrained natural language processing models such as bert gpt3 have achieved good results in processing tasks. however in daily applications these large scale language models usually exist large model size or long running time and the model portability and application are not very convenient. to settle this problem we come up with a lightweight summarization generation method based on knowledge distillation  lw bert kd abbreviated as lbk model  to prove that complex neural network knowledge  taking bert model as an example  can be extracted to lightweight language processing models  taking bilstm model as an example  thus achieving the purpose of complex knowledge extraction and model compression. in this paper the bert model is regarded as the teacher model and the bilstm is regarded as the student model. the knowledge distillation technology is the transfer of knowledge from teacher models to student models so that the generated model is effective lightweight and easy to port. on the chinese abstract generation dataset the training effect of this model is significantly better than the basic transformer baseline. the experimental results show that if the training parameters are increased by 100 times the model achieves comparable results with complex preprocessing language models.", "Pub Date": "2023-09-05"}