{"Title": "Integrating Language Models with Symbolic Formulas for First-Order Logic Reasoning", "Doi": "10.1109/ICASSP48485.2024.10446308", "Authors": ["y. sheng", "l. li", "y. wang", "d. zeng"], "Key Words": ["neural-symbolic integration", "first-order logic reasoning", "graph neural networks"], "Abstract": "performing logical reasoning based on prior knowledge is a crucial human cognitive ability and has been a long standing objective in the field of artificial intelligence. large language models based on transformer architecture have been a common approach for logical reasoning over text. however the current language models often struggle to learn semantic information from logical expressions resulting in underwhelming performance on logical reasoning tasks. in this paper we propose a novel method to convert first order logic  fol  expressions to the form of a graph and integrate it with embeddings from language models to enhance their reasoning ability. the proposed method is designed to learn directly from fol formulas and is able to generalize to any scenarios involving logical expressions. experimental results demonstrate that the proposed method enhances the model\u201a\u00e4\u00f4s ability of learning logical semantic representations and thus it brings a significant improvement on the performance of complex reasoning tasks. the code is available at https //github.com fol gnn.", "Pub Date": "2024-03-18"}