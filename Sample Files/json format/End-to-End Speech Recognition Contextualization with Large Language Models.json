{"Title": "End-to-End Speech Recognition Contextualization with Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10446898", "Authors": ["e. lakomkin", "c. wu", "y. fathullah", "o. kalinli", "m. l. seltzer", "c. fuegen"], "Key Words": ["contextual biasing", "large language models", "speech recognition"], "Abstract": "in recent years large language models  llms  have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. in this paper we introduce a novel method for contextualizing speech recognition models incorporating llms. our approach casts speech recognition as a mixed modal language modeling task based on a pretrained llm. we use audio features along with optional text tokens for context to train the system to complete transcriptions in a decoder only fashion. as a result the system implicitly learns how to leverage unstructured contextual information during training. our empirical results demonstrate a significant improvement in performance with a 6% wer reduction when additional textual context is provided. moreover we find that our method performs competitively improving by 7.5% wer overall and 17% wer on rare words compared to a baseline contextualized rnn t system that has been trained on a speech dataset more than twenty five times larger. overall we demonstrate that by adding only a handful of trainable parameters via adapters we can unlock the contextualized speech recognition capability of the pretrained llm while maintaining the same text only input functionality.", "Pub Date": "2024-03-18"}