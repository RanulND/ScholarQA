{"Title": "Conditional Cross Correlation Network for Video Question Answering", "Doi": "10.1109/ICSC56153.2023.00011", "Authors": ["k. ouenniche", "r. tapu", "t. zaharia"], "Key Words": ["video question answering", "multimodal learning", "cross-correlation"], "Abstract": "video question answering  videoqa  is the process that aims at responding to questions expressed in natural language according to the semantic content of a given video. videoqa is a highly challenging task and demands a comprehensive understanding of the video document including the recognition of the various objects actions and activities involved together with the spatial temporal and causal relations between them. to tackle the challenge of videoqa most methods propose efficient techniques to fuse the representations between visual and textual modalities. in this paper we introduce a novel framework based on a conditional cross correlation network that learns multimodal contextualization with reduced computational and memory requirements. at the core of our approach we consider a cross correlation module designed to learn reciprocally constrained visual textual features combined with a lightweight transformer that fuses the intermodal contextualization between visual and textual modalities. we test the vulnerability of the composing elements of our pipeline using black box attacks. to this purpose we automatically generate semantic preserving rephrased questions. the ablation study conducted confirms the importance of each module in the framework. the experimental evaluation carried out on the msvd qa benchmark validates the proposed methodology with average accuracy scores of 43.58%. when compared with state of the art methods the proposed method yields gains in accuracy of more than 4%and achieves a 43.58% accuracy rate on the msvd qa data set.", "Pub Date": "2023-03-20"}