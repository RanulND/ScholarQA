{"Title": "4-bit CNN Quantization Method With Compact LUT-Based Multiplier Implementation on FPGA", "Authors": ["b. zhao", "y. wang", "h. zhang", "j. zhang", "y. chen", "y. yang"], "Pub Date": "2023-11-06", "Abstract": "to address the challenge of deploying convolutional neural networks  cnns  on edge devices with limited resources this article presents an effective 4 bit quantization scheme for cnn and proposes a dsp free multiplier solution for deploying quantized neural networks on field programmable gate array  fpga  devices. specifically we first introduce a threshold aware quantization  taq  method with a mixed rounding strategy to compress the scale of the model while maintaining the accuracy of the original full precision model. experimental results demonstrate that the proposed quantization method retains a high classification accuracy for 4 bit quantized cnn models. in addition we propose a compact lookup table based multiplier  clm  design that replaces numerical multiplication with a lookup table  lut  of precomputed 4 bit multiplication results leveraging lut6 resources instead of scarce dsp blocks to improve the scalability of fpga to implement multiplication intensive cnn algorithms. the proposed 4 bit clm only consumes 13 lut6 resources surpassing the existing lut based multipliers  lmuls  in terms of resource consumption. the proposed cnn quantization and clm multiplier scheme effectively save fpga resource consumption for fpga implementation on image classification tasks providing strong support for deep learning algorithms in unmanned systems industrial inspection and other relevant vision and measurement scenarios running on dsp constrained edge devices.", "Doi": "10.1109/TIM.2023.3324357", "Key Words": ["convolutional neural network (cnn)", "digital circuit", "field-programmable gate array (fpga)", "lookup table (lut)-based multiplier", "low-precision quantization"]}