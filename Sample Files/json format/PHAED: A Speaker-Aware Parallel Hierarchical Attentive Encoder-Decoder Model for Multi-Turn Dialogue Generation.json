{"Title": "PHAED: A Speaker-Aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-Turn Dialogue Generation", "Doi": "10.1109/TBDATA.2023.3316472", "Authors": ["z. wang", "m. jiang", "j. wang"], "Key Words": ["multi-turn dialogue generation", "open-domain dialogue", "speaker token", "natural language generation", "deep neural networks"], "Abstract": "this article presents a novel open domain dialogue generation model emphasizing the differentiation of speakers in multi turn conversations. differing from prior work that treats the conversation history as a long text we argue that capturing relative social relations among utterances  i.e. generated by either the same speaker or different persons  benefits the machine capturing fine grained context information from a conversation history to improve context coherence in the generated response. given that we propose a parallel hierarchical attentive encoder decoder  phaed  model that can effectively leverage conversation history by modeling each utterance with the awareness of its speaker and contextual associations with the same speaker previous messages. specifically to distinguish the speaker roles over a multi turn conversation  involving two speakers  we regard the utterances from one speaker as responses and those from the other as queries. after understanding queries via hierarchical encoder with inner query and inter query encodings transformer xl style decoder reuses the hidden states of previously generated responses to generate a new response. our empirical results with three large scale benchmarks show that phaed significantly outperforms baseline models on both automatic and human evaluations. furthermore our ablation study shows that dialogue models with speaker tokens can generally decrease the possibility of generating non coherent responses.", "Pub Date": "2024-01-16"}