{"Title": "A Study on the Integration of Pre-Trained SSL, ASR, LM and SLU Models for Spoken Language Understanding", "Doi": "10.1109/SLT54892.2023.10022399", "Authors": ["y. peng", "s. arora", "y. higuchi", "y. ueda", "s. kumar", "k. ganesan", "s. dalmia", "x. chang", "s. watanabe"], "Key Words": ["spoken language understanding", "low resource", "pre-trained models"], "Abstract": "collecting sufficient labeled data for spoken language understanding  slu  is expensive and time consuming. recent studies achieved promising results by using pre trained models in low resource scenarios. inspired by this we aim to ask  which  if any  pre training strategies can improve performance across slu benchmarks? to answer this question we employ four types of pre trained models and their combinations for slu. we leverage self supervised speech and language models  lm  pre trained on large quantities of un paired data to extract strong speech and text representations. we also explore using supervised models pre trained on larger external automatic speech recognition  asr  or slu corpora. we conduct extensive experiments on the slu evaluation  slue  benchmark and observe self supervised pre trained models to be more powerful with pre trained lm and speech models being most beneficial for the sentiment analysis and named entity recognition task respectively.11our code and models will be publicly available as part of the espnet slu toolkit.", "Pub Date": "2023-01-27"}