{"Title": "BERTGACN: Text Classification by Combining BERT and GCN and GAT", "Doi": "10.1109/NNICE58320.2023.10105710", "Authors": ["y. xue"], "Key Words": ["text classification", "bertgacn", "bert", "gcn", "gat"], "Abstract": "with the explosive growth of electronic data text has become one of the most important information carriers and it has become the most common form of data display. text classification involves many fields such as artificial intelligence and pattern recognition so it has important academic research significance and commercial value. how to make the computer accurately locate the effective information in the text and realize automatic classification has become one of the research hotspots. in this study we propose the bertgacn model which combines the large scale pre training model and the double tower structure graph neural networks  gnns  model for text classification tasks. the bertgacn model creates a heterogeneous graph for representing documents and words as nodes respectively. by jointly training the bidirectional encoder representations from transformers  bert  module graph convolutional network  gcn  module and graph attention network  gat  module the bertgacn model effectively learns the structural information of the graph and the association information between nodes which enhances its text classification ability. experiments show that the bertgacn model achieves better results than previous models in a wide range of text classification tasks.", "Pub Date": "2023-04-25"}