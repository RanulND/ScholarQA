{"Title": "EdgeAdaptor: Online Configuration Adaption, Model Selection and Resource Provisioning for Edge DNN Inference Serving at Scale", "Doi": "10.1109/TMC.2022.3189186", "Authors": ["k. zhao", "z. zhou", "x. chen", "r. zhou", "x. zhang", "s. yu", "d. wu"], "Key Words": ["edge intelligence", "edge computing", "dnn inference serving", "online optimization"], "Abstract": "the accelerating convergence of artificial intelligence and edge computing has sparked a recent wave of interest in edge intelligence. while pilot efforts focused on edge dnn inference serving for a single user or dnn application scaling edge dnn inference serving to multiple users and applications is however nontrivial. in this paper we propose an online optimization framework edgeadaptor for multi user and multi application edge dnn inference serving at scale which aims to navigate the three way trade off between inference accuracy latency and resource cost via jointly optimizing the application configuration adaption dnn model selection and edge resource provisioning on the fly. the underlying long term optimization problem is difficult since it is np hard and involves future uncertain information. to address these dual challenges we fuse the power of online optimization and approximate optimization into a joint optimization framework via i  decomposing the long term problem into a series of single shot fractional problems with a regularization technique and ii  rounding the fractional solution to a near optimal integral solution with a randomized dependent scheme. rigorous theoretical analysis derives a parameterized competition ratio of our online algorithms and extensive trace driven simulations verify that its empirical value is no larger than 1.4 in typical scenarios.", "Pub Date": "2023-08-31"}