{"Title": "Partial Sum Quantization for Reducing ADC Size in ReRAM-Based Neural Network Accelerators", "Authors": ["a. azamat", "f. asim", "j. kim", "j. lee"], "Pub Date": "2023-11-21", "Abstract": "while resistive random access memory  reram  crossbar arrays have the potential to significantly accelerate deep neural network  dnn  training through fast and low cost matrix\u201a\u00e4\u00ecvector multiplication peripheral circuits like analog to digital converters  adcs  create a high overhead. these adcs consume over half of the chip power and a considerable portion of the chip cost. to address this challenge we propose advanced quantization techniques that can significantly reduce the adc overhead of reram crossbar arrays  rcas . our methodology interprets adc as a quantization mechanism allowing us to scale the range of adc input optimally along with the weight parameters of a dnn resulting in multiple bit reduction in adc precision. this approach reduces adc size and power consumption by several times and it is applicable to any dnn type  binarized or multibit  and any rca size. additionally we propose ways to minimize the overhead of the digital scaler which is an essential part of our scheme and sometimes required. our experimental results using resnet 18 on the imagenet dataset demonstrate that our method can reduce the size of the adc by 32 times compared to isaac with only a minimal accuracy loss degradation of 0.24%. we also present evaluation results in the presence of reram nonideality  such as stuck at fault .", "Doi": "10.1109/TCAD.2023.3294461", "Key Words": ["analog-to-digital conversion (adc)", "convolutional neural network (cnn)", "in-memory computing accelerator", "memristor", "quantization"]}