{"Title": "Efficiently Transferring Pre-trained Language Model RoBERTa Base English to Hindi Using WECHSEL", "Doi": "10.1109/O-COCOSDA60357.2023.10482976", "Authors": ["r. k. bhukya", "a. chaturvedi", "h. bajaj", "u. shah", "s. singh", "u. s. tiwary"], "Key Words": ["nlp", "lms", "bert", "roberta", "wechsel", "tokenizer"], "Abstract": "a crucial element of natural language processing  nlp  is to make it possible for computers to comprehend and process human language language models  lms  have taken over the discipline in recent years. lms are pre trained models that can be customized and have significantly improved performance for a variety of challenging natural language tasks. bidirectional encoders for transformers  bert  used in both english and other languages is one of the most well known lms. large pre trained lms require enormous computational resources to train on english text. this makes training these models in other languages difficult. this paper used a unique approach termed wechsel to address this problem on the hindi dataset. the wechsel method to the roberta model and assess its effectiveness. the source model\u201a\u00e4\u00f4s english tokenizer is replaced with a tokenizer in the target language hindi. the advantages of this approach for hindi demonstrate that wechsel outperforms other models of comparable size built from scratch with up to 64 times less training effort. wechsel roberta based hindi was fine tuned on ner task using semeval 2022 datasets and the accuracy is greater than any other bert based models. in this paper wechsel roberta based hindi achieved an accuracy of 73.45% which is higher than the rest of the hindi language based bert models.", "Pub Date": "2024-04-02"}