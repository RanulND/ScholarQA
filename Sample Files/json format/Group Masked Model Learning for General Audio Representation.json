{"Title": "Group Masked Model Learning for General Audio Representation", "Doi": "10.1109/ICIP49359.2023.10222551", "Authors": ["s. atito", "m. awais", "t. alex", "j. kittler"], "Key Words": ["audio spectrograms", "self-supervised learning", "vision transformers", "gmml"], "Abstract": "vision transformers have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long range relationships. however transformers are known to be data hungry which require orders of magnitude more data  to train. this has motivated the research in self supervised pretraining of audio transformers which reduces the dependency on large amounts of labeled data and focuses on extracting concise representation of the audio spectrograms. in this paper we propose audio gmml a self supervised transformer for general audio representations that is based on group masked model learning  gmml  and a patch aggregation strategy to improve the performance of learned representations and enforce global structure of the given audio. we evaluate our pretrained models on several downstream tasks setting a new state of the art performance on five audio and speech classification tasks. the code and pretrained weights will be made publicly available for the scientific community.", "Pub Date": "2023-09-11"}