{"Title": "Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models", "Doi": "10.1109/CVPR52729.2023.00640", "Authors": ["w. wu", "x. wang", "h. luo", "j. wang", "y. yang", "w. ouyang"], "Key Words": ["video: action and event understanding"], "Abstract": "vision language models  vlms  pre trained on large  scale image text pairs have demonstrated impressive transferability on various visual tasks. transferring knowledge from such powerful vlms is a promising direction for building effective video recognition models. however current exploration in this field is still limited. we believe that the greatest value of pre trained vlms lies in building a bridge between visual and textual domains. in this paper we propose a novel framework called bike which utilizes the cross modal bridge to explore bidirectional knowledge  i  we introduce the video attribute association mechanism which leverages the video to text knowledge to generate textual auxiliary attributes for complementing video recognition. ii  we also present a temporal concept spotting mechanism that uses the text to video expertise to capture temporal saliency in a parameter free manner leading to enhanced video representation. extensive studies on six popular video datasets including kinetics 400 & 600 ucf 101 hmdb 51 activitynet and charades show that our method achieves state of the art performance in various recognition scenarios such as general zero shot and few shot video recognition. our best model achieves a state of the art accuracy of 88.6% on the challenging kinetics 400 using the released clip model. the code is available at https //github.com whwu95/bike.", "Pub Date": "2023-08-22"}