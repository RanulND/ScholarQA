{"Title": "Diversifying Cross-Domain Few-Shot Learning via Multimodal Image Editing", "Doi": "10.1109/ICASSP48485.2024.10447785", "Authors": ["z. lin", "w. yang", "l. lan", "m. geng", "h. wang", "h. chi", "x. li", "j. wang"], "Key Words": ["cross-domain few-shot learning", "large multimodal model", "image-to-image generation"], "Abstract": "standing out as one of the most widely used tools in cross domain few shot learning  cdfsl  data augmentation forms the bedrock of numerous recent advancements. however the current augmentations in cdfsl are limited in their ability to modify high level semantic attributes resulting in a lack of diversity along key semantic dimensions. one of the most promising tools to edit images with key semantic attributes e.g. backgrounds is image to image generation via large multimodal models  lmms . given the promising image editing results of recent lmms we delve into leveraging lmms to augment data diversity for cdfsl. we propose a novel method named multimodal few shot image editing  mfie  which uses lmms to automatically translate class specific images into class agnostic natural language descriptions for various key semantic attributes in target domains and editing origin images based on class agnostic natural language descriptions. to filter out corrupted data that disturbs the class specific information we apply semantic filtering using image language similarity. experiments on meta datset show that mfie surpasses sota cdfsl algorithms.", "Pub Date": "2024-03-18"}