{"Title": "Joint Modeling of Chinese Minority Language Translation Tasks", "Doi": "10.1109/IALP61005.2023.10337041", "Authors": ["y. guo", "h. zan", "h. xu"], "Key Words": ["chinese bart", "multilingual neural machine translation", "transformer"], "Abstract": "neural machine translation  nmt  normally requires a large amount of parallel corpus to obtain good performance which is often unavailable for minority languages. current methods normally pre train seq2seq models on monolingual data in a denoising manner and then fine tune the parallel data to improve the performance of low resource translation. but minority languages used in adjacent areas may co relate with each other and jointly modeling them may lead to better performance. in this paper we propose to improve the performance of chinese minority language translation with multilingual nmt  mnmt . as the tokens of the minority languages are not covered by either chinese bart or mbart and the vocabulary size of the multilingual data exceeds that of the pre trained model we map the vocabulary of minority languages to that of the pre trained bart according to the frequency and enlarge the bart vocabulary by repeating low frequency tokens respectively to address them. our experiment results on the ccmt 2023 chinese minority language translation tasks show that joint modeling can improve the uyghur to chinese and the tibetan to chinese tasks by +2.85 and +1.30 bleu respectively with bart base and lead to bleu scores of 55.48 53.52 and 48.26 on the mongolian to chinese tibetan to chinese and uyghur to chinese translation tasks respectively with bart large.", "Pub Date": "2023-12-12"}