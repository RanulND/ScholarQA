{"Title": "Teaching Structured Vision & Language Concepts to Vision & Language Models", "Doi": "10.1109/CVPR52729.2023.00261", "Authors": ["s. doveh", "a. arbelle", "s. harary", "e. schwartz", "r. herzig", "r. giryes", "r. feris", "r. panda", "s. ullman", "l. karlinsky"], "Key Words": ["multi-modal learning"], "Abstract": "vision and language  $vl$  models have demonstrated remarkable zero shot performance in a variety of tasks. however some aspects of complex language understanding still remain a challenge. we introduce the collective notion of structured vision & language concepts  svlc  which includes object attributes relations and states which are present in the text and visible in the image. recent studies have shown that even the best $vl$ models struggle with svlc. a possible way of fixing this issue is by collecting dedicated datasets for teaching each svlc type yet this might be expensive and time consuming. instead we propose a more elegant data driven approach for enhancing $vl$ models' understanding of svlcs that makes more effective use of existing $vl$ pre training datasets and does not require any additional data. while automatic understanding of image structure still remains largely unsolved language structure is much better modeled and understood allowing for its effective utilization in teaching $vl$ models. in this paper we propose various techniques based on language structure understanding that can be used to manipulate the textual part of off the shelf paired $vl$ datasets. $vl$ models trained with the updated data exhibit a significant improvement of up to 15% in their svlc understanding with only a mild degradation in their zero shot capabilities both when training from scratch or fine tuning a pre trained model. our code and pretrained models are available at  https //github.com sivandoveh/tsvlc", "Pub Date": "2023-08-22"}