{"Title": "MBFQuant: A Multiplier-Bitwidth-Fixed, Mixed-Precision Quantization Method for Mobile CNN-Based Applications", "Authors": ["p. peng", "m. you", "k. jiang", "y. lian", "w. xu"], "Pub Date": "2023-05-01", "Abstract": "deploying convolutional neural network  cnn  based applications to mobile platforms can be challenging due to the conflict between the restricted computing capacity of mobile devices and the heavy computational overhead of running a cnn. network quantization is a promising way of alleviating this problem. however network quantization can result in accuracy degradation and this is especially the case with the compact cnn architectures that are designed for mobile applications. this paper presents a novel and efficient mixed precision quantization pipeline called mbfquant. it redefines the design space for mixed precision quantization by keeping the bitwidth of the multiplier fixed unlike other existing methods because we have found that the quantized model can maintain almost the same running efficiency so long as the sum of the quantization bitwidth of the weight and the input activation of a layer is a constant. to maximize the accuracy of a quantized cnn model we have developed a simulated annealing  sa  based optimizer that can automatically explore the design space and rapidly find the optimal bitwidth assignment. comprehensive evaluations applying ten cnn architectures to four datasets have served to demonstrate that mbfquant can achieve improvements in accuracy of up to 19.34% for image classification and 1.12% for object detection with respect to a corresponding uniform bitwidth quantized model.", "Doi": "10.1109/TIP.2023.3268562", "Key Words": ["convolutional neural network", "mixed-precision quantization", "large-scale neighborhood search", "model compression"]}