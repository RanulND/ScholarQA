{"Title": "Hint-Enhanced In-Context Learning Wakes Large Language Models Up For Knowledge-Intensive Tasks", "Doi": "10.1109/ICASSP48485.2024.10447527", "Authors": ["y. wang", "q. guo", "x. ni", "c. shi", "l. liu", "h. jiang", "y. yang"], "Key Words": ["large language model", "in-context learning", "retrieval model", "open-domain question answering"], "Abstract": "in context learning  icl  ability has emerged with the increasing scale of large language models  llms  enabling them to learn input label mappings from demonstrations and perform well on downstream tasks. however under the standard icl setting llms may sometimes neglect query related information in demonstrations leading to incorrect predictions. to address this limitation we propose a new paradigm called hint enhanced in context learning  hicl  to explore the power of icl in open domain question answering an important form in knowledge intensive tasks. hicl leverages llms\u201a\u00e4\u00f4 reasoning ability to extract query related knowledge from demonstrations then concatenates the knowledge to prompt llms in a more explicit way. furthermore we track the source of this knowledge to identify specific examples and introduce a hint related example retriever  her  to select informative examples for enhanced demonstrations. we evaluate hicl with her on 3 open domain qa benchmarks and observe average performance gains of 2.89 em score and 2.52 f1 score on gpt 3.5 turbo 7.62 em score and 7.27 f1 score on llama 2 chat 7b compared with standard setting.", "Pub Date": "2024-03-18"}