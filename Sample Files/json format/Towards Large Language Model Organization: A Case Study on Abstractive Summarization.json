{"Title": "Towards Large Language Model Organization: A Case Study on Abstractive Summarization", "Doi": "10.1109/BigData59044.2023.10386199", "Authors": ["k. boros", "m. oyamada"], "Key Words": ["abstractive summarization", "large language models", "faithfulness", "gpt-3.5-turbo", "benchmarking"], "Abstract": "in this work we propose \u201a\u00e4\u00f9llm organization\u201a\u00e4\u00f9 an organizational structure based large language model workflow for improving the performance of standard abstractive summarization techniques and mitigate unfaithful summary generation. we formulated the organizational structure based large language model workflow as a directed acyclic graph  dag  where each node corresponds to an large language model and each edge to a communication protocol. our workflow is benchmarked on 5 datasets from various domains using 7 evaluation metrics. the results indicate that large language model organization could mitigate unfaithfulness and increase the overall performance of abstractive summarization methods.", "Pub Date": "2024-01-22"}