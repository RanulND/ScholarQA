{"Title": "A Systematic Literature Review on Binary Neural Networks", "Authors": ["r. sayed", "h. azmi", "h. shawkey", "a. h. khalil", "m. refky"], "Pub Date": "2023-03-23", "Abstract": "this paper presents an extensive literature review on binary neural network  bnn . bnn utilizes binary weights and activation function parameters to substitute the full precision values. in digital implementations bnn replaces the complex calculations of convolutional neural networks  cnns  with simple bitwise operations. bnn optimizes large computation and memory storage requirements which leads to less area and power consumption compared to full precision models. although there are many advantages of bnn the binarization process has a significant impact on the performance and accuracy of the generated models. to reflect the state of the art in bnn and explore how to develop and improve bnn based models we conduct a systematic literature review on bnn with data extracted from 239 research studies. our review discusses various bnn architectures and the optimization approaches developed to improve their performance. there are three main research directions in bnn  accuracy optimization compression optimization and acceleration optimization. the accuracy optimization approaches include quantization error reduction special regularization gradient error minimization and network structure. the compression optimization approaches combine fractional bnn and pruning. the acceleration optimization approaches comprise computing in memory fpga based implementations and asic based implementations. at the end of our review we present a comprehensive analysis of bnn applications and their evaluation metrics. also we shed some light on the most common bnn challenges and the future research trends of bnn.", "Doi": "10.1109/ACCESS.2023.3258360", "Key Words": ["binary neural network", "convolutional neural network", "deep learning", "optimization approaches", "quantization", "systematic literature review"]}