{"Title": "Towards Evaluation and Understanding of Large Language Models for Cyber Operation Automation", "Doi": "10.1109/CNS59707.2023.10288677", "Authors": ["m. sultana", "a. taylor", "l. li", "s. majumdar"], "Key Words": ["natural language processing", "cyber security", "cyber operational tasks", "benchmark dataset", "autonomous cyber defence"], "Abstract": "can foundational language models be useful in automating cybersecurity tasks? to address this open question systematic and comprehensive evaluation of large language models  llms  across diverse cyber operational tasks  e.g. incident response threat identification forensic analysis etc.  as well as understanding their risks and limitations are crucial. a significant challenge lies in the absence of a standard benchmark dataset encompassing real life cyber operational tasks that can be processed by llms. this paper tackles this challenge by conducting a preliminary study towards evaluation and understanding of llms for cyber operation automation. to that end we first identify a list of defensive cyber operational tasks with increasing complexities and suggests the creation of new datasets to accomplish these tasks. second we review recent works leveraging llms for downstream cyber operational tasks to identify research gaps and open problems. third we propose a framework to understand and benchmark the cyber operational tasks to report potential solutions and research directions for the reliable evaluation of llms. finally this paper serves as an open call to the cybersecurity researchers and professionals to contribute to the development of an open source evaluation framework paving the way for the trustworthy use of foundation models in cyber domain.", "Pub Date": "2023-10-27"}