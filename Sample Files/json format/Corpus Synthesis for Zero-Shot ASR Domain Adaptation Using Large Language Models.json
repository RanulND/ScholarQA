{"Title": "Corpus Synthesis for Zero-Shot ASR Domain Adaptation Using Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10447240", "Authors": ["h. su", "t. -y. hu", "h. s. koppula", "r. vemulapalli", "j. -h. r. chang", "k. yang", "g. v. mantena", "o. tuzel"], "Key Words": ["automatic speech recognition", "large language models", "controllable speech synthesis", "zero-shot asr adaptation"], "Abstract": "while automatic speech recognition  asr  systems are widely used in many real world applications they often do not generalize well to new domains and need to be fine tuned on data from these domains. however target domain data usually are not readily available in many scenarios. in this paper we propose a new strategy for adapting asr models to new target domains without any text or speech from those domains. to accomplish this we propose a novel data synthesis pipeline that uses a large language model  llm  to generate a target domain text corpus and a state of the art controllable speech synthesis model to generate the corresponding speech. we propose a simple yet effective in context instruction fine tuning strategy to increase the effectiveness of llm in generating text corpora for new domains. experiments on the slurp dataset show that the proposed method achieves an average relative word error rate improvement of 28% on unseen target domains without any performance drop in source domains.", "Pub Date": "2024-03-18"}