{"Title": "An Offline-Transfer-Online Framework for Cloud-Edge Collaborative Distributed Reinforcement Learning", "Doi": "10.1109/TPDS.2024.3360438", "Authors": ["t. zeng", "x. zhang", "j. duan", "c. yu", "c. wu", "x. chen"], "Key Words": ["distributed training", "offline-transfer-online", "deep reinforcement learning", "cloud-edge collaborative networks"], "Abstract": "recent advances in deep reinforcement learning  drl  have made it possible to train various powerful agents to perform complex tasks in real time environments. with the next generation communication technologies making cloud edge collaborative artificial intelligence service with evolved drl agents can be a significant scenario. however agents with different algorithms and architectures in the same drl scenario may not be compatible and training them is either time consuming or resource demanding. in this article we design a novel cloud edge collaborative drl training framework named offline transfer online which is a new approach that can speed up the convergence of online drl agents at the edge by interacting with offline agents in the cloud with the minimum data interchanged and without relying on high quality offline datasets. therein we propose a novel algorithm independent knowledge distillation algorithm for online rl agents by leveraging pre trained models and the interface between agents and the environment to transfer distilled knowledge among multiple heterogeneous agents efficiently. extensive experiments show that our algorithm can accelerate the convergence of various online agents in a double to decuple speed with comparable reward achieved in different environments.", "Pub Date": "2024-03-18"}