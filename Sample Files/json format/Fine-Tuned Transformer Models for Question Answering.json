{"Title": "Fine-Tuned Transformer Models for Question Answering", "Doi": "10.1109/ICCCNT56998.2023.10307046", "Authors": ["s. s. lakkimsetty", "s. v. latchireddy", "s. m. lakkoju", "g. r. manukonda", "r. v. v. m. krishna"], "Key Words": ["transformers", "fine-tuning", "few-shot learning", "question-answering"], "Abstract": "effective question answering is crucial for applications involving natural language processing such as conversational artificial systems. the pre trained bert  bidirectional encoder representations from transformers  model achieved outstanding results on the stanford question answering dataset  squad  a well known benchmark for question answering tasks. the three different bert models  bert base bert large distilbert  are pre trained on general text corpus. these bert models are trained to identify the right context in a paragraph to give answers. further fine tuning the models with few shot learning on domain related questions and answers improves the performance. hence our work proposes different strategies to improve the performance of bert models on tasks requiring domain specific question answering tasks using a customized squad dataset. our work highlights the significance of optimizing the bert models on domain specific data for enhanced performance in particular tasks. the results of this study have implications for domain specific knowledge in natural language processing. with a score of 0.9632 the bert large model has the best accuracy out of all the bert models tested. the performance of the bert large model consistently beats that of the other models in several parameters including precision recall and f1 score.", "Pub Date": "2023-11-23"}