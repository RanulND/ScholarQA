{"Title": "NBSync: Parallelism of Local Computing and Global Synchronization for Fast Distributed Machine Learning in WANs", "Doi": "10.1109/TSC.2023.3304312", "Authors": ["h. zhou", "z. li", "h. yu", "l. luo", "g. sun"], "Key Words": ["distributed machine learning", "federated learning", "parameter server system", "and distributed optimization"], "Abstract": "recently due to privacy concerns distributed machine learning in wide area networks  dml wans  attracts increasing attention and has been widely deployed to promote the widespread application of intelligence services that rely on geographically distributed data. dml wans is essentially performing collaboratively federated learning over a combination of servers at both edge and cloud on a large spatial scale. however efficient model training is challenging for dml wans because it is blocked by the high overhead of model parameter synchronization between computing servers over wans. the reason is that there has a sequential dependency between local model computing and global model synchronization of traditional dml wans training methods intrinsically producing a sequential blockage between them e.g. fedavg. when the computing heterogeneity and the low wan bandwidth coexist a long block of global model synchronization prolongs the training time and leads to low utilization of local computing. despite many efforts on alleviating synchronization overhead with novel communication technologies and synchronization methods they still use traditional training patterns with sequential dependency and thereby have very limited improvements such as fedasync and esync. in this article we propose nbsync a novel training algorithm for dml wans which greatly speeds up the model training by the parallelism of local computing and global synchronization. nbsync employs a well designed pipelining scheme which can properly relax the sequential dependency of local computing and global synchronization and process them in parallel so as to overlap their operating overhead in the time dimension. nbsync also realizes flexible differentiated and dynamical local computing for workers to maximize the overlap ratio in dynamically heterogeneous training environments. convergence analysis shows that the convergence rate of nbsync training process is asymptotically equal to that of ssgd and nbsync has a better convergence efficiency. we implemented the prototype of nbsync based on a popular parameter server system i.e. mxnet ps lite library and evaluate its performance on a dml wans testbed. experimental results show that nbsync speeds up training about 1.43\u221a\u00f3\u201a\u00e4\u00ec2.79\u221a\u00f3 than state of the art distributed training algorithms  dtas  in dml wans scenarios where computing heterogeneity and low wan bandwidth coexist.", "Pub Date": "2023-12-14"}