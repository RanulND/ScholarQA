{"Title": "A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?", "Doi": "10.1109/ACCESS.2024.3349952", "Authors": ["j. fields", "k. chovanec", "p. madiraju"], "Key Words": ["nlp", "text classification", "transformers", "survey"], "Abstract": "text classification in natural language processing  nlp  is evolving rapidly particularly with the surge in transformer based models including large language models  llm . this paper presents an in depth survey of text classification techniques across diverse benchmarks addressing applications from sentiment analysis to chatbot driven question answering. methodologically it utilizes nlp facilitated approaches such as co citation and bibliographic coupling alongside traditional research techniques. because new use cases continue to emerge in this dynamic field the study proposes an expanded taxonomy of text classification applications extending the focus beyond unimodal  text only  inputs to explore the emerging field of multimodal classification. while offering a comprehensive review of text classification with llms this review highlights novel questions that arise when approaching the task with transformers  it evaluates the use of multimodal data including text numeric and columnar data and discusses the evolution of text input lengths  tokens  for long text classification  it covers the historical development of transformer based models emphasizing recent advancements in llms  it evaluates model accuracy on 358 datasets across 20 applications with results challenging the assumption that llms are universally superior revealing unexpected findings related to accuracy cost and safety  and it explores issues related to cost and access as models become increasingly expensive. finally the survey discusses new social and ethical implications raised when using llms for text classification including bias and copyright. throughout the review emphasizes the importance of a nuanced understanding of model performance and a holistic approach to deploying transformer based models in real world applications.", "Pub Date": "2024-01-12"}