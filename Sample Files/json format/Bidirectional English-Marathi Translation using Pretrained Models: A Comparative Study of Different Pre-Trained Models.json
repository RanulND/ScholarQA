{"Title": "Bidirectional English-Marathi Translation using Pretrained Models: A Comparative Study of Different Pre-Trained Models", "Doi": "10.1109/INCOFT60753.2023.10425770", "Authors": ["l. saini", "d. vidhyarthi"], "Key Words": ["mbert", "mat", "hugging face", "fine-tuning", "indicbert", "mts", "gelu", "albertmlmhead"], "Abstract": "this study investigates the use of pre trained models for english to marathi translation. the purpose of this study is to investigate english to marathi translation utilising pre trained multilingual language models. the research examines multiple translation models accessible in the hugging face library such as mbart large 50 many to many mmt indicbart m2m100 1.2b helsinki nlp opus mt mr en and umts base. bleu score pre cision brevity penalty chrf score ter score google blue and rouge are among the assessment criteria used. because of this we now have a bidirectional translation model which is formed by the integration of two unidirectional models one that translates english to marathi and the other that translates marathi to english. in spite of the inherent difficulties posed by language asymmetry and the limited parallel data available in bidirectional models the purpose of this study is to investigate potential solutions to these roadblocks. according to the findings the performance of the optimized mbart model has significantly improved whilst the performance of the helsinki nlp model has essentially stayed the same. notably making use of mbart setup and weight transfer led to significant improvements being made to the ai4bharat model.", "Pub Date": "2024-02-22"}