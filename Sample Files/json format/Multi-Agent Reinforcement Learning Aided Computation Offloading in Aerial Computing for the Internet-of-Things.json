{"Title": "Multi-Agent Reinforcement Learning Aided Computation Offloading in Aerial Computing for the Internet-of-Things", "Doi": "10.1109/TSC.2022.3190562", "Authors": ["z. qin", "h. yao", "t. mai", "d. wu", "n. zhang", "s. guo"], "Key Words": ["aerial computing", "computation offloading", "deep reinforcement learning", "mobile edge computing", "multi-agent system"], "Abstract": "leo satellite networks have become a necessary supplement to terrestrial networks aiming to provide worldwide ubiquitous connectivity especially in complicated areas  e.g. mountains oceans and disaster areas  where terrestrial network infrastructures are typically sparingly distributed or unavailable. however the increasing computation intensive internet of things  iot  applications  e.g. real time remote monitoring intelligent transportation  require not only efficient and reliable communication but also massive computing capabilities. constrained by the battery and computing resources the computing tasks and data of applications have to be transmitted to remote cloud servers. this bandwidth limitation and high transmission delay in leo networks will reduce the quality of service  qos  of iot applications. recently the combination of leo networks and edge computing  i.e. satellite mobile edge computing smec  offers significant opportunities to address these problems. the iot devices can directly get the computing resources directly from satellites rather than remote servers thus avoiding long distance transmission. considering the resource constraints on satellites offloading policy plays a crucial role in whole system performance. in this paper we design a hybrid offloading architecture which applies a centralized training and distributed execution framework. also we propose a multi agent actor critic reinforcement learning algorithm where a centralized \u201a\u00e4\u00facritic\u201a\u00e4\u00f9 is augmented with the global network state to ease the training procedure of distributed user equipments  ue  by evaluating the benefits of their decisions while the ues can adjust their policies according to the critic evaluation and choose their own decisions relying on their observations.", "Pub Date": "2023-06-13"}