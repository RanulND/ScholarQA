{"Title": "On Computing Paradigms - Where Will Large Language Models Be Going", "Doi": "10.1109/ICDM58522.2023.00211", "Authors": ["x. wu", "x. zhu", "e. baralis", "r. lu", "v. kumar", "l. rutkowski", "j. tang"], "Key Words": ["computing", "artificial intelligence", "large language models"], "Abstract": "computing generates intelligence. with this statement we do not mean computing\u201a\u00e4\u00f4s capabilities of manipulating numbers shapes symbols and even logics. what we mean is the ingenious design of computing structures which serve as the basis of intelligence generation during program running. in this panel discussion we consider how to obtain such capabilities through some computing paradigms as examples including principal computing logic computing discriminative computing and generative computing. the panelists express their thoughts about the inherent advantages and disadvantages of each of these paradigms in terms of their adaptivity interpretability generality and specificity and dives into detailed discussions about large language models  large language model  a mainstream generative paradigm which leverages the strengths of large pre trained models and downstream prompt tuning to deliver combined intelligence superior to most existing frameworks in natural language processing. the panel outlines potential challenges of the generative paradigm with a strong focus on large language model and emphasizes that future directions of such models will need to address  1  tackling bias discrimination and transparency challenges   2  delivering logical answers with high specificity   3  enabling personalized lightweight and rapid updating mechanisms   4  assessing accreditation tracing and misusages  and  5  ensuring sustainable large language model.", "Pub Date": "2024-02-05"}