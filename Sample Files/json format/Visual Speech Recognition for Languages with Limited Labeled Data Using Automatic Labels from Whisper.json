{"Title": "Visual Speech Recognition for Languages with Limited Labeled Data Using Automatic Labels from Whisper", "Doi": "10.1109/ICASSP48485.2024.10446720", "Authors": ["j. h. yeo", "m. kim", "s. watanabe", "y. m. ro"], "Key Words": ["lip reading", "visual speech recognition", "low-resource language lip reading", "multilingual automated labeling"], "Abstract": "this paper proposes a powerful visual speech recognition  vsr  method for multiple languages especially for low resource languages that have a limited number of labeled data. different from previous methods that tried to improve the vsr performance for the target language by using knowledge learned from other languages we explore whether we can increase the amount of training data itself for the different languages without human intervention. to this end we employ a whisper model which can conduct both language identification and audio based speech recognition. it serves to filter data of the desired languages and transcribe labels from the unannotated multilingual audio visual data pool. by comparing the performances of vsr models trained on automatic labels and the human annotated labels we show that we can achieve similar vsr performance to that of human annotated labels even without utilizing human annotations. through the automated labeling process we label large scale unlabeled multilingual databases voxceleb2 and avspeech producing 1002 hours of data for four low vsr resource languages french italian spanish and portuguese. with the automatic labels we achieve new state of the art performance on mtedx in four languages significantly surpassing the previous methods. the automatic labels are available online  bit.ly/3lajr6w", "Pub Date": "2024-03-18"}