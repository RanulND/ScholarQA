{"Title": "Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems", "Doi": "10.1109/ICASSP48485.2024.10448022", "Authors": ["t. udagawa", "m. suzuki", "g. kurata", "m. muraoka", "g. saon"], "Key Words": ["automatic speech recognition", "knowledge distillation", "large language models", "bert"], "Abstract": "transferring the knowledge of large language models  large language model  is a promising technique to incorporate linguistic knowledge into end to end automatic speech recognition  asr  systems. however existing works only transfer a single representation of large language model  e.g. the last layer of pretrained bert  while the representation of a text is inherently non unique and can be obtained variously from different layers contexts and models. in this work we explore a wide range of techniques to obtain and transfer multiple representations of large language model into a transducer based asr system. while being conceptually simple we show that transferring multiple representations of large language model can be an effective alternative to transferring only a single large language model representation.", "Pub Date": "2024-03-18"}