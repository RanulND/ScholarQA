{"Title": "CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset", "Doi": "10.1109/CVPR52729.2023.01423", "Authors": ["t. gan", "q. wang", "x. dong", "x. ren", "l. nie", "q. guo"], "Key Words": ["multi-modal learning"], "Abstract": "owing to well designed large scale video text datasets recent years have witnessed tremendous progress in video text pre training. however existing large scale video text datasets are mostly english only. though there are certain methods studying the chinese video text pre training they pre train their models on private datasets whose videos and text are unavailable. this lack of large scale public datasets and benchmarks in chinese hampers the research and downstream applications of chinese video text pre training. towards this end we release and benchmark cnvid 3.5m a large scale public cross modal dataset containing over 3.5m chinese video text pairs. we summarize our contributions by three verbs i.e. \u201a\u00e4\u00fabuild\u201a\u00e4\u00f9 \u201a\u00e4\u00fafilter\u201a\u00e4\u00f9 and \u201a\u00e4\u00fapretrain\u201a\u00e4\u00f9  1  to build a public chinese video text dataset we collect over 4.5m videos from the chinese websites. 2  to improve the data quality we propose a novel method to filter out 1m weakly paired videos resulting in the cnvid 3.5m dataset. and 3  we benchmark cnvid 3.5m with three mainstream pixel level pre training architectures. at last we propose the hard sample curriculum learning strategy to promote the pre training performance. to the best of our knowledge cnvid 3.5m is the largest public video text dataset in chinese and we provide the first pixel level benchmarks for chinese video text pre training. the dataset codebase and pre trained models are available at https //github.com cnvid/cnvid 3.5m.", "Pub Date": "2023-08-22"}