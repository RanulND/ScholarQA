{"Title": "Online Distributed Stochastic Gradient Algorithm for Nonconvex Optimization With Compressed Communication", "Authors": ["j. li", "c. li", "j. fan", "t. huang"], "Pub Date": "2024-01-29", "Abstract": "this article examines an online distributed optimization problem over an unbalanced digraph in which a group of nodes in the network tries to collectively search for a minimizer of a time varying global cost function while data are distributed among computing nodes. as the problem size becomes large it will inevitably suffer from the communication bottleneck since each node that exchanges messages potentially transmits large amounts of information to its neighbors. to handle the issue we design an online stochastic gradient algorithm with compressed communication when the knowledge of the gradient is available. we obtain the regret bounds for both nonconvex and convex cost functions which can reach almost the same order of classic distributed optimization algorithms with exact communication. to resolve the scenario when the information of gradients is not accessible a bandit version of the previous algorithm is then proposed. explicit regret bounds of the bandit algorithm are also established for both nonconvex and convex cost functions. the result reveals that the performance of the bandit feedback method is almost close to that of the gradient feedback method. several numerical experiments corroborate the main theoretical findings obtained in this article and exemplify a remarkable speedup when compared to existing distributed algorithms with exact communication.", "Doi": "10.1109/TAC.2023.3327183", "Key Words": ["bandit-feedback", "compressed communication", "distributed optimization (do)", "nonconvex optimization", "online optimization", "stochastic approximation"]}