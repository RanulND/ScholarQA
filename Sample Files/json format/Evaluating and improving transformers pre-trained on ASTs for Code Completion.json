{"Title": "Evaluating and improving transformers pre-trained on ASTs for Code Completion", "Doi": "10.1109/SANER56733.2023.00096", "Authors": ["m. ochs", "k. narasimhan", "m. mezini"], "Key Words": ["ast", "code completion", "transformer"], "Abstract": "automatic code completion is one of the most popular developer assistance features which is usually performed using static program analysis. but studies have shown that static analysis can be riddled with false positives. another issue with static analysis based code completion is that the recommendation does not take into account the history/ context in which the software is operating and rely on type/ alphabetical ordering of suggestions. a recent development that has shown to be promising in this direction is the use of language models such as transformers that are trained on real world code from github to provide context sensitive accurate code completion suggestions. studies on transformer based code completion have shown that such restrictions can be leveraged  i.e  training transformers on structural representations of code  specifically asts  could have a positive impact on the accuracy of code completion. to this end the work by kim et al. implemented and evaluated travtrans which is based on the powerful text prediction language model gpt-2 by training on abstract syntax trees instead of treating code as plain texts. using alternative source code representation such as ast provides the already potent language model with an additional layer of program semantic awareness. but travtrans has adapted several rigid choices regarding various components of the transformer architecture like embedding sizes sliding windows etc. travtrans also suffers from issues related to running out of vocabulary. in this paper we reproduce the travtrans model and perform a deeper fine grained analysis of the impact of various architectural and code level settings on the prediction. as a result of our fine grained analysis we also identify several aspects that need improvements like the fact that the model performs particularly poorly with code involving dictionaries and lists. we also offer solutions to a few of the issues like the out of vocabulary issue. finally our results motivates the need for a customizable transformer architecture for coding tasks.", "Pub Date": "2023-05-15"}