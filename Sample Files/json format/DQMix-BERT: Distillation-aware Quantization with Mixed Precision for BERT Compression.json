{"Title": "DQMix-BERT: Distillation-aware Quantization with Mixed Precision for BERT Compression", "Doi": "10.1109/SMC53992.2023.10394642", "Authors": ["y. tan", "l. jiang", "p. chen", "c. tong"], "Key Words": ["bert compression", "mixed precision quantization", "knowledge distillation", "pre-training language model"], "Abstract": "transformer based architecture models like bert have performed excellently for various natural language processing  nlp  tasks. however these models are usually computationally expensive with a large number of parameters. as a result deploying them in edge devices has become a challenging task. the existing compression work on lower precision quantization still has a severe accuracy decrease and rarely focuses on the information hidden in the different modules of the model. in this paper we propose a distillation aware quantization with mixed precision method combined with quantization and knowledge distillation. we achieve the ultra low mixed precision quantization with the different sensitivity of different modules of bert. moreover we leverage knowledge distillation to reduce the model accuracy degradation. we extensively test our method on four glue tasks. it shows that dqmix bert outperforms the other bert compression methods and even achieves comparable performance to the original bert model while achieving ~8x compression.", "Pub Date": "2024-01-29"}