{"Title": "PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models", "Doi": "10.1109/ACII59096.2023.10388152", "Authors": ["t. feng", "s. narayanan"], "Key Words": ["speech", "emotion recognition", "parameter-efficient fine-tuning", "pre-trained model"], "Abstract": "many recent studies have focused on fine tuning pretrained models for speech emotion recognition  ser  resulting in promising performance compared to traditional methods that rely largely on low level knowledge inspired acoustic features. these pre trained speech models learn general purpose speech representations using self supervised or weakly supervised learning objectives from large scale datasets. despite the significant advances made in ser through the use of pre trained architecture fine tuning these large pre trained models for different datasets requires saving copies of entire weight parameters rendering them impractical to deploy in real world settings. as an alternative this work explores parameter efficient fine tuning  peft  approaches for adapting pre trained speech models for emotion recognition. specifically we evaluate the efficacy of adapter tuning embedding prompt tuning and lora  low rank approximation  on four popular ser testbeds. our results reveal that lora achieves the best fine tuning performance in emotion recognition while enhancing fairness and requiring only a minimal extra amount of weight parameters. furthermore our findings offer novel insights into future research directions in ser distinct from existing approaches focusing on directly fine tuning the model architecture. our code is publicly available under  https //github.com usc sail peft ser.", "Pub Date": "2024-01-15"}