{"Title": "Sensi-Bert: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient Language Model", "Doi": "10.1109/ICASSP48485.2024.10447812", "Authors": ["s. kundu", "s. n. sridhar", "m. szankin", "s. sundaresan"], "Key Words": ["efficient fine-tuning", "sensitivity analysis", "parameter efficient bert"], "Abstract": "large pre trained language models have recently gained significant traction due to their improved performance on various down stream tasks like text classification and question answering requiring only few epochs of fine tuning. however their large model sizes often prohibit their applications on resource constrained edge devices. existing solutions of yielding parameter efficient bert models largely rely on compute exhaustive training and fine tuning. moreover they often rely on additional compute heavy models to mitigate the performance gap. in this paper we present sensi bert a sensitivity driven efficient fine tuning of bert models that can take an off the shelf pre trained bert model and yield highly parameter efficient models for downstream tasks. in specific we perform sensitivity analysis to rank each individual parameter tensor that then is used to trim them accordingly during fine tuning for a given parameter or flops budget. our experiments show the efficacy of sensi bert across different downstream tasks including mnli qqp qnli sst 2 mrpc cola and squad showing better performance at similar or smaller parameter budget compared to various alternatives.", "Pub Date": "2024-03-18"}