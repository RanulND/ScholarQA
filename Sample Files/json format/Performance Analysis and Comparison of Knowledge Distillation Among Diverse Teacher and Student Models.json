{"Title": "Performance Analysis and Comparison of Knowledge Distillation Among Diverse Teacher and Student Models", "Doi": "10.1109/ICCE-Asia59966.2023.10326343", "Authors": ["j. lee", "t. t. l. vuong", "j. wang", "j. kan", "k. byeon", "j. c. lee", "s. jung", "d. c. bui", "a. t. nguyen", "h. yang", "j. t. kwak"], "Key Words": ["lightweight model", "knowledge distillation", "cnn"], "Abstract": "with the advancements in deep learning the development of large scale models has become essential for many machine learning tasks in various fields such as computer vision and natural language. such large scale models demand extensive computational power and memory posing challenges for practical deployment on mobile and edge devices. to address these issues knowledge distillation a technique for reproducing the performance of large scale models with a smaller model has been proposed. this paper investigates the effectiveness of knowledge distillation across various combinations of teach student model pairs for image classification to deepen the understanding of knowledge distillation and the relationship between large  and small scale models.", "Pub Date": "2023-11-27"}