{"Title": "MixPipe: Efficient Bidirectional Pipeline Parallelism for Training Large-Scale Models", "Doi": "10.1109/DAC56929.2023.10247730", "Authors": ["w. zhang", "b. zhou", "x. tang", "z. wang", "s. hu"], "Key Words": ["pipeline parallelism", "data parallelism", "deep learning", "large models", "distributed training"], "Abstract": "the rapid development of large scale deep neural networks has put forward an urgent demand for the efficiency of parallel training. recently bidirectional pipeline parallelism has been recognized as an effective approach for improving training throughput. this paper proposes mixpipe a novel bidirectional pipeline parallelism for efficiently training large scale models in synchronous scenarios. compared with previous proposals mixpipe achieves a better balance between pipeline utilization and device utilization which benefits from the flexible regulating for the number of micro batches injected into the bidirectional pipelines at the beginning. mixpipe also features a mixed schedule to balance memory usage and further reduce the bubble ratio. evaluation results show that  for transformer based language models  i.e. bert and gpt-2 models  mixpipe improves the training throughput by up to 2.39\u221a\u00f3 over the state of the art synchronous pipeline approaches.", "Pub Date": "2023-09-15"}