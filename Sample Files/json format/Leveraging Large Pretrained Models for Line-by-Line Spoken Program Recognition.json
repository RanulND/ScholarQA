{"Title": "Leveraging Large Pretrained Models for Line-by-Line Spoken Program Recognition", "Doi": "10.1109/ICASSP48485.2024.10448435", "Authors": ["s. nowrin", "k. vertanen"], "Key Words": ["low resource speech recognition", "large pre-trained models", "voice programming", "language modeling"], "Abstract": "spoken programming languages significantly differ from natural english due to the inherent variability in speech patterns among programmers and the wide range of programming constructs. in this paper we employ wav2vec 2.0 to enhance the accuracy of transcribing spoken programming languages like java. adapting a model with just one hour of spoken programs that had prior exposure to a substantial amount of natural english labeled data we achieve a word error rate  wer  of 8.7% surpassing the high 28.4% wer of a model trained solely on natural english. decoding with a domain specific n gram model and subsequently rescoring the n best list with a fine tuned large language model tailored to the programming domain resulted in a wer of 5.5% on our test set.", "Pub Date": "2024-03-18"}