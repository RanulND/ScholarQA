{"Title": "Keep Decoding Parallel With Effective Knowledge Distillation From Language Models To End-To-End Speech Recognisers", "Doi": "10.1109/ICASSP48485.2024.10447305", "Authors": ["m. hentschel", "y. nishikawa", "t. komatsu", "y. fujita"], "Key Words": ["knowledge distillation", "bert", "ctc", "speech recognition"], "Abstract": "this study presents a novel approach for knowledge distillation  kd  from a bert teacher model to an automatic speech recognition  asr  model using intermediate layers. to distil the teacher\u201a\u00e4\u00f4s knowledge we use an attention decoder that learns from bert\u201a\u00e4\u00f4s token probabilities. our method shows that language model  lm  information can be more effectively distilled into an asr model using both the intermediate layers and the final layer. by using the intermediate layers as distillation target we can more effectively distil lm knowledge into the lower network layers. using our method we achieve better recognition accuracy than with shallow fusion of an external lm allowing us to maintain fast parallel decoding. experiments on the librispeech dataset demonstrate the effectiveness of our approach in enhancing greedy decoding with connectionist temporal classification  ctc .", "Pub Date": "2024-03-18"}