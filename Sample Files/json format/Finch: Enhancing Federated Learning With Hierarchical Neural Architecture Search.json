{"Title": "Finch: Enhancing Federated Learning With Hierarchical Neural Architecture Search", "Doi": "10.1109/TMC.2023.3315451", "Authors": ["j. liu", "j. yan", "h. xu", "z. wang", "j. huang", "y. xu"], "Key Words": ["edge computing", "federated learning", "non-iid data", "neural architecture search"], "Abstract": "federated learning  fl  has been widely adopted to train machine learning models over massive data in edge computing. most works of fl employ pre defined model architectures on all participating clients for model training. however these pre defined architectures may not be the optimal choice for the fl setting since manually designing a high performance neural architecture is complicated and burdensome with intense human expertise and effort which easily makes the model training fall into the local suboptimal solution. to this end neural architecture search  nas  has been applied to fl to address this critical issue. unfortunately the search space of existing federated nas approaches is extraordinarily large resulting in unacceptable completion time on the resource constrained edge clients especially under the non independent and identically distributed  non iid  setting. in order to remedy this we propose a novel framework called finch which adopts hierarchical neural architecture search to enhance federated learning. in finch we first divide the clients into several clusters according to the data distribution. then some subnets are sampled from a pre trained supernet and allocated to the specific client clusters for searching the optimal model architecture in parallel so as to significantly accelerate the process of model searching and training. the extensive experimental results demonstrate the high effectiveness of our proposed framework. specifically finch can reduce the completion time by about 30.6% and achieve an average accuracy improvement of around 9.8% compared with the baselines.", "Pub Date": "2024-04-04"}