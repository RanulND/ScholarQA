{"Title": "Generating Natural Language From Logic Expressions With Structural Representation", "Doi": "10.1109/TASLP.2023.3263784", "Authors": ["x. wu", "y. cai", "z. lian", "h. -f. leung", "t. wang"], "Key Words": ["natural language generation", "logic expressions", "tree-lstm", "graph convolutional networks"], "Abstract": "incorporating logic reasoning with deep neural networks  dnns  is an important challenge in machine learning. in this article we study the problem of converting logical expressions into natural language. in particular given a sequential logic expression the goal is to generate its corresponding natural sentence. since the information in a logic expression often has a hierarchical structure a sequence to sequence baseline struggles to capture the full dependencies between words and hence it often generates incorrect sentences. to alleviate this problem we propose a model to convert structural logic expressions into natural language  sletonl . sletonl converts sequential logic expressions into structural representation and leverages structural encoders to capture the dependencies between nodes. the quantitative and qualitative analyses demonstrate that our proposed method outperforms the seq2seq model which is based on the sequential representation and outperforms strong pretrained language models  e.g. t5 bart gpt3  with a large margin  28.6 in bleu3  in out of distribution evaluation.", "Pub Date": "2023-04-20"}