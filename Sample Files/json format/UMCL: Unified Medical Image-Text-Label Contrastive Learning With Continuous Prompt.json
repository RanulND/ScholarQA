{"Title": "UMCL: Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt", "Doi": "10.1109/BIBM58861.2023.10386034", "Authors": ["y. wang", "g. wang"], "Key Words": ["multimodal pretrain", "vision-language model", "medical image analysis"], "Abstract": "contrastive language image pre training  clip  can leverage large dataset of unlabeled image text pairs which have demonstrated impressive performance in various downstream tasks. given that annotating medical data is timeconsuming and laborious image text pre training has promising applications in exploiting large scale medical image and radiology report dataset. however medical image text pre training faces several challenges as follows   1  due to privacy concerns the amount of available medical data is relatively small compared to natural data leading to weaker generalization ability of the model.  2  medical images are highly similar with only finegrained differences in subtleties resulting in a large number of false negative sample pairs in comparison learning.  3  the hand crafted prompt usually differs from the natural medical image report subtle changes in wording can lead to significant differences in performance. in this paper we propose a unified image text label contrastive learning framework based on continuous prompts with three main contributions. first we unified the data of images text and labels which greatly expanded the training data that the model could utilize. second we address the issue of data diversity and the impact of hand crafted prompts on model performance by introducing continuous implicit prompts. lastly we propose a image text label contrastive training to mitigate the problem of too many false negative samples. we demonstrate through sufficient experiments that the unified medical contrastive learning  umcl  framework exhibits excellent performance on several downstream tasks.", "Pub Date": "2024-01-18"}