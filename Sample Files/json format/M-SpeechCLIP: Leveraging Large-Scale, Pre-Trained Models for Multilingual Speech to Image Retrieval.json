{"Title": "M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval", "Doi": "10.1109/ICASSP49357.2023.10096882", "Authors": ["l. berry", "y. -j. shih", "h. -f. wang", "h. -j. chang", "h. -y. lee", "d. harwath"], "Key Words": ["visually-grounded speech", "multimodal speech processing", "multilingual speech processing", "self-supervised learning"], "Abstract": "this work investigates the use of large scale english only pre trained models  clip and hubert  for multilingual image speech retrieval. for non english image speech retrieval we outperform the current state of the art performance by a wide margin both when training separate models for each language and with a single model which processes speech in all three languages. we identify key differences in model behavior and performance between english and non english settings attributable to the english only pre training of clip and hubert and investigate how fine tuning the pre trained models impacts these differences. finally we show that our models can be used for mono  and cross lingual speech text retrieval and cross lingual speech speech retrieval despite never having seen any parallel speech text or speech speech data during training.", "Pub Date": "2023-05-05"}