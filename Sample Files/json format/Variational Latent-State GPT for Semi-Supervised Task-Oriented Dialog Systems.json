{"Title": "Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems", "Doi": "10.1109/TASLP.2023.3240661", "Authors": ["h. liu", "y. cai", "z. lin", "z. ou", "y. huang", "j. feng"], "Key Words": ["task oriented dialog systems", "semi-supervised learning", "variational learning", "gpt"], "Abstract": "recently two approaches fine tuning large pre trained language models and variational training have attracted significant interests separately for semi supervised end to end task oriented dialog  tod  systems. in this paper we propose variational latent state gpt model  vls gpt  which is the first to combine the strengths of the two approaches. among many options of models we propose the generative model and the inference model for variational learning of the end to end tod system both as auto regressive language models based on gpt 2 which can be further trained over a mix of labeled and unlabeled dialog data in a semi supervised manner. variational training of vls gpt is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models which use turn level first order markovian. the inference model in vls gpt is non markovian due to the use of the transformer architecture. in this work we establish recursive monte carlo approximation  rmca  to the variational objective with non markovian inference model and prove its unbiasedness. further we develop the computational strategy of sampling then forward computation to realize rmca which successfully overcomes the memory explosion issue of using gpt in variational learning and speeds up training. semi supervised tod experiments are conducted on two benchmark multi domain datasets of different languages   multiwoz2.1 and crosswoz. vls gpt is shown to significantly outperform both supervised only and semi supervised self training baselines.", "Pub Date": "2023-02-17"}