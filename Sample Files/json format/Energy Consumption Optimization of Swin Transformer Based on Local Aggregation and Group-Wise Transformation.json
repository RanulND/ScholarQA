{"Title": "Energy Consumption Optimization of Swin Transformer Based on Local Aggregation and Group-Wise Transformation", "Doi": "10.1109/CVIDL58838.2023.10167219", "Authors": ["y. liu", "s. chen", "z. lei", "p. wang"], "Key Words": ["swin transformer", "model compression", "attention mechanism", "deep learning energy consumption"], "Abstract": "the transformer is a natural language processing model that uses attention mechanisms originally introduced by google in 2017. due to its excellent feature extraction capabilities researchers in the industry have explored the application of transformer in computer vision tasks. following the success of the vision transformer in 2020 the swin transformer was introduced which outperforms traditional convolutional neural networks on various benchmarks in the vision field. as a result swin transformer has become a popular backbone for practical production applications. while swin transformer excels in accuracy its large parameter size and high energy consumption pose challenges for deployment at the edge. addressing this issue is crucial for the wide scale adoption of swin transformer. current research on energy consumption for transformer based models is still in its early stages. to reduce model size and energy consumption while preserving accuracy this study focuses on model compression. specifically we propose a combination of group wise transformation and local aggregation methods from traditional cnn models. by introducing local aggregation and group wise transformation calculation methods while maintaining the original shifted window attention mechanism of swin transformer the proposed approach effectively reduces the number of model parameters. our experimental results show that the swin transformer model trained with this method on an rtx-3090 graphics card can save up to 24% of energy with an accuracy improvement of about 1%.", "Pub Date": "2023-07-10"}