{"Title": "Dynamic Inference with Grounding Based Vision and Language Models", "Doi": "10.1109/CVPR52729.2023.00258", "Authors": ["b. uzkent", "a. garg", "w. zhu", "k. doshi", "j. yi", "x. wang", "m. omar"], "Key Words": ["efficient and scalable vision"], "Abstract": "transformers have been recently utilized for vision and language tasks successfully. for example recent image and language models with more than 200m parameters have been proposed to learn visual grounding in the pre training step and show impressive results on downstream vision and language tasks. on the other hand there exists a large amount of computational redundancy in these large models which skips their run time efficiency. to address this problem we propose dynamic inference for grounding based vision and language models conditioned on the input image text pair. we first design an approach to dynamically skip multihead self attention and feed forward network layers across two backbones and multimodal network. additionally we propose dynamic token pruning and fusion for two backbones. in particular we remove redundant tokens at different levels of the backbones and fuse the image tokens with the language tokens in an adaptive manner. to learn policies for dynamic inference we train agents using reinforcement learning. in this direction we replace the cnn backbone in a recent grounding based vision and language model mdetr with a vision transformer and call it vitmdetr. then we apply our dynamic inference method to vitmdetr called d vitdmetr and perform experiments on image language tasks. our results show that we can improve the run time efficiency of the state of the art models mdetr and glip by up to ~ 50% on referring expression comprehension and segmentation and vqa with only maximum ~ 0.3% accuracy drop.", "Pub Date": "2023-08-22"}