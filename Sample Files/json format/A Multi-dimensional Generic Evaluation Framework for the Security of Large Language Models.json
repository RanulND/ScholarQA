{"Title": "A Multi-dimensional Generic Evaluation Framework for the Security of Large Language Models", "Doi": "10.1109/ICBAIE59714.2023.10281279", "Authors": ["z. yu"], "Key Words": ["security evaluation", "benchmark test", "framework", "generic", "large language model"], "Abstract": "in light of the widespread adoption of large language models their susceptibility to security vulnerabilities cannot be overlooked. as a result it has become imperative to evaluate their proficiency in addressing issues such as toxicity bias and disinformation. however current research focused on appraising and mitigating security risks has predominantly concentrated on specific facets leading to disparities in evaluation criteria. in contrast there has been relatively limited attention given to multidimensional and universal frameworks for security evaluation. in this context this paper delves into the realm of generic evaluation frameworks for security that offer support for cross language and multi category analysis. we underscore the existing challenges associated with prominent large language models concerning security issues and develop a comprehensive test data set to furnish researchers with a tool for quantifying security aspects. through comprehensive evaluations across three major benchmark tests we identify distinct strengths and weaknesses exhibited by each open source large language model to varying degrees. by employing a multi dimensional security evaluation framework we can attain a more holistic comprehension of the performance exhibited by each model across diverse security dimensions. this approach holds significant value in advancing the domain of security research and facilitating the practical application of language models.", "Pub Date": "2023-10-17"}