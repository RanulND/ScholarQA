{"Title": "Transfer Learning Approach to Multilabel Biomedical Literature Classification using Transformer Models", "Doi": "10.1109/I2CT57861.2023.10126262", "Authors": ["p. d. thushari", "s. niazi", "s. meena"], "Key Words": ["transformers", "biomedical literature", "bidirectional encoder representations", "multilabel biomedical text classification", "feed forward neural networks"], "Abstract": "the recently developed transformer models have a significant influence on natural language processing  nlp  research. transformer based models have attained innovative results on numerous nlp benchmarks in linguistics. researchers have also investigated transformer models for a variety of applications in the biomedical field. to facilitate information classification this work aims to thoroughly investigate four state of the art transformer based models  bert roberta scibert and clinicalbert . large language model training however requires a lot of time and processing resources. as a result pre trained language models like bert are advantageous because they  1  offer cutting edge performance  2  the knowledge gained can be used to do several tasks such as categorization and  3  free up practitioners from having to gather enough resources  including the hardware data and time  to train models. however pre trained models possibly will perform poorly in some domains since they are generic. here we inspect the case of multilabel classification for biomedical literature which is a domain that has received relatively minor attention in the literature assessing pre trained language models. based on our study using scibert as the baseline model gave the best results compared to the rest of the models with an 89.35% micro f1 score. in the context of classifying biomedical literature domain specific pre trained models tend to perform better compared to generic models.", "Pub Date": "2023-05-23"}