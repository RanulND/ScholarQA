{"Title": "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models", "Doi": "10.1109/LCA.2023.3305386", "Authors": ["j. choi", "j. park", "k. kyung", "n. s. kim", "j. h. ahn"], "Key Words": ["transformer-based generative model", "processing-in-memory", "attention"], "Abstract": "transformer based generative models such as gpt summarize an input sequence by generating key value  kv  matrices through attention and generate the corresponding output sequence by utilizing these matrices once per token of the sequence. both input and output sequences tend to get longer which improves the understanding of contexts and conversation quality. these models are also typically batched for inference to improve the serving throughput. all these trends enable the models\u201a\u00e4\u00f4 weights to be reused effectively increasing the relative importance of sequence generation especially in processing kv matrices through attention. we identify that the conventional computing platforms  e.g. gpus  are not efficient at handling this attention part for inference because each request generates different kv matrices it has a low operation per byte ratio regardless of the batch size and the aggregate size of the kv matrices can even surpass that of the entire model weights. this motivates us to propose attacc which exploits the fact that the kv matrices are written once during summarization but used many times  proportional to the output sequence length  each multiplied by the embedding vector corresponding to an output token. the volume of data entering leaving attacc could be more than orders of magnitude smaller than what should be read internally for attention. we design attacc with multiple processing in memory devices each multiplying the embedding vector with the portion of the kv matrices within the devices saving external  inter device  bandwidth and energy consumption.", "Pub Date": "2023-09-22"}