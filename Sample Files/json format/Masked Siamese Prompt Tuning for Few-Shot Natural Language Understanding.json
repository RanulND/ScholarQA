{"Title": "Masked Siamese Prompt Tuning for Few-Shot Natural Language Understanding", "Doi": "10.1109/TAI.2023.3275132", "Authors": ["s. ni", "h. -y. kao"], "Key Words": ["few-shot", "masked language model (mlm)", "natural language understanding", "prompt learning"], "Abstract": "recently prompt based learning has shown excellent performance on few shot scenarios. using frozen language models to tune trainable continuous prompt embeddings has become a popular and powerful methodology. for few shot natural language understanding even if we freeze the parameters of the pretrained language model the learned pseudo prompts might still be overfitted. in this article we propose a novel masked siamese prompt tuning  msp tuning  to improve few shot natural language understanding. concretely msp tuning masks randomly out part of the prompt tokens to get a pair of masked siamese prompts for each sample. each training sample is then fed to the model twice with the masked siamese prompts. finally msp tuning minimizes the jensen\u201a\u00e4\u00ecshannon divergence  jsd  between the two output probability distributions of the pretrained language model to regularize the model further. experiment results on the few shot glue benchmark and superglue benchmark show that msp tuning outperforms previous approaches. numerically our msp tuning achieves an average improvement of 1.79%  bert base  and 1.39%  bert large  of the glue benchmark and 1.90%  roberta base  and 1.71%  roberta large  of the superglue benchmark compared to state of the art method p tuning. our method facilitates applying large pretrained language models in natural language understanding.", "Pub Date": "2024-02-12"}