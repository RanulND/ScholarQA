{"Title": "MO-AVC: Deep-Reinforcement-Learning-Based Trajectory Control and Task Offloading in Multi-UAV-Enabled MEC Systems", "Doi": "10.1109/JIOT.2023.3329869", "Authors": ["z. gao", "l. yang", "y. dai"], "Key Words": ["mobile edge computing (mec)", "multiobjective reinforcement learning (morl)", "task offloading", "trajectory control", "unmanned aerial vehicle (uav)"], "Abstract": "we investigate the joint trajectory control and task offloading  jtcto  problem in multiunmanned aerial vehicle  uav  enabled mobile edge computing  mec . however existing jtcto solutions primarily focus on fixed uav enabled mec scenario variations and necessitate extensive interaction to adapt to new scenarios. moreover we consider minimizing task latency and uav\u201a\u00e4\u00f4s energy consumption and maximizing the quantity of tasks collected by the uav as optimization goals. however this optimization problem is characterized by multiple conflicting goals that should be adjusted according to their relative significance. in this article we present a multiobjective actor variations critic based jtcto solution  mo avc . first a group of reinforcement learning strategies is utilized to collect experience on training scenarios which are employed to learn embeddings of both strategies and scenarios. further these two embeddings are used as inputs to train the actor variations critic  avc  which explicitly estimates the total return in a space of jtcto strategies and uav enabled mec scenarios. when adapting to a new scenario just a few steps of scenario interaction are enough to predict the scenario embedding thus selecting strategies by maximizing the trained avc. second we propose an actor conditioned critic framework where the outputs are conditioned on the varying significance of goals and present a weight dynamic memory based experience replay to address the intrinsic instability of the dynamic weight context. finally simulation results show that mo avc can quickly adapt to new scenarios. moreover mo avc reduces the latency by 7.56%\u201a\u00e4\u00ec10.57% the energy consumption by 11.11%\u201a\u00e4\u00ec17.27% and increases the tasks number by 10.33%\u201a\u00e4\u00ec15.54% compared to existing solutions.", "Pub Date": "2024-03-27"}