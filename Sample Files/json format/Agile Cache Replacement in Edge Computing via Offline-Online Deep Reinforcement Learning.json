{"Title": "Agile Cache Replacement in Edge Computing via Offline-Online Deep Reinforcement Learning", "Doi": "10.1109/TPDS.2024.3368763", "Authors": ["z. wang", "j. hu", "g. min", "z. zhao", "z. wang"], "Key Words": ["cache replacement", "convolutional neural network", "deep reinforcement learning", "edge computing", "offline training"], "Abstract": "one fundamental problem of content caching in edge computing is how to replace contents in edge servers with limited capacities to meet the dynamic requirements of users without knowing their preferences in advance. recently online deep reinforcement learning  drl  based caching methods have been developed to address this problem by learning an edge cache replacement policy using samples collected from continuous interactions  trial and error  with the environment. however in practice the online data collection phase is often expensive and time consuming thus hindering the practical deployment of online drl based methods. to bridge this gap we propose a novel agile edge cache replacement method based on offline online deep reinforcement learning  acorn  which can efficiently learn an edge cache replacement policy offline from a training dataset collected by a behavior policy  e.g. least recently used  and then improve it with fast online fine tuning. we also design a specific convolutional neural network structure with multiple branches to effectively extract content popularity knowledge from the dataset. experimental results show that the offline policy generated by acorn outperforms the behavior policy by up to 38%. through online fine tuning acorn also achieves the number of cache hits as good as that of several advanced drl based methods while significantly reducing the number of training epochs by up to 40%.", "Pub Date": "2024-03-07"}