{"Title": "Self-Attention and Transformers: Driving the Evolution of Large Language Models", "Doi": "10.1109/ICEICT57916.2023.10245906", "Authors": ["q. luo", "w. zeng", "m. chen", "g. peng", "x. yuan", "q. yin"], "Key Words": ["self-attention", "transformer", "large language model (llm)", "natural language processing (nlp)", "generative pretrained transformer (gpt)"], "Abstract": "transformers originally introduced for machine translation and built upon the self attention mechanism have undergone a remarkable evolution establishing themselves as the bedrock of large language models  llms . their unparalleled capacity to model intricate relationships and capture extensive dependencies within sequences has propelled their prominence. this article presented in a popular science format serves as an introduction to the transformer architecture elucidating its innovative structure that enables efficient processing of long sequences and capturing dependencies over extended distances. we believe that this resource will prove valuable to college students or youth researchers aspiring to delve into the study and research of modern artificial intelligence  ai  domains.", "Pub Date": "2023-09-22"}