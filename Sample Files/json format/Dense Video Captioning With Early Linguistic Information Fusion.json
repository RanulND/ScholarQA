{"Title": "Dense Video Captioning With Early Linguistic Information Fusion", "Doi": "10.1109/TMM.2022.3146005", "Authors": ["n. aafaq", "a. mian", "n. akhtar", "w. liu", "m. shah"], "Key Words": ["context modeling", "dense video captioning", "event localisation", "language and vision", "video captioning"], "Abstract": "dense captioning methods generally detect events in videos first and then generate captions for the individual events. events are localized solely based on the visual cues while ignoring the associated linguistic information and context. whereas end to end learning may implicitly take guidance from language these methods still fall short of the power of explicit modeling. in this paper we propose a visual semantic embedding  vise  framework that models the word s  context distributional properties over the entire semantic space and computes weights for all the n grams such that higher weights are assigned to the more informative n grams. the weights are accounted for in learning distributed representations of all the captions to construct a semantic space. to perform the contextualization of visual information and the constructed semantic space in a supervised manner we design visual semantic joint modeling network  vsjm net . the learned vise embeddings are then temporally encoded with a hierarchical descriptor transformer  hdt . for caption generation we exploit a transformer architecture to decode the input embeddings into natural language descriptions. experiments on the large scale activitynet captions dataset and youcook ii dataset demonstrate the efficacy of our method.", "Pub Date": "2023-06-08"}