{"Title": "Generative Pre-Trained Transformer for Kazakh Text Generation Tasks", "Doi": "10.1109/OPCS59592.2023.10275765", "Authors": ["g. tolegen", "a. toleu", "r. mussabayev", "b. zhumazhanov", "g. ziyatbekova"], "Key Words": ["large language model", "transformer", "text generation", "low-resource language", "kazakh language"], "Abstract": "this paper presents an empirical study evaluating text generation models for the low resource and morphologically complex kazakh language. in this study we leveraged a transformer based neural architecture. initially we trained a large language model for the kazakh language using a substantial text corpus. then we fine tuned the base model using a specialized question answering dataset. the proposed pretrained language model  plm  was evaluated on a conversation response generation task investigating its performance across domains and languages. the findings of cross lingual comparison underlines the challenges of achieving good results for low resource languages like kazakh with current solutions. the experimental findings revealed that despite the low resource characteristics of kazakh language kazakh plms and its fin tuned model for question answering were obtained and its performance for the bleu score was 8.5% which was better than the random guess 4.01%. additionally two kazakh text corpora were collected for this work. one of datasets comprised a large collection of kazakh text from different domains and was used to train a large language model. second datasets were specifically collected for the question answering task for kazakh and russian languages.", "Pub Date": "2023-10-13"}