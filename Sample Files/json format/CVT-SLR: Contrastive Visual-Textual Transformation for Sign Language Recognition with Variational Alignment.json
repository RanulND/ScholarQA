{"Title": "CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment", "Doi": "10.1109/CVPR52729.2023.02216", "Authors": ["j. zheng", "y. wang", "c. tan", "s. li", "g. wang", "j. xia", "y. chen", "s. z. li"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "sign language recognition  slr  is a weakly supervised task that annotates sign videos as textual glosses. recent studies show that insufficient training caused by the lack of large scale available sign datasets becomes the main bottleneck for slr. most slr works thereby adopt pretrained visual modules and develop two mainstream solutions. the multi stream architectures extend multi cue visual features yielding the current sota performances but requiring complex designs and might introduce potential noise. alternatively the advanced single cue slr frameworks using explicit cross modal alignment between visual and textual modalities are simple and effective potentially competitive with the multi cue framework. in this work we propose a novel contrastive visual textual transformation for slr cvt slr to fully explore the pretrained knowledge of both the visual and language modalities. based on the single cue cross modal alignment framework we propose a variational autoencoder  vae  for pretrained contextual knowledge while introducing the complete pretrained language module. the vae implicitly aligns visual and textual modalities while benefiting from pretrained contextual knowledge as the traditional contextual module. meanwhile a contrastive cross modal alignment algorithm is designed to explicitly enhance the consistency constraints. extensive experiments on public datasets  phoenix 2014 and phoenix 2014t  demonstrate that our proposed cvt slr consistently outperforms existing single cue methods and even outperforms sota multi cue methods. the source codes and models are available at https //github.com binbinjiang/cvt slr.", "Pub Date": "2023-08-22"}