{"Title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis Using Learned Phonetic Representations", "Doi": "10.1109/TASLP.2023.3313424", "Authors": ["c. liu", "z. -h. ling", "l. -h. chen"], "Key Words": ["speech synthesis", "multilingual", "wav2vec 2.0", "phonetic representations"], "Abstract": "this article presents a multilingual speech synthesis approach that leverages learned phonetic representations to eliminate the need for pronunciation dictionaries in target languages. the learned phonetic representations consist of unsupervised phonetic representations  upr  and supervised phonetic representations  spr . to extract uprs a pre trained wav2vec 2.0 model is utilized while a language independent automatic speech recognition  li asr  model with a connectionist temporal classification  ctc  loss is employed to derive segment level sprs from the speech data of target languages. an acoustic model using uprs and sprs as intermediate representations is then designed comprising a upr predictor an spr predictor and a representation to mel spectrogram  rtm  converter. the two predictors generate uprs and sprs from texts respectively. the rtm converter first combines uprs with sprs using a transformer based encoder and then feeds the merged representations into a decoder to produce mel spectrograms. considering the difficulty of collecting large training corpora for all languages in multilingual speech synthesis the parameters of both the two predictors and the rtm converter can be pre trained on non target languages to further improve model performance. experimental results on six target languages demonstrate that our method outperformed the approaches directly predicting mel spectrograms from character or phoneme sequences and pre training the acoustic model using a multilingual corpus further improved the performance of synthetic speech.", "Pub Date": "2023-10-23"}