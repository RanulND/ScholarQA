{"Title": "Explaining Transformer-based Code Models: What Do They Learn? When They Do Not Work?", "Doi": "10.1109/SCAM59687.2023.00020", "Authors": ["a. h. mohammadkhani", "c. tantithamthavorn", "h. hemmatif"], "Key Words": ["explainable ai (xai)", "llm", "code models", "interpretable", "attention", "transformer."], "Abstract": "in recent years there has been a wide interest in designing deep neural network based models that automate downstream software engineering tasks on source code such as code document generation code search and program repair. although the main objective of these studies is to improve the effectiveness of the downstream task many studies only attempt to employ the next best neural network model without a proper in depth analysis of why a particular solution works or does not on particular tasks or scenarios. in this paper using an example explainable ai  xai  method  attention mechanism  we study two recent large language models  llms  for code  codebert and graphcodebert  on a set of software engineering downstream tasks  code document generation  cdg  code refinement  cr  and code translation  ct . through quantitative and qualitative studies we identify what codebert and graphcodebert learn  put the highest attention on in terms of source code token types  on these tasks. we also show some of the common patterns when the model does not work as expected  performs poorly even on easy problems  and suggest recommendations that may alleviate the observed challenges.", "Pub Date": "2023-12-20"}