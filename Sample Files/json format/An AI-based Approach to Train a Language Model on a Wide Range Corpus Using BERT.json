{"Title": "An AI-based Approach to Train a Language Model on a Wide Range Corpus Using BERT", "Doi": "10.1109/SCEECS61402.2024.10482037", "Authors": ["a. bhutda", "g. sakarkar", "n. shelke", "k. paithankar", "r. panchal"], "Key Words": ["bert", "transformer", "natural language processing (nlp)", "tokenizer", "medical corpus"], "Abstract": "in 2018 google developed and released bert  bidirectional encoder representation from transformers  for natural language processing  nlp . the english language models in bert are two. to help close this data gap a range of techniques have been developed for pre training general purpose language representation models on the huge amount of unannotated content available on the web. unlike when bert is trained on these datasets from scratch the model must first be pre trained on small amounts of data. once a measurable result is obtained it can then be applied to tasks like nlp sentiment analysis and question answering which causes to a notable rise in accuracy. the purpose of this research is to train the language model and conduct a descriptive study of bert in various applications. for the implementation throughout the task we are taking a famous textbook of medicine \"harrison\u201a\u00e4\u00f4s principle of internal medicine\" into consideration. the research has made advantage of high configuration computation specifically the nvidia tesla dgx v100 gpu.", "Pub Date": "2024-04-02"}