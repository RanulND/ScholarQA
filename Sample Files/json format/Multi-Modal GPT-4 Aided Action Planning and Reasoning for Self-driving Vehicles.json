{"Title": "Multi-Modal GPT-4 Aided Action Planning and Reasoning for Self-driving Vehicles", "Doi": "10.1109/ICASSP48485.2024.10446745", "Authors": ["f. chi", "y. wang", "p. nasiopoulos", "v. c. m. leung"], "Key Words": ["autonomous driving", "large language model", "multi-modal", "object detection", "segmentation"], "Abstract": "explainable decision making is critical for building trust in autonomous vehicles. we investigate the use of a pre trained large language model  llm  to derive comprehensible driving decisions from multi modal time series data captured by a monocular camera on an autonomous vehicle. leveraging a graph of thought structure the llm learns policies that perform robustly while generating natural language rationales. we generate a novel multi modal dataset with sequential images scene labels and driving actions. results demonstrate our method produces human  understandable explanations for its driving choices providing transparency. our work indicates incorporating language based reasoning enables accountable and transparent decision making for self driving cars making llm a potential solution for autonomous driving.", "Pub Date": "2024-03-18"}