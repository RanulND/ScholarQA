{"Title": "A Floating-Point 6T SRAM In-Memory-Compute Macro Using Hybrid-Domain Structure for Advanced AI Edge Chips", "Doi": "10.1109/JSSC.2023.3309966", "Authors": ["p. -c. wu", "j. -w. su", "l. -y. hong", "j. -s. ren", "c. -h. chien", "h. -y. chen", "c. -e. ke", "h. -m. hsiao", "s. -h. li", "s. -s. sheu", "w. -c. lo", "s. -c. chang", "c. -c. lo", "r. -s. liu", "c. -c. hsieh", "k. -t. tang", "m. -f. chang"], "Key Words": ["artificial intelligence (ai)", "compute-in-memory (cim)", "floating-point (fp)", "inference", "static random access memory (sram)"], "Abstract": "advanced artificial intelligence edge devices are expected to support floating point  fp  multiply and accumulation operations while ensuring high energy efficiency and high inference accuracy. this work presents an fp compute in memory  cim  macro that exploits the advantages of computing in the time digital and analog voltage domain for high energy efficiency and accuracy. this work employs  1  a hybrid domain macrostructure to enable the computation of both the exponent and mantissa within the same cim macro  2  a time domain computing scheme for energy efficient exponent computation  3  a product exponent based input mantissa alignment scheme to enable the accumulation of the product mantissa in the same column  and 4  a place value dependent digital\u201a\u00e4\u00ecanalog hybrid computing scheme to enable energy efficient mantissa computations of sufficient accuracy. a 22 nm 832 knowledge base fp cim macro fabricated using foundry provided compact 6t static random access memory  sram  cells achieved a high energy efficiency of 72.14 tera floating point operations per second  tflops /w while performing fp multiply and accumulate  mac  operations involving bf16 input bf16 weight fp32 output and 128 accumulations.", "Pub Date": "2023-12-28"}