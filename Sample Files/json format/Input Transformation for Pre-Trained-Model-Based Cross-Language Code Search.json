{"Title": "Input Transformation for Pre-Trained-Model-Based Cross-Language Code Search", "Doi": "10.1109/QRS-C60940.2023.00021", "Authors": ["m. geng", "d. dong", "p. lu"], "Key Words": ["cross-language code-to-code search", "graph to sequence", "pretrained language models"], "Abstract": "cross language code to code search has great po tential to boost software development and software mainte nance. however performing this task is nontrivial since it requires to accurately understand the semantics of code written in different programming languages. to address this challenge a natural idea is to leverage the power of pretrained language models trained on diverse languages and have shown great potential in producing high quality representations for code across different languages. a dominating way of utilizing pretrained models is to directly use code token sequences as the inputs due to their transformer based architectures. nonetheless beyond the lexical information code snippets inherently contain rich semantic information which may not be adequately captured through the token sequence. to overcome this limitation we propose an input transformation approach that given a code snippet can generate a sequence with semantic information as the input to the pretrained model which enables us to effectively obtain the representations of the code. our key insight is that code snippets in different languages that implement the identical functionality although may differ significantly with respect to the token sequences or the syntactic structures could share certain similarities regarding to their program dependency graphs  pdgs . therefore instead of directly using the token sequence we propose to first build the semantic graph that can model the semantics of code in different languages based on the data flow and control flow information by optimizing the pdgs. after that a graph to sequence transformation module is designed and the final transformation result can be obtained. finally the contrastive learning is exploited to fine tune the model. our large scale evaluation results show that our method can achieve promising effectiveness because it consistently outperforms the state of the art c4 approach by at least 6% with respect to the mean reciprocal rank  mrr  value under six different settings.", "Pub Date": "2024-02-19"}