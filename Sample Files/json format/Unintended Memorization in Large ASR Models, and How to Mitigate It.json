{"Title": "Unintended Memorization in Large ASR Models, and How to Mitigate It", "Doi": "10.1109/ICASSP48485.2024.10446083", "Authors": ["l. wang", "o. thakkar", "r. mathews"], "Key Words": ["memorization", "automatic speech recognition", "gradient clipping", "privacy auditing", "exposure"], "Abstract": "it is well known that neural networks can unintentionally memorize their training examples causing privacy concerns. however auditing memorization in large non auto regressive automatic speech recognition  asr  models has been challenging due to the high compute cost of existing methods such as hardness calibration. in this work we design a simple auditing method to measure memorization in large asr models without the extra compute overhead. concretely we speed up randomly generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. hence accurate predictions only for sped up training examples can serve as clear evidence for memorization and the corresponding accuracy can be used to measure memorization. using the proposed method we showcase memorization in the state of the art asr models. to mitigate memorization we tried gradient clipping during training to bound the influence of any individual example on the final model. we empirically show that clipping each example\u201a\u00e4\u00f4s gradient can mitigate memorization for sped up training examples with up to 16 repetitions in the training set. furthermore we show that in large scale distributed training clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.", "Pub Date": "2024-03-18"}