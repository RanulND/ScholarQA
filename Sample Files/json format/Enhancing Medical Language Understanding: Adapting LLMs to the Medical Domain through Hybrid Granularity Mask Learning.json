{"Title": "Enhancing Medical Language Understanding: Adapting LLMs to the Medical Domain through Hybrid Granularity Mask Learning", "Doi": "10.1109/BIBM58861.2023.10385287", "Authors": ["l. fan", "x. liu", "y. wang", "g. yang", "z. du", "g. wang"], "Key Words": ["language models", "domain adaptation", "medical knowledge", "mask learning", "medical question-answering"], "Abstract": "large language models have made remarkable strides in natural language understanding and generation. however their performance in specialized fields like medicine often falls short due to the lack of domain specific knowledge during pre training. while fine tuning on labeled medical data is a common approach for task adaptation it may not capture the comprehensive medical knowledge required. in this paper we proposed a hybrid granularity mask learning  hgm  method for domain adaptation in the medical field. our method incorporates multi level linguistic characteries including token entity and subsentence to enable the model to acquire medical knowledge comprehensively. we fine tune a medical specific language model derived from chatglm 6b and bloom 7b on downstream medical tasks and evaluate its performance. the results demonstrate a significant improvement compared to the baseline thus affirming the effectiveness of our proposed method.", "Pub Date": "2024-01-18"}