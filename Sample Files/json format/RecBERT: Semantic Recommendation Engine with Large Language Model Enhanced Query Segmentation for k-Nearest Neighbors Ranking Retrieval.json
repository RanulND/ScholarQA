{"Title": "RecBERT: Semantic Recommendation Engine with Large Language Model Enhanced Query Segmentation for k-Nearest Neighbors Ranking Retrieval", "Doi": "10.23919/ICN.2024.0004", "Authors": ["r. wu"], "Key Words": ["sentence transformer", "simple contrastive learning", "large language models", "query segmentation", "k-nearest neighbors"], "Abstract": "the increasing amount of user traffic on internet discussion forums has led to a huge amount of unstructured natural language data in the form of user comments. most modern recommendation systems rely on manual tagging relying on administrators to label the features of a class or story which a user comment corresponds to. another common approach is to use pre trained word embeddings to compare class descriptions for textual similarity then use a distance metric such as cosine similarity or euclidean distance to find top $k$ neighbors. however neither approach is able to fully utilize this user generated unstructured natural language data reducing the scope of these recommendation systems. this paper studies the application of domain adaptation on a transformer for the set of user comments to be indexed and the use of simple contrastive learning for the sentence transformer fine tuning process to generate meaningful semantic embeddings for the various user comments that apply to each class. in order to match a query containing content from multiple user comments belonging to the same class the construction of a subquery channel for computing class level similarity is proposed. this channel uses query segmentation of the aggregate query into subqueries performing k nearest neighbors  knn  search on each individual subquery. recbert achieves state of the art performance outperforming other state of the art models in accuracy precision recall and f1 score for classifying comments between four and eight classes respectively. recbert outperforms the most precise state of the art model  distilroberta  in precision by 6.97% for matching comments between eight classes.", "Pub Date": "2024-03-28"}