{"Title": "Divisible Task Offloading for Multiuser Multiserver Mobile Edge Computing Systems Based on Deep Reinforcement Learning", "Doi": "10.1109/ACCESS.2023.3302528", "Authors": ["l. tang", "h. qin"], "Key Words": ["mobile edge computing", "the divisible task offloading", "double deep q-network", "self-adaptive \u0153\u00b5-greedy exploration strategy", "prioritized experience replay technique"], "Abstract": "mobile edge computing  mec  is a promising computing paradigm that enables offloading tasks to edge servers to decrease the load on user equipment  ue  and the latency of services. however blind offloading of data intensive tasks may cause edge node congestion and increase the task\u201a\u00e4\u00f4s time delay. therefore developing a new incentive deep reinforcement learning based scheme is imperative to solve such divisible task offloading in this end edge cloud orchestrated environment. first we build a novel mec based state space framework containing multiple servers and ue and introduce a new dynamic task division into multiple subtasks mode to execute in parallel on multiple nodes with multiple service providers. next we model the process of the divisible task offloading by the markov decision process  mdp  and derive a sufficient condition in the subtask offloading to maximize the quality of physical experience  qope  of the divisible task. finally we propose a new self adaptive q network with prioritized experience replay  sq per  algorithm based on double deep q network  ddqn  in which the experience replay technique and the traditional  $\\varepsilon $  greedy exploration strategy are optimized to improve learning efficiency and stability. simulation results show that the sq per algorithm makes better utilization of environmental resources and significantly reduces time delay of the task than other methods in complex scenarios.", "Pub Date": "2023-08-14"}