{"Title": "Distractor Generation Through Text-to-Text Transformer Models", "Doi": "10.1109/ACCESS.2024.3361673", "Authors": ["d. de-fitero-dominguez", "e. garcia-lopez", "a. garcia-cabot", "j. -a. del-hoyo-gabaldon", "a. moreno-cediel"], "Key Words": ["artificial intelligence", "natural languages", "natural language processing", "computer applications", "educational technology"], "Abstract": "in recent years transformer language models have made a significant impact on automatic text generation. this study focuses on the task of distractor generation in spanish using a fine tuned multilingual text to text model namely mt5. our method outperformed established baselines based on lstm networks confirming the effectiveness of transformer architectures in such nlp tasks. while comparisons with other transformer based solutions yielded diverse outcomes based on the metric of choice our method notably achieved superior results on the rouge metric compared to the gpt-2 approach. although traditional evaluation metrics such as bleu and rouge are commonly used this paper argues for more context sensitive metrics given the inherent variability in acceptable distractor generation results. among the contributions of this research is a comprehensive comparison with other methods an examination of the potential drawbacks of multilingual models and the introduction of alternative evaluation metrics. future research directions derived from our findings and a review of related works are also suggested with a particular emphasis on leveraging other language models and transformer architectures.", "Pub Date": "2024-02-23"}