{"Title": "Personalized Edge Intelligence via Federated Self-Knowledge Distillation", "Doi": "10.1109/TPDS.2022.3225185", "Authors": ["h. jin", "d. bai", "d. yao", "y. dai", "l. gu", "c. yu", "l. sun"], "Key Words": ["edge computing", "personalized federated learning", "knowledge distillation"], "Abstract": "federated learning  fl  is an emerging approach in edge computing for collaboratively training machine learning models among multiple devices which aims to address limited bandwidth system heterogeneity and privacy issues in traditional centralized training. however the existing federated learning methods focus on learning a shared global model for all devices which may not always be ideal for different devices. such situations become even worse when each edge device has its own data distribution or task. in this paper we study personalized federated learning in which our goal is to train models to perform well for individual clients. we observe that the initialization in each communication round causes the forgetting of historical personalized knowledge. based on this observation we propose a novel personalized federated learning  pfl  framework via self knowledge distillation named pfedsd. by allowing clients to distill the knowledge of previous personalized models to current local models pfedsd accelerates the process of recalling the personalized knowledge for the latest initialized clients. moreover self knowledge distillation provides different views of data in feature space to realize an implicit ensemble of local models. extensive experiments on various datasets and settings demonstrate the effectiveness and robustness of pfedsd.", "Pub Date": "2022-12-21"}