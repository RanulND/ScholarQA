{"Title": "A Hardware-Friendly Tiled Singular-Value Decomposition-Based Matrix Multiplication for Transformer-Based Models", "Doi": "10.1109/LCA.2023.3323482", "Authors": ["h. li", "j. choi", "y. kwon", "j. h. ahn"], "Key Words": ["transformer-based model", "gpu", "tiled singular vector decomposition"], "Abstract": "transformer based models have become the backbone of numerous state of the art natural language processing  nlp  tasks including large language models. matrix multiplication a fundamental operation in the transformer based models accounts for most of the execution time. while singular value decomposition  svd  can accelerate this operation by reducing the amount of computation and memory footprints through rank size reduction it leads to degraded model quality due to challenges in preserving important information. moreover this method does not effectively utilize the resources of modern gpus. in this paper we propose a hardware friendly approach  matrix multiplication based on tiled singular value decomposition  tsvd . tsvd divides a matrix into multiple tiles and performs matrix factorization on each tile using svd. by breaking down the process into smaller regions tsvd mitigates the loss of important data. we apply the matrices decomposed by tsvd for matrix multiplication and our tsvd based matrix multiplication  tsvd matmul  leverages gpu resources more efficiently compared to the svd approach. as a result tsvd matmul achieved a speedup of 1.03\u221a\u00f3 to 3.24\u221a\u00f3 compared to the svd approach at compression ratios ranging from 2 to 8. when deployed to gpt 2 tsvd not only performs competitively with a full fine tuning on the e2e nlg task but also achieves a speedup of 1.06\u221a\u00f3 to 1.24\u221a\u00f3 at 2 to 8 compression ratios while increasing accuracy by up to 1.5 bleu score.", "Pub Date": "2023-11-20"}