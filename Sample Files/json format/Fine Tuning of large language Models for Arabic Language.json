{"Title": "Fine Tuning of large language Models for Arabic Language", "Doi": "10.1109/AICCSA59173.2023.10479346", "Authors": ["a. tamer", "a. -a. hassan", "a. ali", "n. salah", "w. medhat"], "Key Words": ["language model", "arabic language", "prompt engineering"], "Abstract": "in recent years long language models have made significant progress enabling machines to interpret and process human language. however the arabic language presents unique challenges due to its rich morphology and diverse sentence structures. the development of specialized language models for arabic question answering has implications for improved human  computer interaction cultural preservation and accessibility. this paper aims to enhance the comprehension and contextual understanding of arabic posed questions by leveraging the capabilities of the llama language model and the xlnet transformer. the arcd dataset which mainly consists of an arabic dataset for question answering was used to fine  tune the llama 2.0 and xlnet. by utilizing llama and xlnet transformers separately this paper contributes to the construction of an nlp pipeline that can properly understand and process arabic text to provide answers depending on a particular arabic context by using llama and xlnet transformers individually. it is important to note that arabic datasets were not previously used to train the llama language model. the llama language model received accuracy scores of 93.70", "Pub Date": "2024-04-02"}