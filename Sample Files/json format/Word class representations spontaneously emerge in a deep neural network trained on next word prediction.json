{"Title": "Word class representations spontaneously emerge in a deep neural network trained on next word prediction", "Doi": "10.1109/ICMLA58977.2023.00223", "Authors": ["k. surendra", "a. schilling", "p. stoewer", "a. maier", "p. krauss"], "Key Words": ["successor representations", "cognitive maps", "word classes", "deep neural networks", "text prediction", "syntax", "construction grammar", "usage-based models of language acquisition", "chatgpt"], "Abstract": "how do humans learn language and can the first language be learned at all? these fundamental questions are still hotly debated. in contemporary linguistics there are two major schools of thought that give completely opposite answers. according to chomsky theory of universal grammar language cannot be learned because children are not exposed to sufficient data in their linguistic environment. in contrast usage based models of language assume a profound relationship between language structure and language use. in particular contextual mental processing and mental representations are assumed to have the cognitive capacity to capture the complexity of actual language use at all levels. the prime example is syntax i.e. the rules by which words are assembled into larger units such as sentences. typically syntactic rules are expressed as sequences of word classes. however it remains unclear whether word classes are innate as implied by universal grammar or whether they emerge during language acquisition as suggested by usage based approaches. here we address this issue from a machine learning and natural language processing perspective. in particular we trained an artificial deep neural network on predicting the next word provided sequences of consecutive words as input. subsequently we analyzed the emerging activation patterns in the hidden layers of the neural network. strikingly we find that the internal representations of nine word input sequences cluster according to the word class of the tenth word to be predicted as output even though the neural network did not receive any explicit information about syntactic rules or word classes during training. this surprising result suggests that also in the human brain abstract representational categories such as word classes may naturally emerge as a consequence of predictive coding and processing during language acquisition.", "Pub Date": "2024-03-19"}