{"Title": "English to Kannada Translation Using BERT Model", "Doi": "10.1109/NMITCON58196.2023.10276314", "Authors": ["s. shetty", "p. n. nayak g", "p. mady", "v. k. bhustali", "c. hegde"], "Key Words": ["bert", "decoder", "encoder", "fairseq", "mbert", "rnn", "sentencepiece tokenizer", "transformers", "wordpiece tokenizer"], "Abstract": "translation of content on the internet from one language to the other is essential for conveying information to the people around the world. this can be text audio or video based information. today the bert model which stands for bidirectional encoder representations from transformers helps computers understand the meaning of ambiguous language in text by using surrounding text to establish context. in this paper an english to kannada translation model using the pre trained uncased bert framework has been presented. low resource language like kannada requires a vocabulary creation and an appropriate preprocessing. wordpiece and sentencepiece tokenizers have been used for appropriately clean and prepare the data for model building. uncased pre trained bert model is used as an encoder for model building and the fairseq algorithm is used for decoding. the model is trained and tested on samanantar dataset consisting of 6 lakhs of records. the proposed custom model has outperformed for about 29% compared to multilingual bert model and it also outperformed ordinary transformer model by 50%. these findings have proved that custom language models perform better in case of indic languages especially the dravidian languages like kannada.", "Pub Date": "2023-10-17"}