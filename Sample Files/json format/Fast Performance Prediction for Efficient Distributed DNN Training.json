{"Title": "Fast Performance Prediction for Efficient Distributed DNN Training", "Doi": "10.1109/LCA.2023.3316452", "Authors": ["y. yun", "e. park"], "Key Words": ["distributed training", "performance modeling", "large language model", "3d parallelism"], "Abstract": "training large scale dnn models requires parallel distributed training using hyper scale systems. to make the best use of the numerous accelerators it is essential to intelligently combine different parallelization schemes. however as the size of dnn models increases the possible combinations of schemes become enormous and consequently finding the optimal parallel plan becomes exceedingly expensive and practically unfeasible. in this letter we introduce a novel cost model the markovian performance estimator  mpe . this model provides affordable estimates of the throughput of various parallel plans promoting efficient and fast searches for the ideal parallel plan even when resources are limited. significantly this work is pioneering in explaining the expensive nature of searching for an optimal plan and addressing it using intuitive performance estimations based on real device evaluations. our experiments demonstrate the effectiveness of the mpe revealing that it accelerates the optimization process up to 126x faster  36.4 on average  than the existing state of the art baseline alpa.", "Pub Date": "2023-09-27"}