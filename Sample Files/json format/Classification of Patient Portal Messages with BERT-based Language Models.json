{"Title": "Classification of Patient Portal Messages with BERT-based Language Models", "Doi": "10.1109/ICHI57859.2023.00033", "Authors": ["y. ren", "d. wu", "a. khurana", "g. mastorakos", "s. fu", "n. zong", "j. fan", "h. liu", "m. huang"], "Key Words": ["patient portals", "patient portal messages", "natural language processing", "text classification", "deep learning", "language model", "bert"], "Abstract": "the patient portal is a secure platform that enables patients to conveniently access to their medical records and communicate with and seek support from their care teams for various healthcare issues. the care teams are receiving a rising number of patient portal messages  ppms  generated by patients as the increase of patient engagement in patient portal especially during the covid-19 pandemic. it opens great opportunities to develop artificial intelligence  ai  solutions to supports the care teams and reduce their workload. previous studies have shown the potential of machine learning and natural language processing  nlp  for classification of ppms to automate message triage. recent progress in neural network architectures  e.g. the transformer  and the emergence of large scale pre trained language models are pushing the state of the art for multiple nlp tasks including text classification. however there has been very little research studies about the performance of pre trained language models for the ppms classification task. this study proposes to investigate the performance of multiple state of the art nlp pre trained language models based on the language style and the nature of content for the multi category ppms classification task. the results could contribute to developing future advanced and precise triage system of ppms for clinical applications. we developed ppm classifiers leveraging five popular bidirectional encoder representations from transformers  bert  language models including the generic bert models  bert and roberta  domain specific bert models  biobert and bio clinicalbert  and source specific bert model  bertweet  which were pre trained with corpus in different domain and from different sources. the bert based models were fine tuned and evaluated with an annotated corpus of 2239 annotated ppms with four categories. our results show that source specific model bertweet outperformed all other tested models with a 0.78 accuracy score and a 0.75 f-1 score. both domain specific models biobert and bio clinicalbert achieved better results  f1 scores of 0.74 and 0.72  compared with the generic models bert and roberta  f 1 scores of 0.70 and 0.66  respectively. bertweet gained the benefit from the large scale english tweets pre train corpus which has informal and colloquial language similar to ppms. the domain specific models biobert and bio clinicalbert performed better than the generic models because they were pre trained with large scale biomedical and clinical corpora and can learn better representation of ppms for better recognition of health concepts. the results show that the language models pre trained by a corpus with similar language style and domain concepts are beneficial to the downstream ppm classification. it would be worth to explore the combination of multiple featured language models in order to further improve the model performance.", "Pub Date": "2023-12-11"}