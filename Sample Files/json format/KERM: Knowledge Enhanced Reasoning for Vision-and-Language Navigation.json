{"Title": "KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation", "Doi": "10.1109/CVPR52729.2023.00254", "Authors": ["x. li", "z. wang", "j. yang", "y. wang", "s. jiang"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "vision and language navigation  vln  is the task to enable an embodied agent to navigate to a remote location following the natural language instruction in real scenes. most of the previous approaches utilize the entire features or object centric features to represent navigable candidates. however these representations are not efficient enough for an agent to perform actions to arrive the target location. as knowledge provides crucial information which is complementary to visible content in this paper we propose a knowledge enhanced reasoning model  kerm  to leverage knowledge to improve agent navigation ability. specifically we first retrieve facts  i.e. knowledge described by language descriptions  for the navigation views based on local regions from the constructed knowledge base. the re trieved facts range from properties of a single object  e.g. color shape  to relationships between objects  e.g. action spatial position  providing crucial information for vln. we further present the kerm which contains the purification fact aware interaction and instruction guided aggregation modules to integrate visual history instruction and fact features. the proposed kerm can automatically select and gather crucial and relevant cues obtaining more accurate action prediction. experimental results on the reverie r2r and soon datasets demonstrate the effectiveness of the proposed method. the source code is available at https //github.com xiangyangli20/kerm.", "Pub Date": "2023-08-22"}