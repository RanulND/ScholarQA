{"Title": "JEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition", "Doi": "10.1109/ICASSP49357.2023.10095249", "Authors": ["z. meng", "w. wang", "r. prabhavalkar", "t. n. sainath", "t. chen", "e. variani", "y. zhang", "b. li", "a. rosenberg", "b. ramabhadran"], "Key Words": ["speech recognition", "text injection", "internal lm"], "Abstract": "we propose jeit a joint end to end  e2e  model and internal language model  ilm  training method to inject large scale unpaired text into ilm during e2e training which improves rare word speech recognition. with jeit the e2e model computes an e2e loss on audio transcript pairs while its ilm estimates a cross entropy loss on unpaired text. the e2e model is trained to minimize a weighted sum of e2e and ilm losses. during jeit ilm absorbs knowledge from unpaired text while the e2e training serves as regularization. unlike ilm adaptation methods jeit does not require a separate adaptation step and avoids the need for kullback leibler divergence regularization of ilm. we also show that modular hybrid autoregressive transducer  mhat  performs better than hat in the jeit framework and is much more robust than hat during ilm adaptation. to push the limit of unpaired text injection we further propose a combined jeit and joist training  cjjt  that benefits from modality matching encoder text injection and ilm training. both jeit and cjjt can foster a more effective lm fusion. with 100b unpaired sentences jeit cjjt improves rare word recognition accuracy by up to 16.4% over a model trained without unpaired text.", "Pub Date": "2023-05-05"}