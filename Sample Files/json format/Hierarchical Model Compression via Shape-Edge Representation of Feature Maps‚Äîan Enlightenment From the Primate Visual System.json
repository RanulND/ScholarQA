{"Title": "Hierarchical Model Compression via Shape-Edge Representation of Feature Maps\u201a\u00c4\u00eean Enlightenment From the Primate Visual System", "Doi": "10.1109/TMM.2022.3216477", "Authors": ["h. zhang", "l. liu", "b. kang", "n. zheng"], "Key Words": ["model compression", "feature maps", "shape and edge", "primate visual system", "deep neural network"], "Abstract": "the cumbersome computation of deep neural networks  dnns  limits their practical deployment on resource constrained mobile multimedia devices. to deploy dnns on devices with limited computing resources model compression techniques are leveraged to accelerate the networks where network pruning can improve the inference efficiency of dnns by removing redundant weights and structures. as one of the important components of dnns the feature maps  fms  can be leveraged to evaluate the importance of network structures for dnn pruning. however previous methods neglect to fully explore the characteristics of fms in network pruning. in this paper we investigate the high capacity and resource efficient analogy ventral dual pathway primates visual system  pvs  to propose a hierarchical pruning framework  dubbed as hpse . in an efficient pvs the analog pathway analyzes low frequency information to facilitate the high frequency information inference in ventral stream. in hpse we extract the low frequency shape information and high frequency edge information from fms to present a novel pruning pipeline that resembles the analysis mechanism of pvs. in particular we first imitate the analogy pathway to group different fms in each layer by calculating the shape feature overlap. secondly we leverage the edge information modulated by the grouping results of the first step to prune the network. the effectiveness of hpse is verified by pruning various dnns on different benchmarks. for example for resnet 56 on cifar 10 hpse reduces 52.9% of flops with a slight accuracy improvement  for resnet 50 on imagenet we achieve 54.3% flops drop with only 0.49% top 1 accuracy loss.", "Pub Date": "2023-11-06"}