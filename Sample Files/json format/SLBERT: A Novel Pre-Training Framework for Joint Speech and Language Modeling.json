{"Title": "SLBERT: A Novel Pre-Training Framework for Joint Speech and Language Modeling", "Doi": "10.1109/ICASSP49357.2023.10094658", "Authors": ["o. susladkar", "p. gatti", "s. kumar yadav"], "Key Words": ["joint speech and language pre-training", "intent classification", "multimodal sentiment analysis"], "Abstract": "we propose slbert  speech and language pre training framework for bert  an end to end trainable framework for learning joint representations of speech and language modalities. we enhance the well known bert architecture to provide a dual stream multimodal architecture that processes both speech and language input. to enable effective information exchange between the two modalities we introduce a novel attention fusion mechanism via af blocks. to acquire robust contrastive representations for speech and language processing applications we pre train slbert on three auxiliary tasks  masked language modeling masked speech modeling and speech language matching. we evaluate our proposed model on two well known multimodal tasks  intent classification and sentiment analysis. our model achieves state of the art results on both benchmarks while surpassing even larger baselines.", "Pub Date": "2023-05-05"}