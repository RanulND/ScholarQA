{"Title": "Multiway-Adapter: Adapting Multimodal Large Language Models for Scalable Image-Text Retrieval", "Doi": "10.1109/ICASSP48485.2024.10446792", "Authors": ["z. long", "g. killick", "r. mccreadie", "g. a. camarasa"], "Key Words": ["multimodal large language models", "image-text retrieval", "adapter", "transformers", "transfer learning"], "Abstract": "as multimodal large language models  mllms  grow in size adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. indeed traditional fine tuning methods are costly due to the need for extensive task specific training. while efficient adaptation methods exist that aim to reduce these costs in practice they suffer from shallow inter modal alignment which severely hurts model effectiveness. to tackle these computational challenges and improve inter modal alignment we introduce the multiway adapter  mwa  a novel framework featuring an \u201a\u00e4\u00f2alignment enhancer\u201a\u00e4\u00f4. this enhancer deepens inter modal alignment enabling high transferability with minimal tuning effort. our experiments show that unlike prior efficient tuning approaches mwa maintains model effectiveness while reducing training time by up to 57%. mwa is also lightweight increasing model size by only 2 3%  in terms of parameters  for state of the art foundation models like beit 3 large. these results demonstrate that mwa provides an efficient and effective adaptation method for mllms significantly broadening their applicability.", "Pub Date": "2024-03-18"}