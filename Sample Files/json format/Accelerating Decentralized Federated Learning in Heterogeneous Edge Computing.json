{"Title": "Accelerating Decentralized Federated Learning in Heterogeneous Edge Computing", "Doi": "10.1109/TMC.2022.3178378", "Authors": ["l. wang", "y. xu", "h. xu", "m. chen", "l. huang"], "Key Words": ["edge computing", "decentralized federated learning", "topology construction", "model compression"], "Abstract": "in edge computing  ec  federated learning  fl  enables massive devices to collaboratively train ai models without exposing local data. in order to avoid the possible bottleneck of the parameter server  ps  architecture we concentrate on the decentralized federated learning  dfl  which adopts peer to peer  p2p  communication without maintaining a global model. however due to the intrinsic features of ec e.g. resource limitation and heterogeneity network dynamics and non iid data dfl with a fixed p2p topology and or an identical model compression ratio for all workers results in a slow convergence rate. in this paper we propose an efficient algorithm  termed coco  to accelerate dfl by integrating optimization of topology construction and model compression. concretely we adaptively construct p2p topology and determine specific compression ratios for each worker to conquer the system dynamics and heterogeneity under bandwidth constraints. to reflect how the non iid data influence the consistency of local models in dfl we introduce the consensus distance i.e. the discrepancy between local models as the quantitative metric to guide the fine grained operations of the joint optimization. extensive simulations and testbed experiments show that coco achieves 10\u221a\u00f3 speedup and reduces the communication cost by about $50\\%$50% on average compared with the existing dfl baselines.", "Pub Date": "2023-08-04"}