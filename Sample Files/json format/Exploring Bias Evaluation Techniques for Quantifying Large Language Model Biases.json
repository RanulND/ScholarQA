{"Title": "Exploring Bias Evaluation Techniques for Quantifying Large Language Model Biases", "Doi": "10.1109/IALP61005.2023.10337300", "Authors": ["j. he", "n. lin", "m. shen", "d. zhou", "a. yang"], "Key Words": ["large language models", "bias", "fairness metrics"], "Abstract": "in recent years there has been a surge in the adoption of large language models  llms  such as \u201a\u00e4\u00fachatg pt\u201a\u00e4\u00f9 trained by openai. these models have gained popularity due to their impressive performance in various real world applications. however research has shown that small pre trained language models  plms  such as bert exhibit biases particularly gender bias that mirror societal stereotypes. given the shared architecture between llms and small plms like transformer there is concern that these biases may also exist in llms. although some studies suggest the presence of biases in llms there is no consensus on how these biases should be measured. this paper employs three internal bias metrics namely seat stereoset and crows pairs to evaluate nine bias involving gender age race occupation nationality religion sexual orientation physical appearance and disability in five open source llms  llama llama2 alpaca vicuna and mpt  thereby determining their specific bias level. the experimental results demonstrate varying degrees of bias within these llms with some models displaying high levels of bias that could potentially lead to harm in specific domains. interestingly we also discover that despite their larger architectures and greater number of parameters compared to small plms like bert these llms exhibit a lower level of bias. we posit that the inclusion of fairness considerations during the pre training phase of these language model based learners  llms  is the primary contributing factor. this involves prioritizing the use of \u201a\u00e4\u00fafair\u201a\u00e4\u00f9 corpora while constructing the training data and our experimental findings confirm the effectiveness of such an approach. finally by identifying the presence and measuring the specific level of bias we contribute to the ongoing discourse on the mitigation of bias and the responsible usage of llms in various applications.", "Pub Date": "2023-12-12"}