{"Title": "Exploring Bias Evaluation Techniques for Quantifying Large Language Model Biases", "Doi": "10.1109/IALP61005.2023.10337300", "Authors": ["j. he", "n. lin", "m. shen", "d. zhou", "a. yang"], "Key Words": ["large language models", "bias", "fairness metrics"], "Abstract": "in recent years there has been a surge in the adoption of large language models  large language model  such as \u201a\u00e4\u00fachatg pt\u201a\u00e4\u00f9 trained by openai. these models have gained popularity due to their impressive performance in various real world applications. however research has shown that small pre trained language models  plms  such as bert exhibit biases particularly gender bias that mirror societal stereotypes. given the shared architecture between large language model and small plms like transformer there is concern that these biases may also exist in large language model. although some studies suggest the presence of biases in large language model there is no consensus on how these biases should be measured. this paper employs three internal bias metrics namely seat stereoset and crows pairs to evaluate nine bias involving gender age race occupation nationality religion sexual orientation physical appearance and disability in five open source large language model  llama llama2 alpaca vicuna and mpt  thereby determining their specific bias level. the experimental results demonstrate varying degrees of bias within these large language model with some models displaying high levels of bias that could potentially lead to harm in specific domains. interestingly we also discover that despite their larger architectures and greater number of parameters compared to small plms like bert these large language model exhibit a lower level of bias. we posit that the inclusion of fairness considerations during the pre training phase of these language model based learners  large language model  is the primary contributing factor. this involves prioritizing the use of \u201a\u00e4\u00fafair\u201a\u00e4\u00f9 corpora while constructing the training data and our experimental findings confirm the effectiveness of such an approach. finally by identifying the presence and measuring the specific level of bias we contribute to the ongoing discourse on the mitigation of bias and the responsible usage of large language model in various applications.", "Pub Date": "2023-12-12"}