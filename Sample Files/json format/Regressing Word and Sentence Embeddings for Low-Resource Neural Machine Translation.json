{"Title": "Regressing Word and Sentence Embeddings for Low-Resource Neural Machine Translation", "Doi": "10.1109/TAI.2022.3187680", "Authors": ["i. j. unanue", "e. z. borzeshi", "m. piccardi"], "Key Words": ["machine translation (mt)", "neural machine translation (nmt)", "regularization", "sentence embeddings", "word embeddings"], "Abstract": "in recent years neural machine translation  nmt  has achieved unprecedented performance in the automated translation of resource rich languages. however it has not yet managed to achieve a comparable performance over the many low resource languages and specialized translation domains mainly due to its tendency to overfit small training sets and consequently strive for new data. for this reason in this article we propose a novel approach to regularize the training of nmt models to improve their performance over low resource language pairs. in the proposed approach the model is trained to copredict the target training sentences both as the usual categorical outputs  i.e. sequences of words  and as word and sentence embeddings. the fact that word and sentence embeddings are pretrained over large corpora of monolingual data helps the model generalize beyond the available translation training set. extensive experiments over three low resource language pairs have shown that the proposed approach has been able to outperform strong state of the art baseline models with more marked improvements over the smaller training sets  e.g. up to $+6.57$ bleu points in basque\u201a\u00e4\u00ecenglish translation . a further experiment on unsupervised nmt has also shown that the proposed approach has been able to improve the quality of machine translation even with no parallel data at all.", "Pub Date": "2023-05-24"}