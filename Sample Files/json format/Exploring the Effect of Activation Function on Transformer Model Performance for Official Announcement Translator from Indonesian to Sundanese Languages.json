{"Title": "Exploring the Effect of Activation Function on Transformer Model Performance for Official Announcement Translator from Indonesian to Sundanese Languages", "Doi": "10.1109/ICCoSITE57641.2023.10127770", "Authors": ["b. d. wijanarko", "d. fitria murad", "y. heryadi", "c. tho", "k. hashimoto"], "Key Words": ["activation function", "transformer model", "neural machine translation", "indonesian language", "sundanese language"], "Abstract": "automated language translation involving low resource language has gained wide interest from many research communities in the past decade. one lesson learned from the past covid-19 pandemic particularly in indonesia is that many local governments have to release regular public announcements to keep people following health protocol especially when they are in public areas. many studies showed some evidence that rural people in indonesia which covers a large proportion of indonesia\u201a\u00e4\u00f4s population feel more convenience receiving official announcements in their local language. however translating official announcement from the national language to many local languages in indonesia require many experienced bilingual translators and time. this paper presents exploration results in developing an automated language translator model to translate texts in bahasa indonesia to the sundanese language. in particular this study aims to explore the effect of relu sigmoid and tanh activation functions of the vanilla transformer model on its translation performance. the experiment results showed that the activation function under study gives similar training accuracy  0.98 . however relu achieves better performance than tanh in terms of validation accuracy training loss and validation loss.", "Pub Date": "2023-05-23"}