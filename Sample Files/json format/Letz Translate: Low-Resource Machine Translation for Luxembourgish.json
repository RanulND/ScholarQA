{"Title": "Letz Translate: Low-Resource Machine Translation for Luxembourgish", "Doi": "10.1109/ICNLP58431.2023.00036", "Authors": ["y. song", "s. ezzini", "j. klein", "t. bissyande", "c. lefebvre", "a. goujon"], "Key Words": ["neural machine translation", "low-resource languages", "low-resource translation", "knowledge distillation", "luxembourgish"], "Abstract": "natural language processing of low resource languages  lrl  is often challenged by the lack of data. therefore achieving accurate machine translation  mt  in a low resource environment is a real problem that requires practical solutions. research in multilingual models have shown that some lrls can be handled with such models. however their large size and computational needs make their use in constrained environments  e.g. mobile iot devices or limited old servers  impractical. in this paper we address this problem by leveraging the power of large multilingual mt models using knowledge distillation. knowledge distillation can transfer knowledge from a large and complex teacher model to a simpler and smaller student model without losing much in performance. we also make use of high resource languages that are related or share the same linguistic root as the target lrl. for our evaluation we consider luxembourgish as the lrl that shares some roots and properties with german. we build multiple resource efficient models based on german knowledge distillation from the multilingual no language left behind  nllb  model and pseudo translation. we find that our efficient models are more than 30% faster and perform only 4% lower compared to the large state of the art nllb model.", "Pub Date": "2023-09-06"}