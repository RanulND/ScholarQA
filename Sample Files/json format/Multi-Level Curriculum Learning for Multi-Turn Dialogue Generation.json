{"Title": "Multi-Level Curriculum Learning for Multi-Turn Dialogue Generation", "Doi": "10.1109/TASLP.2023.3322583", "Authors": ["g. chen", "r. zhan", "d. f. wong", "l. s. chao"], "Key Words": ["dialogue generation", "curriculum learning", "dynamic learning", "multi-level training strategy"], "Abstract": "since deep learning is the dominant paradigm in the multi turn dialogue generation task large scale training data is the key factor affecting the model performance. to make full use of the training data the existing work directly applied curriculum learning to the multi turn dialogue generation task training model in a \u201a\u00e4\u00faeasy to hard\u201a\u00e4\u00f9 way. but the design of the current methodology does not consider dialogue specific features. to close this gap we propose a multi level curriculum learning  mlcl  method for multi turn dialogue generation by considering the word level linguistic feature and utterance level semantic relation in a dialogue. the motivation is that word level knowledge is beneficial to understanding complex utterance level dependency of dialogue. thus we design two difficulty measurements and a self adaptive curriculum scheduler making the model gradually shift the learning focus from word level to utterance level information during the training process. we also verify the independence and complementarity of the two measurements at different levels. we evaluate the performance on two widely used multi turn dialogue datasets and the results demonstrate that our proposed method outperforms the strong baselines and existing cl methods in terms of automated metrics and human evaluation. we will release the code files upon acceptance.", "Pub Date": "2023-10-23"}