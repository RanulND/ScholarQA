{"Title": "Decentralized Federated Learning With Intermediate Results in Mobile Edge Computing", "Doi": "10.1109/TMC.2022.3221212", "Authors": ["s. chen", "y. xu", "h. xu", "z. jiang", "c. qiao"], "Key Words": ["federated learning", "intermediate result exchanging", "mobile edge computing", "p2p communication"], "Abstract": "the emerging federated learning  fl  permits all workers  e.g. mobile devices  to cooperatively train a model using their local data at the network edge. in order to avoid the possible bottleneck of conventional parameter server architecture the decentralized federated learning  dfl  is developed on the peer to peer  p2p  communication. in dfl model exchanging among workers is usually regarded as an atomic operation which largely affects the total bandwidth consumption during model training. given the limited communication resource on workers model exchanging will pose a great challenge when meeting with the large scale models. herein we propose to let workers exchange the intermediate results instead of the entire model with each other. we provide theoretical analysis of dfl based on intermediate result exchanging which reveals the relationship between the training performance and the exchanging interval  i.e. the number of local updating iterations  of intermediate results. according to the convergence bound we propose an adaptive exchanging interval  or frequency  algorithm called fed ir which optimizes the trade off between communication cost and training performance. extensive simulation results show that compared with the model exchanging methods our proposed algorithms can save communication traffic of around 42%$\\sim$\u201a\u00e0\u00ba81% while still achieving the similar accuracy.", "Pub Date": "2023-12-05"}