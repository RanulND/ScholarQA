{"Title": "Performance Characterization of Large Language Models on High-Speed Interconnects", "Doi": "10.1109/HOTI59126.2023.00022", "Authors": ["h. qi", "l. dai", "w. chen", "z. jia", "x. lu"], "Key Words": ["large language models", "characterization", "transformer", "gpt", "bert", "t5"], "Abstract": "large language models  llms  have recently gained significant popularity due to their ability to generate human like text and perform a wide range of natural language processing tasks. training these models usually requires a large amount of computational resources and is often done in a distributed manner. the use of high speed interconnects can significantly influence the efficiency of distributed training. therefore there poses a need for systematic studies to explore the distributed training characteristics of these models on high speed interconnects. this paper presents a comprehensive performance characterization of representative large language models  gpt bert and t5. we evaluate their training performance in terms of iteration time interconnect utilization and scalability over different high speed interconnects and communication protocols including tcp ip ipoib and rdma. we observe that interconnects play a vital role in llm training. specifically rdma-100 gbps outperforms ipoib 100 gbps and tcp ip 10 gbps by an average of 2.51x and 4.79x regarding training iteration time and scores the highest interconnect utilization  up to 60 gbps  in both strong and weak scaling compared to ipoib with up to 20 gbps and tcp ip with up to 9 gbps leading to the shortest training time. we also observe that larger models tend to have higher requirements for communication bandwidth especially for allreduce during backward propagation which can take up to 91.12% of training time. through our evaluation we envision opportunities to improve the communication time for better training performance of llms. we extensively explore and summarize the role communication plays in distributed llm training.", "Pub Date": "2023-10-20"}