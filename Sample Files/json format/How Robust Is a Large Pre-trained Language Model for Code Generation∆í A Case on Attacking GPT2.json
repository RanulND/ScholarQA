{"Title": "How Robust Is a Large Pre-trained Language Model for Code Generation\u2206\u00ed A Case on Attacking GPT2", "Doi": "10.1109/SANER56733.2023.00076", "Authors": ["r. zhu", "c. zhang"], "Key Words": ["code generation", "adversarial attack", "large pre-trained language models", "robustness"], "Abstract": "large pre trained language models have shown strong capabilities in the field of natural language processing and in recent years research demonstrated that they can also produce surprising results from natural language descriptions in automatic code generation applications. although such models have performed well in a variety of domains there is evidence that they can be affected by adversarial attacks consequently it has become important to measure the robustness of models by producing adversarial examples. in this study we proposed an attack method called the modifier for code generation attack  m cga  which is the first time a white box adversarial attack has been applied to the field of code generation. the m cga method measures the robustness of a model by producing adversarial examples that can cause the model to produce code that is incorrect or does not meet the criteria for use. preliminary experimental results showed the m cga method to be an effective attack method providing a new research direction in automatic code synthesis.", "Pub Date": "2023-05-15"}