{"Title": "Residualtransformer: Residual Low-Rank Learning With Weight-Sharing For Transformer Layers", "Doi": "10.1109/ICASSP48485.2024.10446321", "Authors": ["y. wang", "j. li"], "Key Words": ["weight sharing", "low-rank approximation", "model compression", "transformer", "speech recognition and translation"], "Abstract": "memory constraint of always on devices is one of the major concerns when deploying speech processing models on these devices. while larger models trained with sufficiently large amount of data generally perform better making them fit in the device memory is a demanding challenge. in this paper we aim to reduce model size by reparameterizing model weights across transformer encoder layers and assuming a special weight composition and structure. more specifically inspired by resnet  and the more recent lora  work we propose an approach named residualtransformer where each weight matrix in a transformer layer comprises 1  a shared full rank component with its adjacent layers and 2  a unique low rank component to itself. the low rank matrices only account for a small amount of model size increase. in addition we add diagonal weight matrices to improve modeling capacity of the low rank matrices. experiments of our 10k hour speech recognition and speech translation tasks show that the transformer encoder size can be reduced by \u201a\u00e0\u00ba3\u221a\u00f3 with very slight performance degradation.", "Pub Date": "2024-03-18"}