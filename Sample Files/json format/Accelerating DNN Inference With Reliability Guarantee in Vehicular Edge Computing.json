{"Title": "Accelerating DNN Inference With Reliability Guarantee in Vehicular Edge Computing", "Doi": "10.1109/TNET.2023.3279512", "Authors": ["k. liu", "c. liu", "g. yan", "v. c. s. lee", "j. cao"], "Key Words": ["vehicular edge computing", "dnn inference acceleration", "reliability guarantee", "overlapped partitioning", "mobility-aware offloading"], "Abstract": "this paper explores on accelerating deep neural network  dnn  inference with reliability guarantee in vehicular edge computing  vec  by considering the synergistic impacts of vehicle mobility and vehicle to vehicle infrastructure  v2v v2i  communications. first we show the necessity of striking a balance between dnn inference acceleration and reliability in vec and give insights into the design rationale by analyzing the features of overlapped dnn partitioning and mobility aware task offloading. second we formulate the cooperative partitioning and offloading  cpo  problem by presenting a cooperative dnn partitioning and offloading scenario followed by deriving an offloading reliability model and a dnn inference delay model. the cpo is proved as np hard. third we propose two approximation algorithms i.e. submodular approximation allocation algorithm  sa3  and feed me the rest algorithm  fmtr . in particular sa3 determines the edge allocation in a centralized way which achieves 1/3 optimal approximation on maximizing the inference reliability. on this basis fmtr partitions the dnn models and offloads the tasks to the allocated edge nodes in a distributed way which achieves 1/2 optimal approximation on maximizing the inference reliability. finally we build the simulation model and give a comprehensive performance evaluation which demonstrates the superiority of the proposed solutions.", "Pub Date": "2023-12-19"}