{"Title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian", "Doi": "10.1109/TELFOR59449.2023.10372792", "Authors": ["a. cvetanovi\u0192\u00e1", "p. tadi\u0192\u00e1"], "Key Words": ["natural language processing", "question answering", "neural networks", "transformer"], "Abstract": "in this paper we focus on generating a synthetic qa dataset using an adapted translate align retrieve method. we created the largest serbian qa dataset which we name squad sr. to acknowledge the script duality in serbian we generated both cyrillic and latin versions of the dataset. we investigate the dataset quality and use it to fine tune several pre trained models. best results were obtained by fine tuning the berti\u0192\u00e1 model on latin squad sr dataset achieving 73.91% exact match and 82.97% f1 score on the benchmark xquad dataset which we translated into serbian for the purpose of evaluation. the results show that our model exceeds zero shot baselines but fails to go beyond human performance. we also note the advantage of using a monolingual pre trained model over multilingual as well as the performance increase gained by using latin over cyrillic. finally we conclude that squad sr is of sufficient quality for fine tuning a serbian qa model and can be used in absence of a manually crafted dataset.", "Pub Date": "2024-01-01"}