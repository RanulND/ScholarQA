{"Title": "A Quantization Approach for the Reduced Size of Large Language Models", "Doi": "10.1109/KST61284.2024.10499664", "Authors": ["r. k. kodali", "y. p. upreti", "l. boppana"], "Key Words": ["large language model", "transformer", "natural language processing", "quantized models"], "Abstract": "the use of large language models is widespread in a range of applications including natural language processing and multimodal tasks. however these models are computationally intensive. this work presents a novel approach that shows the ability to reduce the size of publicly available large language model including llama 2 7b gpt j and llama. this work uses a parameter efficient fine tuning  peft  library. the experiment reveals that the quantized version of large language model had a considerable reduction in memory size and significantly improved the model operational efficiency. this quantization process has the potential to bridge the gap between sophisticated language models and practical deployment scenarios providing opportunities for the use of large languaae models in resource constrained applications.", "Pub Date": "2024-04-17"}