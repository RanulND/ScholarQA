{"Title": "Research on High Performance Hybrid Model for Text Sentiment Analysis", "Doi": "10.1109/ICISE60366.2023.00096", "Authors": ["g. wang", "y. huang", "j. li", "d. shen", "z. li"], "Key Words": ["sentiment analysis", "natural language processing", "attention mechanisms", "deep learning"], "Abstract": "long short term memory  lstm  neural networks and attention mechanisms have been widely used in the field of sentiment analysis. however in the existing research sentiment analysis has always had large deviations in experimental results due to incomplete feature mining. therefore in this paper a hierarchical multi attention model is proposed which uses lstm and transformer encoder structures to analyze text information step by step. firstly convolutional feature extraction is carried out on the preprocessed text vector then the upper lower text correlation feature is captured by lstm. in addition we use the multi head attention mechanism for further analysis and finally the sentiment classification is output in which the existence of multi attention makes the model improve the model efficiency on the basis of perfect feature extraction. the proposed lstm transformer framework is evaluated on datasets such as the sentiment corpus  yelp13 and imdb. experiments show that this method has good performance.", "Pub Date": "2023-11-16"}