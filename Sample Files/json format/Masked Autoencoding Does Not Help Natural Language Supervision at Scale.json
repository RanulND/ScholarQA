{"Title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale", "Doi": "10.1109/CVPR52729.2023.02244", "Authors": ["f. weers", "v. shankar", "a. katharopoulos", "y. yang", "t. gunter"], "Key Words": ["multi-modal learning"], "Abstract": "self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. recent works such as m3ae  and slip  have suggested that these approaches can be effectively combined but most notably their results use small <20m examples  pre training datasets and don't effectively reflect the large scale regime  > 100m samples  that is commonly used for these approaches. here we investigate whether a similar approach can be effective when trained with a much larger amount of data. we find that a combination of two state of the art approaches  masked autoencoders mae  and contrastive language image pretraining clip  provides a benefit over clip when trained on a corpus of 11.3m image text pairs but little to no benefit  as evaluated on a suite of common vision tasks  over clip when trained on a large corpus of 1.4b images. our work provides some much needed clarity into the effectiveness  or lack thereof  of self supervision for large scale image text training.", "Pub Date": "2023-08-22"}