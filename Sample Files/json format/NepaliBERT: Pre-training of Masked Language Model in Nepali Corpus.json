{"Title": "NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus", "Doi": "10.1109/I-SMAC58438.2023.10290690", "Authors": ["s. pudasaini", "s. shakya", "a. tamang", "s. adhikari", "s. thapa", "s. lamichhane"], "Key Words": ["natural language processing", "bert", "nepali corpus", "word embedding", "word2vec", "doc2vec"], "Abstract": "recent improvement in the transformer model has provided us with state of the art architectures like bert roberta and gpt 3. though better results are obtained for the real world nlp problems with the use of these architectures there is still a lot to achieve in the nepali domain. nepali language which uses the devanagari script has rich semantics and grammatical structure but due to the lack of computational resources the optimum results are yet to be achieved by using the state of the art architectures . this is why there is no publicly available nlp nepali model which can be used by other researchers. this research study intends to fill this research gap by providing word embeddings for the nepali language trained on word2vec doc2vec and bert architecture which can be used as a base for creating benchmark results on different nlp tasks.", "Pub Date": "2023-10-26"}