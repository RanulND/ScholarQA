{"Title": "Measuring and Modifying Factual Knowledge in Large Language Models", "Doi": "10.1109/ICMLA58977.2023.00122", "Authors": ["p. pezeshkpour"], "Key Words": ["large language models", "factual knowledge", "knowledge instillation", "hallucination"], "Abstract": "large language models  llms  store an extensive amount of factual knowledge obtained from vast collections of text. to effectively utilize these models for downstream tasks it is crucial to have reliable methods for measuring their knowledge. however existing approaches for knowledge measurement have certain limitations and despite recent efforts they fail to provide accurate measurements and the necessary insights for modifying the knowledge within llms. in this work we employ information theory based measurements to provide a framework estimating the factual knowledge contained within large language models. more specifically we measure knowledge by analyzing the llm prediction probability distribution before and after instilling the target knowledge employing metrics such as entropy and kl divergence. introducing our metrics we first assess their accuracy in comparison to previous ranking based methods surpassing them by around 30% in a synthetic experiment. then we explore two prominent methods of knowledge instillation discovering that llms exhibit limitations in capturing new knowledge under specific circumstances for one of these methods. lastly we demonstrate the applicability of our methods in extracting unlearned and mislearned facts in llms through their application to in context learning.", "Pub Date": "2024-03-19"}