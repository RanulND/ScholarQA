{"Title": "Correction Focused Language Model Training For Speech Recognition", "Doi": "10.1109/ICASSP48485.2024.10447802", "Authors": ["y. ma", "z. liu", "o. kalinli"], "Key Words": ["language modeling", "correction focused training", "domain adaptation", "large language models", "speech recognition"], "Abstract": "language models  lms  have been commonly adopted to boost the performance of automatic speech recognition  asr  particularly in domain adaptation tasks. conventional way of lm training treats all the words in corpora equally resulting in suboptimal improvements in asr performance. in this work we introduce a novel correction focused lm training approach which aims to prioritize asr fallible words. the word level asr fallibility score representing the likelihood of asr mis recognition is defined and shaped as a prior word distribution to guide the lm training. to enable correction focused training with text only corpora large language models  large language model  are employed as fallibility score predictors and text generators through multi task fine tuning. experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. compared with conventional lms correction focused training achieves up to relatively 5.5% word error rate  wer  reduction in sufficient text scenarios. in insufficient text scenarios lm training with llmgenerated text achieves up to relatively 13% wer reduction while correction focused training further obtains up to relatively 6% wer reduction.", "Pub Date": "2024-03-18"}