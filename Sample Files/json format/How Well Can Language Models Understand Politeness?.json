{"Title": "How Well Can Language Models Understand Politeness?", "Doi": "10.1109/CAI54212.2023.00106", "Authors": ["c. li", "b. pang", "w. wang", "l. hu", "m. gordon", "d. marinova", "b. balducci", "y. shang"], "Key Words": ["bert", "chatgpt", "large language model (llm)", "politeness prediction"], "Abstract": "politeness plays a key role in social communications. previous work proposed an svm based computational method for predicting politeness using linguistic features on a corpus that contains wikipedia and stack exchange requests data. to extend this prior work we focus on evaluating the performance of state of the art language models on politeness prediction using the same dataset. two models are applied in this study. first we fine tune bert on politeness data and then use the fine tuned model for politeness prediction. second we use chatgpt to predict politeness. the results show that both fine tuned bert and chatgpt achieved better results than the state of the art results on both wikipedia and stack exchange data. fine tuned bert outperforms zero shot chatgpt but chatgpt can provide explanations for its prediction. moreover fine tuned bert outperforms human level performance by 2.28% on wikipedia corpus.", "Pub Date": "2023-08-02"}