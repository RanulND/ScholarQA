{"Title": "HAMMER: Hardware-Friendly Approximate Computing for Self-Attention With Mean-Redistribution And Linearization", "Doi": "10.1109/LCA.2022.3233832", "Authors": ["s. lee", "r. hwang", "j. park", "m. rhu"], "Key Words": ["approximate computing", "hardware accelerator", "neural network", "sparse computation", "transformers"], "Abstract": "the recent advancement of the natural language processing  nlp  models is the result of the ever increasing model size and datasets. most of these modern nlp models adopt the transformer based model architecture whose main bottleneck is exhibited in the self attention mechanism. as the computation required for self attention increases rapidly as the model size gets larger self attentions have been the main challenge for deploying nlp models. consequently there are several prior works which sought to address this bottleneck but most of them suffer from significant design overheads and additional training requirements. in this work we propose hammer hardware friendly approximate computing solution for self attentions employing mean redistribution and linearization which effectively increases the performance of self attention mechanism with low overheads. compared to previous state of the art self attention accelerators hammer improves performance by $1.2 1.6\\times$1.2 1.6\u221a\u00f3 and energy efficiency by $1.2 1.5\\times$1.2 1.5\u221a\u00f3.", "Pub Date": "2023-02-02"}