{"Title": "Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition", "Doi": "10.1109/ASRU57964.2023.10389732", "Authors": ["y. li", "y. wu", "j. li", "s. liu"], "Key Words": ["domain adaptation", "speech recognition", "large language model"], "Abstract": "the integration of language models  lms  has proven to be an effective way to address domain shifts in speech recognition. however these approaches usually require a significant amount of target domain text data for the training of lms. different from these methods in this work with only a domain specific text prompt we propose two zero shot asr domain adaptation methods using llama a 7 billionparameter large language model  llm . llm is used in two ways  1  second pass rescoring  reranking n best hypotheses of a given asr system with llama  2  deep llm fusion  incorporating llm into the decoder of an encoder decoder based asr system. experiments show that with only one domain prompt both methods can effectively reduce word error rates  wer  on out of domain tedlium 2 and spgispeech datasets. especially the deep llm fusion has the advantage of better recall of entity and out of vocabulary words.", "Pub Date": "2024-01-19"}