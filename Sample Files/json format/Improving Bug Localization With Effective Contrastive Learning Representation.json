{"Title": "Improving Bug Localization With Effective Contrastive Learning Representation", "Doi": "10.1109/ACCESS.2022.3228802", "Authors": ["z. luo", "w. wang", "c. cen"], "Key Words": ["contrastive learning representation", "bug localization", "pre-trained language model", "software maintenance"], "Abstract": "automated localization of buggy files can accelerate developers\u201a\u00e4\u00f4 efficiency of software maintenance improving the quality of software products. state of the art approaches for bug localization is based on neural networks e.g. rnn or cnn and can learn semantic feature from the given bug report. however these simple neural architectures are difficult to learn the deep contextual feature from bug reports which hurts the semantic mapping between bug reports and their corresponding buggy files. to resolve the above problem in this paper we propose a bug localization approach that combines pre trained language models and contrastive learning namely coloc. specifically coloc first is pre trained on a large scale bug report corpus in an unsupervised way to learn the deep contextual feature of each token in the bug report according to its context. afterward coloc is further pre trained by a contrastive learning objective to learn the contrastive learning representations both of bug reports and buggy files. contrastive learning can help coloc to learn the semantic differences between different bug reports and buggy files. to evaluate the effectiveness of coloc we choose five baseline approaches and compare their performance on a public dataset. the experimental results show that coloc outperforms all baseline approaches by up to 76.00% in terms of mrr achieving new results for bug localization.", "Pub Date": "2023-04-05"}