{"Title": "X$^{2}$2-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks", "Doi": "10.1109/TPAMI.2023.3339661", "Authors": ["y. zeng", "x. zhang", "h. li", "j. wang", "j. zhang", "w. zhou"], "Key Words": ["vision language foundation models", "vision language pre-training"], "Abstract": "vision language pre training aims to learn alignments between vision and language from a large amount of data. most existing methods only learn image text alignments. some others utilize pre trained object detectors to leverage vision language alignments at the object level. in this paper we propose to learn multi grained vision language alignments by a unified pre training framework that learns multi grained aligning and multi grained localization simultaneously. based on it we present x$^{2}$2 vlm an all in one model with a flexible modular architecture in which we further unify image text pre training and video text pre training in one model. x$^{2}$2 vlm is able to learn unlimited visual concepts associated with diverse text descriptions. experiment results show that x$^{2}$2 vlm performs the best on base and large scale for both image text and video text tasks making a good trade off between performance and model scale. moreover we show that the modular design of x$^{2}$2 vlm results in high transferability for it to be utilized in any language or domain. for example by simply replacing the text encoder with xlm r x$^{2}$2 vlm outperforms state of the art multilingual multi modal pre trained models without any multilingual pre training.", "Pub Date": "2024-04-03"}