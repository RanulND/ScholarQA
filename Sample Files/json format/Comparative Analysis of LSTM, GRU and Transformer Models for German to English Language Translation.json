{"Title": "Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation", "Doi": "10.1109/ASIANCON58793.2023.10270018", "Authors": ["p. ghadekar", "n. malwatkar", "n. sontakke", "n. soni"], "Key Words": ["neural machine translation", "recurrent neural network", "long short-term memory", "gated recurrent unit", "transformer"], "Abstract": "natural language processing  nlp  encompasses a broad range of techniques and methodologies for processing and understanding human language. one of the most important nlp applications that has experienced significant advancements and has gained immense importance over the years is neural machine translation. research on german to english language machine translation has remained a prominent area of research within the field of natural language processing and deep. this paper presents an in depth analysis of three significant models that are used for neural machine translation namely recurrent neural network with long short term memory recurrent neural network with gated recurrent unit and the transformer. for the implementation of each model a large data corpus of 221534 sentence pairs is used. two evaluation metrics are employed to assess the performance of models i.e. the bleu score and the rouge score. bleu-4 score of 0.386 0.402 and 0.482 is obtained for rnn+lstm rnn+gru and transformer model respectively. precision recall and f1 score of rouge score are studied which points to similar results as that learning of the bleu score. both the evaluation metrics suggest that the transformer model outperforms both variants of rnn. the study also paves the way for further investigation in this area by offering important information about how each model is implemented and the outcomes it produces.", "Pub Date": "2023-10-10"}