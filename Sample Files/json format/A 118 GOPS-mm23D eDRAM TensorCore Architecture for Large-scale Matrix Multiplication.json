{"Title": "A 118 GOPS/mm23D eDRAM TensorCore Architecture for Large-scale Matrix Multiplication", "Doi": "10.1109/HiPC58850.2023.00020", "Authors": ["m. yang", "y. wang", "j. p. kulkarni"], "Key Words": ["ml accelerator", "matrix multiplication", "monolithic 3d", "edram", "compute-in-memory", "cim"], "Abstract": "the computational demands for recent large transformer  based language models and neural radiance fields  nerf  have rapidly increased impacting applications like conversational ai and mixed reality  mr . current accelerator architectures struggle to cope with the vast computational requirements creating a gap with slowly growing hardware resources. this paper proposes repurposing memory components as high density computational units leveraging recent advancements in back end of line  beol  transistors and monolithic 3d integration techniques. an ultra high density monolithic 3d edram is presented as a reconfigurable matrix multiplication unit co designed with analog computation circuits achieving energy efficiency up to 2.41 tops w performance up to 1.71 tops on bfloat16 and compute intensity up to 118 gops mm2. a comprehensive multi cube core  architecture is also devised and optimized with bit stationary tensorcore dataflow. we evaluate the proposed architecture on state of the art machine learning models  nerf and llama 7b improving the computation density by up to 6.59x and 1.12x compared with gpu and state of the art vector processor designs respectively.", "Pub Date": "2024-04-05"}