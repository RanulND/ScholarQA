{"Title": "Efficient Text Analysis with Pre-Trained Neural Network Models", "Doi": "10.1109/SLT54892.2023.10022565", "Authors": ["j. cui", "h. lu", "w. wang", "s. kang", "l. he", "g. li", "d. yu"], "Key Words": ["text analysis", "tts frontend", "g2p", "text normalization", "punctuation", "weakly supervised learning", "phrase-based attention"], "Abstract": "this paper investigates the application of pre trained bert model in three classic text analysis tasks  chinese grapheme to phoneme g2p  text normalization tn  and sentence punctuation annotation. even though the full sized bert has prominent modeling power there are two challenges for it in real applications  the requirement for annotated training data and the considerable computational cost. in this paper we propose bert based low latency solutions. to collect sufficient training corpus for g2p we transfer knowledge from existing rule based system to bert through a large amount of unlabeled corpus. the new model could convert all characters directly from raw texts with higher accuracy. we also propose a hybrid two stage text normalization pipeline which reduces the sentence error rate by 25% compared to the rule based system. we offer both supervised and weakly supervised versions and find that the latter has only 1% accuracy drop from the former.", "Pub Date": "2023-01-27"}