{"Title": "Evaluation of Hallucination and Robustness for Large Language Models", "Doi": "10.1109/QRS-C60940.2023.00089", "Authors": ["r. hu", "j. zhong", "m. ding", "z. ma", "m. chen"], "Key Words": ["large language model", "hallucination", "robustness", "evaluation"], "Abstract": "as large language models  llms  rapidly advance rigorous testing and evaluation of these models grows increasingly crucial. to address this need we have developed three types of questions  chinese contextual english contextual and language context independent. testing in both chinese and english probes the llms' hallucination tendencies. we investigate the impact of language on hallucinations from two perspectives  the type of language used in the input prompt and the cultural context underlying the prompt content. additionally 52 multi domain single choice questions from c eval are presented in original and randomized order to assess robustness to perturbations. among the five llms the tests demonstrate gpt  4 has the strongest anti hallucination and robustness capabilities answering with greater accuracy consistency and reliability. chatglm ranks second and outperforms gpt  4 on chinese context dependent questions. emergent testing phenomena are analyzed from the user perspective. hallucinated responses are categorized and potential causal factors leading to hallucination and fragility are examined. based on these findings viable avenues for improvement are proposed.", "Pub Date": "2024-02-19"}