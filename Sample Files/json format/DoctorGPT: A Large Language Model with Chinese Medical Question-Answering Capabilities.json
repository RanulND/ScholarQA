{"Title": "DoctorGPT: A Large Language Model with Chinese Medical Question-Answering Capabilities", "Doi": "10.1109/HDIS60872.2023.10499472", "Authors": ["w. li", "l. yu", "m. wu", "j. liu", "m. hao", "y. li"], "Key Words": ["large language model", "medical question-answering system", "artificial intelligence"], "Abstract": "large language models  llms  have made incredible strides recently in understanding and reacting to user intents. however these models typically excel in english and have not been specifically trained for medical applications leading to suboptimal performance in responding to medical inquiries such as diagnostic queries and drug recommendations. in this paper we propose doctorgpt a domain specific large language model tailored for medical question answering tasks. doctorgpt leverages the open source baichuan2 as its foundational model undergoes extensive pre training on medical encyclopedic data to incorporate medical knowledge and subsequently undergoes fine tuning on a dataset consisting of two million medical instruction dialogue pairs to enhance its question answering capabilities. when compared to general purpose large models doctorgpt demonstrates significant advantages in chinese medical question answerinz  o&a  tasks.", "Pub Date": "2024-04-17"}