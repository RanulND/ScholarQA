{"Title": "Imitating Teaching: An Automated Approach Using Large Language Model", "Doi": "10.1109/CHILECON60335.2023.10418678", "Authors": ["g. o. leiva", "g. h. vigneau", "d. y. sep\u221a\u222blveda"], "Key Words": ["large language model (llm)", "lora", "qlora", "fine-tuning", "circuit theory 1"], "Abstract": "in this article we present the results of fine tuning performed on a large language model  llm  called falcon which comprises 7 billion parameters  7b  and is an open source. our primary focus lies in enabling the model to mimic university teaching enabling it to respond to questions and emulate the instructional style of a teacher in the course \u201a\u00e4\u00facircuit theory\u201a\u00e4\u00f9. to achieve this goal we built a comprehensive database using all the online classes recorded during the pandemic for the aforementioned course. leveraging the whisper model we accurately transcribed the audio content of these classes thereby establishing the requisite corpus for fine tuning the falcon 7b model. to accomplish this task efficiently we adopted effective techniques such as low range adapters  lora  and data quantization  qlora  enabling us to conduct the fine tuning process on a single gpu with a 15 gb capacity. subsequently we use gpt-4 compared the responses provided by the base model with those obtained after the fine tuning process using a set of 10 questions closely related to the course content. these comparisons enabled us to evaluate the effectiveness and precision of finetuning applied to the falcon 7b model. the results obtained so far show a good result in fine tuning the falcon 7b model to imitate university teaching and the quality of the responses about the course. this innovative approach has the potential for valuable applications within education providing an efficient tool for interactive learning and answering online queries.", "Pub Date": "2024-02-15"}