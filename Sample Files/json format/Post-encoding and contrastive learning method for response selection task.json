{"Title": "Post-encoding and contrastive learning method for response selection task", "Doi": "10.1109/ICNLP58431.2023.00050", "Authors": ["x. xue", "c. li", "z. lu", "y. zhang", "s. xiao"], "Key Words": ["retrieval-based dialogue systems", "pre-trained language model", "transformer", "contrastive learning"], "Abstract": "retrieval based dialogue systems have achieved great performance improvements after the raise of pre trained language models and transformer mechanisms. in the process of context and response selection the pre trained language model can capture the relationship between texts but current existing methods don\u201a\u00e4\u00f4t consider the order of sentences and the relationship between the context and the response. at the same time as the problem of a small number of positive samples in retrieval based dialogue systems it is difficult to train a learning model with high performance. in addition existing methods usually requires the larger computational cost after splicing the context and the response. to solve the above problems we propose a post encoding approach combining with the strategy of contrastive learning. the order of the context and the relationship between sentences in dialogues and response are reflected in the encoding process and a new loss function is designed for contrastive learning. the propose approach is validated through experiments on public datasets. the experiment results show that our model achieves better performance and effectiveness compared to existing methods.", "Pub Date": "2023-09-06"}