{"Title": "Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining", "Doi": "10.1109/WACV57701.2024.00547", "Authors": ["u. sahin", "h. li", "q. khan", "d. cremers", "v. tresp"], "Key Words": ["algorithms", "vision + language and/or other modalities", "algorithms", "image recognition and understanding", "algorithms", "machine learning architectures", "formulations", "and algorithms"], "Abstract": "contemporary large scale visual language models  vlms  exhibit strong representation capacities making them ubiquitous for enhancing image and text understanding tasks. they are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. despite this vlms often struggle with compositional reasoning tasks which require a fine grained understanding of the complex interactions of objects and their attributes. this failure can be attributed to two main factors  1  contrastive approaches have traditionally focused on mining negative examples from existing datasets. however the mined negative examples might not be difficult for the model to discriminate from the positive. an alternative to mining would be negative sample generation 2  but existing generative approaches primarily focus on generating hard negative texts associated with a given image. mining in the other direction i.e. generating negative image samples associated with a given text has been ignored. to overcome both these limitations we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities i.e. images and texts. leveraging these generative hard negative samples we significantly enhance vlms\u201a\u00e4\u00f4 performance in tasks involving multimodal compositional reasoning. our code and dataset are released at https //ugorsahin.github.io enhancing multimodal compositional reasoning of vlm.html.", "Pub Date": "2024-04-09"}