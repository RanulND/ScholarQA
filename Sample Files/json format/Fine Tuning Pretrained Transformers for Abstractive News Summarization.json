{"Title": "Fine Tuning Pretrained Transformers for Abstractive News Summarization", "Doi": "10.1109/EASCT59475.2023.10393603", "Authors": ["b. m. g", "p. k. r", "e. r", "s. j. n. r. b", "v. anvitha", "v. sulochana"], "Key Words": ["natural language processing", "t5", "gpt2", "bart", "summarization", "transformers and fine tuning"], "Abstract": "automated text summarization is now more important than ever due to the exponential rise of internet text data. it takes time to sift through large amounts of content especially when manually creating accurate summaries of big publications. automating the summarization process is crucial to increase the effectiveness of machine learning models. there are two ways to create summaries  abstractive summarization which includes analyzing the original text to create a summary and extractive summarization which chooses pertinent sentences from the original text. the t5 gpt2 and bart pre trained transformer models for abstractive news summarization will be compared in this study. for our investigation we have used the cnn dailymail dataset which includes summaries created by humans for assessing and contrasting the summaries produced by various models. in order to better understand how transformer models perform for text summarization tasks we analyze which model performs better for abstractive news summarization with fine tuning.", "Pub Date": "2024-01-22"}