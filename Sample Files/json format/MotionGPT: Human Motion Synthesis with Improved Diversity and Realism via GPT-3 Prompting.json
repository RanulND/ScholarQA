{"Title": "MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting", "Doi": "10.1109/WACV57701.2024.00499", "Authors": ["j. ribeiro-gomes", "t. cai", "z. \u221a\u00e5. milacski", "c. wu", "a. prakash", "s. takagi", "a. aubel", "d. kim", "a. bernardino", "f. de la torre"], "Key Words": ["algorithms", "generative models for image", "video", "3d", "etc.", "algorithms", "biometrics", "face", "gesture", "body pose", "applications", "arts / games / social media"], "Abstract": "there are numerous applications for human motion synthesis including animation gaming robotics or sports science. in recent years human motion generation from natural language has emerged as a promising alternative to costly and labor intensive data collection methods relying on motion capture or wearable sensors  e.g. suits . despite this generating human motion from textual descriptions remains a challenging and intricate task primarily due to the scarcity of large scale supervised datasets capable of capturing the full diversity of human activity.this study proposes a new approach called motiongpt to address the limitations of previous text based human motion generation methods by utilizing the extensive semantic information available in large language models  large language model . we first pretrain a doubly text conditional motion diffusion model on both coarse  \"high level\"  and detailed  \"low level\"  ground truth text data. then during inference we improve motion diversity and alignment with the training set by zero shot prompting gpt-3 for additional \"low level\" details. our method achieves new state of the art quantitative results in terms of fr\u221a\u00a9chet inception distance  fid  and motion diversity metrics and improves all considered metrics. furthermore it has strong qualitative performance producing natural results. code is available at https //github.com humansensinglab/motiongpt", "Pub Date": "2024-04-09"}