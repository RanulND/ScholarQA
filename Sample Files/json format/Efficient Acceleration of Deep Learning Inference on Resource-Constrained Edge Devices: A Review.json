{"Title": "Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review", "Doi": "10.1109/JPROC.2022.3226481", "Authors": ["m. m. h. shuvo", "s. k. islam", "j. cheng", "b. i. morshed"], "Key Words": ["algorithm\u201a\u00e4\u00echardware codesign", "artificial intelligence (ai)", "artificial intelligence on edge (edge-ai)", "deep learning (dl)", "model compression", "neural accelerator"], "Abstract": "successful integration of deep neural networks  dnns  or deep learning  dl  has resulted in breakthroughs in many areas. however deploying these highly accurate models for data driven learned automatic and practical machine learning  ml  solutions to end user applications remains challenging. dl algorithms are often computationally expensive power hungry and require large memory to process complex and iterative operations of millions of parameters. hence training and inference of dl models are typically performed on high performance computing  hpc  clusters in the cloud. data transmission to the cloud results in high latency round trip delay security and privacy concerns and the inability of real time decisions. thus processing on edge devices can significantly reduce cloud transmission cost. edge devices are end devices closest to the user such as mobile phones cyber\u201a\u00e4\u00ecphysical systems  cpss  wearables the internet of things  iot  embedded and autonomous systems and intelligent sensors. these devices have limited memory computing resources and power handling capability. therefore optimization techniques at both the hardware and software levels have been developed to handle the dl deployment efficiently on the edge. understanding the existing research challenges and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence  artificial intelliegence  capability. mainly four research directions have been pursued for efficient dl inference on edge devices  1  novel dl architecture and algorithm design  2  optimization of existing dl methods  3  development of algorithm\u201a\u00e4\u00echardware codesign  and 4  efficient accelerator design for dl deployment. this article focuses on surveying each of the four research directions providing a comprehensive review of the state of the art tools and techniques for efficient edge inference.", "Pub Date": "2023-01-11"}