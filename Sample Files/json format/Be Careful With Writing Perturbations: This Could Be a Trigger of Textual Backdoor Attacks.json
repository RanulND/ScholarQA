{"Title": "Be Careful With Writing Perturbations: This Could Be a Trigger of Textual Backdoor Attacks", "Doi": "10.1109/SWC57546.2023.10449255", "Authors": ["s. yang", "q. li", "p. wang", "z. lian", "j. hou", "y. rao"], "Key Words": ["textual backdoor attacks", "natural language processing (nlp) models"], "Abstract": "recent research has shown that large natural language processing  nlp  models have been vulnerable to a security threat called backdoor attacks. the current privately triggered backdoor attack achieves the attack target by modifying the label of the poisoned data to a specified label. still the result often ignores the consistency of the sample semantics and the label thus failing to achieve stealthy attacks on system users and system deployers. to address this issue we use human written text perturbations which are less likely to be detected as machine manipulation and removed by defense systems as a backdoor trigger and apply a mild adversarial perturbation to the poisoned samples before implanting the backdoor thereby addressing the semantic inconsistency caused by modifying the labels of the poisoned data. furthermore we use learnable backdoor triggers to improve the stealth of backdoor attacks while avoiding the conflict between textual adversarial perturbations and backdoor implantation. experiments show that our attack achieves close to 100% attack success rates on sst 2 olid and ag\u201a\u00e4\u00f4s news datasets without affecting the utility of existing nlp models.", "Pub Date": "2024-03-01"}