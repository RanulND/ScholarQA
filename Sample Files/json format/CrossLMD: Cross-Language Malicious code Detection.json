{"Title": "CrossLMD: Cross-Language Malicious code Detection", "Doi": "10.1109/SWC57546.2023.10449012", "Authors": ["z. xu", "x. han", "w. zuo", "x. luo", "z. wang", "x. wu"], "Key Words": ["cross-language", "malicious source code detection", "static analysis", "fine-tuning", "deep learning"], "Abstract": "current malicious code detection models usually mainly focus on variants of popular programming languages and source codes while ignoring the inherent semantic information in source codes. these approaches present significant challenges in the face of the vast amount of multi programming language code that exists in repositories open source communities blogs and forums. to address this problem we propose crosslmd a cross language malicious code detection model capable of identifying malicious code across various programming languages using a single model. crosslmd leverages large scale pre trained models to understand semantic dependencies between code written in different languages. we built a neural network classifier on top of the pre trained model to fine tune the pre trained model for cross programming language malicious code detection tasks. our experimental results demonstrate that crosslmd can effectively identify malicious code written in never encountered programming languages exhibiting superior performance compared to baseline methods. our proposed model provides a more comprehensive solution for malicious code detection taking into account the inherent semantic information of source code regardless of language or syntax changes. utilizing a single model to detect malicious code in multiple programming languages simplifies the detection process and improves efficiency and cost effectiveness.", "Pub Date": "2024-03-01"}