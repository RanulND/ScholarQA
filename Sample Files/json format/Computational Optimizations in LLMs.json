{"Title": "Computational Optimizations in LLMs", "Doi": "10.1109/ICMLANT59547.2023.10372971", "Authors": ["a. asesh", "m. dugar"], "Key Words": ["large language models (llm)", "chatbot generative pre-trained transformer (chat gpt)", "natural language processing (nlp)", "parameter-efficient transfer learning (petl)", "neural networks (nn)", "sentiment analysis", "model calibration", "fine-tuning", "side-tuning", "sequence-to-sequence rnn"], "Abstract": "over recent years the proliferation of microblogging and textual messaging platforms has led to an exponential surge in textual data necessitating advanced automated techniques for sentiment elucidation. contemporary methodologies despite their efficacy often demand substantial computational expenditure and manifest pronounced overfitting in scenarios involving non standard dataset distributions. parameter efficient transfer learning  petl  has emerged as a promising strategy to mitigate the exorbitant computational overheads associated with large scale model training albeit not without its computational burden. this research an extension of extant literature introduces a pioneering miniature orthogonal network  minion  approach. it synergistically couples a diminutive sequence to sequence recurrent neural network  rnn  with a static transformer model eschewing the need for backpropagation through the voluminous transformer. experimental results affirm that minion ensures a notable reduction in computational requisites in juxtaposition with antecedent implementations and comprehensive model fine tuning simultaneously curtailing model over assurance while preserving commendable accuracy metrics in sentiment categorization tasks.", "Pub Date": "2024-01-01"}