{"Title": "Research on Image Captioning Based on Vision-language Pre-trained Models", "Doi": "10.1109/BigDIA60676.2023.10429361", "Authors": ["s. zou", "y. xie", "j. yan", "y. wei", "x. luan"], "Key Words": ["image caption", "pre-trained model", "transformer", "attention mechanism"], "Abstract": "image captioning is a vision language task that targets at describing an image by generating a coherent sentence automatically. this technology allows computers to understand and describe images like humans enabling further processing of images. it combines computer vision and natural language processing which has broad applications. this paper investigates the use of vision language pre trained models for image captioning. in this article an encoder decoder hybrid model based on pvt and bert is pre trained on a large number of image text pairs possessing both understanding and generation capabilities. it consists of three components  1  an image encoder and a text encoder to extract image and text features respectively  2  a multi modal encoder to align the two modalities  and 3  a text decoder to generate text descriptions. this paper focuses on enhancing the image encoder of the model which is crucial for image captioning models as it extracts high resolution image features. reducing training time and computational costs can also improve the applicability of pre trained models. improvements are made to the original model by modifying the vit model architecture of the image encoding module. a feature pyramid structure and a spatial reduction attention mechanism is introduced to extract high resolution image features while reducing computational complexity and memory usage thereby enhancing the universality of the model. the model adopts a pure transformer architecture discarding the convolutional structure and achieves excellent performance.", "Pub Date": "2024-02-15"}