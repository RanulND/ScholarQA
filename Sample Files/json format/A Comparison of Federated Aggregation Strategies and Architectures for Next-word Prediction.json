{"Title": "A Comparison of Federated Aggregation Strategies and Architectures for Next-word Prediction", "Doi": "10.1109/BigData59044.2023.10386579", "Authors": ["y. sakhnovych", "r. r\u221a\u2202ttger", "r. mayer"], "Key Words": ["federated learning federated averaging language models next-word prediction"], "Abstract": "federated learning is an important technique for training language models which are frequently used for next word prediction since federated learning allows utilising large quantities of real life data without compromising the privacy of the data owners. training a model that generalises well in this setting is a challenging task due to the inherent statistical heterogeneity of the training data and due to the hardware limitations of private mobile devices. there are different approaches that address these issues e.g. through model selection different aggregation and learning strategies and update compression. in this paper two popular model architectures namely long short term memory  lstm  and gated recurrent unit  gru  are evaluated in centralised and federated settings. for federated learning the vanilla federated averaging algorithm and two alternatives that try to address statistical heterogeneity namely fedprox which uses a proximal term to restrict the divergence from the global model during local model training and federated attention which has similar aims of reducing the distance between models as well to ensure faster convergence and improve generalisation but is performing this during the aggregation station are evaluated for their achieved perplexity and accuracy in various settings on two datasets. based on these results we provide guidelines on which methods to use depending on the scenario.", "Pub Date": "2024-01-22"}