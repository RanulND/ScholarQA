{"Title": "CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose", "Doi": "10.1109/CVPR52729.2023.02229", "Authors": ["x. zhang", "w. wang", "z. chen", "y. xu", "j. zhang", "d. tao"], "Key Words": ["humans: face", "body", "pose", "gesture", "movement"], "Abstract": "animal pose estimation is challenging for existing image based methods because of limited training data and large intra  and inter species variances. motivated by the progress of visual language research we propose that pre trained language models  e.g. clip  can facilitate animal pose estimation by providing rich prior knowledge for describing animal keypoints in text. however we found that building effective connections between pre trained language models and visual animal keypoints is non trivial since the gap between text based descriptions and keypoint based visual features about animal pose can be significant. to address this issue we introduce a novel prompt based contrastive learning scheme for connecting language and animal pose  clamp  effectively. the clamp attempts to bridge the gap by adapting the text prompts to the animal keypoints during network training. the adaptation is decomposed into spatialaware and feature aware processes and two novel contrastive losses are devised correspondingly. in practice the clamp enables the first cross modal animal pose estimation paradigm. experimental results show that our method achieves state of the art performance under the supervised few shot and zero shot settings outperforming image based methods by a large margin. the code is available at https //github.com xuzhang1199/clamp.", "Pub Date": "2023-08-22"}