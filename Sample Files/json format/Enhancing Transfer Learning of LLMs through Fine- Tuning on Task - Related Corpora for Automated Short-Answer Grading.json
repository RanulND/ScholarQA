{"Title": "Enhancing Transfer Learning of LLMs through Fine- Tuning on Task - Related Corpora for Automated Short-Answer Grading", "Doi": "10.1109/ICMLA58977.2023.00255", "Authors": ["n. kazi", "i. kahanda"], "Key Words": ["automated short answer grading", "asag", "large language models", "transfer learning", "semantic inference", "recognizing textual entailment", "mnli", "semeval", "scientsbank"], "Abstract": "automated short answer grading  asag  is a cru cial element of any intelligent tutoring platform. machine learning  ml  has shown great promise for asag. however this task remains challenging even for deep learning  dl  approaches and large language models  llms  requiring semantic inference and textual entailment recognition. the semeval 2013 task 7 the joint student response analysis and 8th recognizing textual entailment challenge is a benchmark widely used for research on asag. the scientsbank data included in this collection contains nearly 11000 answers to 197 assessment questions in 15 different science domains. despite the popularity only a few researchers have explored the potential of dl or llms for this task. in this project we explore the effectiveness of the roberta large model an llm trained on an extensive text corpus for language comprehension. by fine tuning the model on the multi genre natural language inference  mnli  corpus for semantic inference and subsequently on the scientsbank dataset with a focus on the 3 way labels of correct incorrect and contradictory we achieved a weighted fl score of 0.77 0.72 and 0.72 on unseen answers questions and domains respectively. notably our model significantly benefits from fine tuning on the mnli corpus particularly in enhancing its performance on the contradictory class  which constitutes only 10% of the dataset  through transfer learning leading to significant improvements on the more challenging test sets  unseen questions and unseen domains.", "Pub Date": "2024-03-19"}