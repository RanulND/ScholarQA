{"Title": "Sequence to Sequence Pre-Trained Model for Natural Language Processing", "Doi": "10.1109/CSET58993.2023.10346822", "Authors": ["g. dhasmana", "p. k. h r", "g. p. m s"], "Key Words": ["bart", "nlp", "pre-trained model", "t5 model", "sequence - to -sequence model"], "Abstract": "pre trained model is a transfer learning model trained on huge dataset. these model can be reused to solve new problems. pre  trained model uses the encoder structure. sequence   to sequence model uses the encoder   decoder structure. deep learning model can do the better task whenever large dataset is available but it may not be sequence   to   sequence pre   trained model  sspt . sspt model takes the chunks of random text and it will try to decode the clean text. for the natural language processing both bert and bart can be used. bert is more kind of bidirectional encoder structure with an objective of mask filling language whereas bart is both encoder and decoder structure with left to right language modelling task. t5 is the text to text transformer which is aims to achieve state of the art result. mt5 is the massively multilingual text to text transformers which is the variant of t5. this paper focus more on sspt model such as bart and t5 model the result indicates that bart performance is better over the other trained model. prompting and fine tuning shall be used to improve the performance of the existing model.", "Pub Date": "2023-12-19"}