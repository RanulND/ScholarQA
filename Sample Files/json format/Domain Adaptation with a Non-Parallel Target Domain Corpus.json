{"Title": "Domain Adaptation with a Non-Parallel Target Domain Corpus", "Doi": "10.1109/ICAICTA59291.2023.10390007", "Authors": ["t. kinouchi", "a. ogawa", "y. wakabayashi", "k. ohta", "n. kitaoka"], "Key Words": ["self-supervised learning", "density ratio approach", "shallow fusion", "domain adaptation"], "Abstract": "the accuracy of conventional asr systems depends largely on the amount of speech and associated transcription data available in the target domain for model training. however preparing parallel training data each time a model is built for a new domain is costly and time consuming. to solve this problem we propose a method of domain adaptation that does not require the use of a large amount of parallel data from the target domain since most of the data used for model training is not from the target domain that uses only non parallel speech and text  data in which the two contents do not correspond to each other  which are relatively inexpensive to collect. we performed domain adaptation in two steps. 1  a pre trained wav2vec 2.0 model is further pre trained with a large amount of target domain speech data and then fine tuned with a large amount of non target domain speech and its transcriptions. 2  density ratio approach  dra  on the language model trained with target domain text during inference. in an evaluation experiment an asr model trained using the proposed method obtained a cer approximately 9 pts lower than the same asr model trained with a non target domain corpus when target domain speech was used in the test set a 33.6 % reduction in relative cer.", "Pub Date": "2024-01-16"}