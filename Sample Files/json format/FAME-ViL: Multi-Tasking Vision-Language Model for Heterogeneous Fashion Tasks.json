{"Title": "FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks", "Doi": "10.1109/CVPR52729.2023.00262", "Authors": ["x. han", "x. zhu", "l. yu", "l. zhang", "y. -z. song", "t. xiang"], "Key Words": ["vision", "language", "and reasoning"], "Abstract": "in the fashion domain there exists a variety of vision and language  v+l  tasks including cross modal retrieval text guided image retrieval multi modal classification and image captioning. they differ drastically in each individual input output format and dataset size. it has been common to design a task specific model and fine tune it independently from a pre trained v+l model  e.g. clip . this results in parameter inefficiency and inability to exploit inter task relatedness. to address such issues we propose a novel fashion focused multi task efficient learning method for vision and language tasks  fame vil  in this work. compared with existing approaches fame vil applies a single model for multiple heterogeneous fashion tasks therefore being much more parameter efficient. it is enabled by two novel components   1  a task versatile architecture with cross attention adapters and task specific adapters integrated into a unified v+l model and  2  a stable and effective multi task training strategy that supports learning from heterogeneous data and prevents negative transfer. extensive experiments on four fashion tasks show that our fame vil can save 61.5% of parameters over alternatives while significantly outperforming the conventional independently trained single task models. code is available at https //github.com brandonhanx/fame vil.", "Pub Date": "2023-08-22"}