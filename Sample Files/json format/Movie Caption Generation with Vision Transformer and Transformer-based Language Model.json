{"Title": "Movie Caption Generation with Vision Transformer and Transformer-based Language Model", "Doi": "10.1109/IIAI-AAI59060.2023.00027", "Authors": ["s. nakamura", "h. yanagimoto", "k. hashimoto"], "Key Words": ["video captioning", "vision transformer", "transformer-based language model", "deep learning"], "Abstract": "the paper presents a video caption generation system using vision transformer  vit  and a transformer based language model. a video is regarded as images with time information but it is difficult to analyze a video with ordinary image recognition techniques because of the difference between spacial information and time information. so the system analyzes each image with vit and integrates all image features into a video feature. vit is a state of the art object recognition system and consists of stacked transformers. vit is not trained with a training dataset but is employed with a pre trained model. in a caption generation module a caption is generated with transformer decoders based on the video feature. the caption generation module is trained with a training dataset. in experiments we use a large scale video captioning dataset and train the proposed system. as experiment results the proposed system achieves 0.27 f measure in rouge l and we confirm that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment. the results show the proposed system is superior to the system with vgg16.", "Pub Date": "2023-12-29"}