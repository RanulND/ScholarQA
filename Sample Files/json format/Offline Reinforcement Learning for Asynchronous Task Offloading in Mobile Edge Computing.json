{"Title": "Offline Reinforcement Learning for Asynchronous Task Offloading in Mobile Edge Computing", "Doi": "10.1109/TNSM.2023.3316626", "Authors": ["b. zhang", "f. xiao", "l. wu"], "Key Words": ["task offloading", "mobile edge computing", "offline reinforcement learning", "mobile sensing"], "Abstract": "edge servers which are located in close proximity to mobile users have become key components for providing augmented computation and bandwidth. as the resources of edge servers are limited and shared it is critical for the decentralized mobile users to determine the amount of offloaded workload to avoid competition or waste of the public resources at the edge servers. reinforcement learning  rl  methods which are sequential and model free have been widely considered as a promising approach. however directly deploying rl in edge computing remains elusive since arbitrary exploration in real online environments often leads to poor user experience. to avoid the costly interactions in this paper we propose an offline rl framework which can be optimized by using a static offline dataset only. in essence our method first trains a supervised offline model to simulate the edge computing environment dynamics and then optimize the offloading policy in the offline environment with cost free interactions. as the offloading requests are mostly asynchronous we adopt a mean field approach that treats all neighboring users as a single agent. the problem can then be simplified and reduced to a game between only two players. moreover we limit the length of the offline model rollout to ensure the simulated trajectories are accurate so that the trained offloading policies can be generalized to unseen online environments. theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. in the experiments we first train the offline simulation environment with a real historical data set and then optimize the offloading policy in this environment model. the results show that our algorithm can converge very fast during training. in the execution the algorithm still achieves high performance in the online environment.", "Pub Date": "2024-02-07"}