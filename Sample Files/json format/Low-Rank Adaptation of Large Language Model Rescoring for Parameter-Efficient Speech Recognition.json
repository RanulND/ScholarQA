{"Title": "Low-Rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition", "Doi": "10.1109/ASRU57964.2023.10389632", "Authors": ["y. yu", "c. -h. h. yang", "j. kolehmainen", "p. g. shivakumar", "y. gu", "s. r. r. ren", "q. luo", "a. gourav", "i. -f. chen", "y. -c. liu", "t. dinh", "a. g. d. filimonov", "s. ghosh", "a. stolcke", "a. rastow", "i. bulyko"], "Key Words": ["low-rank adaptation", "neural language model rescoring", "parameter-efficient speech recognition"], "Abstract": "we propose a neural language modeling system based on low rank adaptation  lora  for speech recognition output rescoring. although pretrained language models  lms  like bert have shown superior performance in second pass rescoring the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. here we present a method based on low rank decomposition to train a rescoring bert model and adapt it to new domains using only a fraction  0.08%  of the pretrained parameters. these inserted matrices are optimized through a discriminative training objective along with a correlation based regularization loss. the proposed low rank adaptation rescorebert  lorb  architecture is evaluated on librispeech and internal datasets with decreased training times by factors between 5.4 and 3.6.", "Pub Date": "2024-01-19"}