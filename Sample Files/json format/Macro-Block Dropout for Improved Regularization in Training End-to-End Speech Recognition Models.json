{"Title": "Macro-Block Dropout for Improved Regularization in Training End-to-End Speech Recognition Models", "Doi": "10.1109/SLT54892.2023.10023451", "Authors": ["c. kim", "s. indurti", "j. park", "w. sung"], "Key Words": ["neural-network", "regularization", "macro-block", "dropout", "end-to-end speech recognition"], "Abstract": "this paper proposes a new regularization algorithm referred to as macro block dropout. the overfitting issue has been a difficult problem in training large neural network models. the dropout technique has proven to be simple yet very effective for regularization by preventing complex co adaptations during training. in our work we define a macro block that contains a large number of units from the input to a recurrent neural network  rnn . rather than applying dropout to each unit we apply random dropout to each macro block. this algorithm has the effect of applying different drop out rates for each layer even if we keep a constant average dropout rate which has better regularization effects. in our experiments using recurrent neural network transducer  rnn t  this algorithm shows relatively 4.30 % and 6.13 % word error rates  wers  improvement over the conventional dropout on librispeech test clean and test other. with an attention based encoder decoder  aed  model this algorithm shows relatively 4.36 % and 5.85 % wers improvement over the conventional dropout on the same test sets.", "Pub Date": "2023-01-27"}