{"Title": "Hastening Stream Offloading of Inference via Multi-Exit DNNs in Mobile Edge Computing", "Doi": "10.1109/TMC.2022.3218724", "Authors": ["z. liu", "j. song", "c. qiu", "x. wang", "x. chen", "q. he", "h. sheng"], "Key Words": ["task offloading", "dnn inference", "model partition", "edge computing", "deep reinforcement learning"], "Abstract": "as the primary driver of intelligent mobile applications deep neural networks  dnns  have gradually deployed to millions of mobile devices producing massive latency sensitive and computation intensive tasks daily. mobile edge computing facilitates the deployment of computing resources at the edge which enables fine grained offloading of dnn inference tasks from mobile devices to edge nodes. however most existing studies have not systematically considered three crucial performance aspects  scheduling multiple streams of dnn inference tasks leveraging multi exit models to hasten task processing and partitioning inference models for partial offloading. to this end this paper proposes an adaptive inference framework in mobile edge computing which can dynamically select the exit point and partition point for multiple inference task streams. we design a dynamic programming algorithm to obtain an efficient solution under the ideal condition that task arrival information is known. further we design a learning based algorithm for online scheduling whose training efficiency is improved based on historical experience initialization and priority experience replay. experimental results show that compared with the greedy algorithm the online algorithm improves the performance on two environmental parameters by an average of 5.9% and 32% respectively.", "Pub Date": "2023-12-05"}