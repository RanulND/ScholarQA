{"Title": "Improving Fine-tuning Pre-trained Models on Small Source Code Datasets via Variational Information Bottleneck", "Doi": "10.1109/SANER56733.2023.00039", "Authors": ["j. liu", "c. sha", "x. peng"], "Key Words": ["pre-trained model", "fine-tuning", "small software engineering dataset", "information bottleneck"], "Abstract": "small datasets are common in software engineering tasks such as linguistic smell detection and code runtime complexity prediction as crafting these datasets often involves expert knowledge. prior work usually applies machine learning algorithms  e.g. logistic regression and svm  with hand crafted features to tackle them which could outperform neural models such as cnn. recently researchers have employed fine tuning large pre trained code models on various code related tasks thanks to their transferability. however it might be still instable and overfitting when fine tuning on small datasets. in this paper we firstly conduct an empirical study to fine tune codebert a  on four code related small datasets and observe the instability phenomenon. this could be induced by over capacity and irrelevant features inherent in these large pre trained code models with respective to those small datasets. to address this issue we leverage variational information bottleneck to filter out irrelevant features when fine tuning the models. the experiments demonstrate the out performance of our method compared to standard fine tuning and regularization method such as dropout and weight decay. we also experimentally study the stability of our method through varying dataset sizes. our code and data are available at https //github.com little pikachu hash vibcodebert.", "Pub Date": "2023-05-15"}