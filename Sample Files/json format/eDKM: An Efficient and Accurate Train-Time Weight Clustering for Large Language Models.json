{"Title": "eDKM: An Efficient and Accurate Train-Time Weight Clustering for Large Language Models", "Doi": "10.1109/LCA.2024.3363492", "Authors": ["m. cho", "k. a. vahid", "q. fu", "s. adya", "c. c. d. mundo", "m. rastegari", "d. naik", "p. zatloukal"], "Key Words": ["computational and artificial intelligence", "artificial intelligence", "learning systems", "machine learning", "deep learning"], "Abstract": "since large language models or llms have demonstrated high quality performance on many complex language tasks there is a great interest in bringing these llms to mobile devices for faster responses and better privacy protection. however the size of llms  i.e. billions of parameters  requires highly effective compression to fit into storage limited devices. among many compression techniques weight clustering a form of non linear quantization is one of the leading candidates for llm compression and supported by modern smartphones. yet its training overhead is prohibitively significant for llm fine tuning. especially differentiable kmeans clustering or dkm has shown the state of the art trade off between compression ratio and accuracy regression but its large memory complexity makes it nearly impossible to apply to train time llm compression. in this letter we propose a memory efficient dkm implementation edkm powered by novel techniques to reduce the memory footprint of dkm by orders of magnitudes. for a given tensor to be saved on cpu for the backward pass of dkm we compressed the tensor by applying uniquification and sharding after checking if there is no duplicated tensor previously copied to cpu. our experimental results demonstrate that edkm can fine tune and compress a pretrained llama 7b model from 12.6 gb to 2.5 gb  3 b weight  with the alpaca dataset by reducing the train time memory footprint of a decoder layer by 130\u221a\u00f3 while delivering good accuracy on broader llm benchmarks  i.e. 77.7% for piqa 66.1% for winograde and so on .", "Pub Date": "2024-03-07"}