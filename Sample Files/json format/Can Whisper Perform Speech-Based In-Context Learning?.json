{"Title": "Can Whisper Perform Speech-Based In-Context Learning?", "Doi": "10.1109/ICASSP48485.2024.10446502", "Authors": ["s. wang", "c. -h. yang", "j. wu", "c. zhang"], "Key Words": ["large pre-trained models", "in-context learning", "automatic speech recognition", "test-time adaptation", "whisper model"], "Abstract": "this paper investigates the in context learning abilities of the whisper automatic speech recognition  asr  models released by openai. a novel speech based in context learning  sicl  approach is proposed for test time adaptation which can reduce the word error rates  wers  with only a small number of labelled speech samples without gradient descent. language level adaptation experiments using chinese dialects showed that when applying sicl to isolated word asr consistent and considerable relative wer reductions can be achieved using whisper models of any size on two dialects which is on average 32.3%. a k nearest neighbours based in context example selection technique can be applied to further improve the efficiency of sicl which can increase the average relative wer reduction to 36.4%. the findings are verified using speaker adaptation or continuous speech recognition tasks and both achieved considerable relative wer reductions. detailed quantitative analyses are also provided to shed light on sicl\u201a\u00e4\u00f4s adaptability to phonological variances and dialect specific lexical nuances.", "Pub Date": "2024-03-18"}