{"Title": "Zero-Shot Intent Classification Using a Semantic Similarity Aware Contrastive Loss and Large Language Model", "Doi": "10.1109/ICASSP48485.2024.10446276", "Authors": ["j. cho", "r. s. srinivasa", "c. -h. lee", "y. m. saidutta", "c. yang", "y. shen", "h. jin"], "Key Words": ["zero-shot classification", "intent classification", "contrastive loss", "large language model", "cross-modal learning"], "Abstract": "zero shot systems can reduce the cost of collecting data and training in a new domain since they can work directly with the test data without further training. in this paper we build zero shot systems for intent classification based on semantic similarity aware contrastive loss  sscl  that addresses an issue in the original cl which treats non corresponding pairs indiscriminately. we confirm that sscl outperforms cl through experiments. then we explore how including text or speech in domain data during the sscl training affects the out of domain intent classification.during the zero shot classification embeddings for a set of classes in the new domain are generated to calculate the similarities between each class embedding and an input utterance embedding after which the most similar class is predicted for the utterance\u201a\u00e4\u00f4s intent. although manually collected text sentences per class can be used to generate the class embedding the data collection can be costly. thus we explore how to generate better class embeddings without human collected text data in the target domain. the best proposed method employing an instruction tuned llama2 a public large language model shows the performance comparable to the case where the human collected text data was used implying the importance of accurate class embedding generation.", "Pub Date": "2024-03-18"}