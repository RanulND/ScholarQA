{"Title": "BanglaGPT: A Generative Pretrained Transformer-Based Model for Bangla Language", "Doi": "10.1109/ICICT4SD59951.2023.10303383", "Authors": ["m. s. salim", "h. murad", "d. das", "f. ahmed"], "Key Words": ["bangla nlp", "banglagpt", "bangla text generation model"], "Abstract": "natural language processing  nlp  has entered a new era with the advent of pre trained language models paving the way for constructing robust language models. pretrained transformer based models such as gpt-2 have become prevalent due to their cutting edge efficiency. however these approaches rely heavily on resource intensive languages forcing other languages to adopt multilingual frameworks  mgpt . the mgpt model could perform better for low resource languages such as bangla because the model has been trained on a diverse dataset spanning multiple languages. recent studies show that the language specific gpt model outperforms the multilingual mgpt model. in this research we have proposed a pretrained monolingual gpt model called banglagpt using the objective of causal language modeling  clm . due to the lack of available large datasets for nlp tasks in bangla we have created a bangla language model dataset called banglaclm using a 26.24 gb bangla corpus scraped from several public websites. we have used a subword based tokenization algorithm named byte pair encoding  bpe  for bangla and finally trained the bangla gpt2 model from scratch using the banglaclm dataset. our pretrained banglagpt provides state of the art performance for bangla text generation with a perplexity score of 2.86 and a loss score of 0.45 on the test set.", "Pub Date": "2023-11-06"}