{"Title": "Inference-Based No-Learning Approach on Pre-Trained BERT Model Retrieval", "Doi": "10.1109/BigComp60711.2024.00044", "Authors": ["h. -l. pham", "r. mibayashi", "t. yamamoto", "m. p. kato", "y. yamamoto", "y. shoji", "h. ohshima"], "Key Words": ["bert model retrieval", "language model", "model retrieval", "text classification"], "Abstract": "in recent years the practice of leveraging pre trained machine learning models for specific tasks has gained traction. instead of training models from the ground up it is now common to fine tune existing pre trained models. however users have the responsibility to select a pre trained model that is suitable with their task a challenge given the number of pre trained models available. conventionally the suitability of a pre trained model for a task is ascertained through fine tuning which is costly in term of time and computational resources. this research introduces an efficient retrieval method for bert pre trained models in document classification tasks. our contributions are as follows   i  we defined the problem of pre trained model retrieval   ii  we developed a benchmark dataset by fine tuning and evaluating twenty distinct pre trained bert models across seventeen document classification tasks   iii  we propose a method to retrieve suitable pre trained bert models without actual fine tuning.", "Pub Date": "2024-04-11"}