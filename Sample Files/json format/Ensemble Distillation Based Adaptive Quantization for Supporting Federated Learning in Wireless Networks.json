{"Title": "Ensemble Distillation Based Adaptive Quantization for Supporting Federated Learning in Wireless Networks", "Authors": ["y. -j. liu", "g. feng", "d. niyato", "s. qin", "j. zhou", "x. li", "x. xu"], "Pub Date": "2023-06-09", "Abstract": "federated learning  fl  has become a promising technique for developing intelligent wireless networks. in traditional fl paradigms local models are usually required to be homogeneous for aggregation. however due to heterogeneous models coming with wireless system heterogeneity it is preferable for user equipments  ues  to undertake appropriate amount of computing and or data transmission work based on system constraints. meanwhile considerable communication costs are incurred by model training when a large number of ues participate in fl and or the transmitted models are large. therefore resource efficient training schemes for heterogeneous models are essential for enabling fl based intelligent wireless networks. in this paper we propose an adaptive quantization scheme based on ensemble distillation  aqed  to facilitate heterogeneous model training. we first partition and group the participating ues into clusters where the local models in specific clusters are homogeneous with different quantization levels. then we propose an augmented loss function by jointly considering ensemble distillation loss quantization levels and wireless resources constraints. in aqed model aggregations are performed at two levels  model aggregation for individual clusters and distillation loss aggregation for cluster ensembles. numerical results show that the aqed scheme can significantly reduce communication costs and training time in comparison with some state of the art solutions.", "Doi": "10.1109/TWC.2022.3222717", "Key Words": ["federated learning", "wireless network", "adaptive quantization", "ensemble distillation", "heterogeneous models"]}