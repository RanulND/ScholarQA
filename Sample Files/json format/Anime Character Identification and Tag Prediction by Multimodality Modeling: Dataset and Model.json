{"Title": "Anime Character Identification and Tag Prediction by Multimodality Modeling: Dataset and Model", "Doi": "10.1109/IJCNN54540.2023.10191980", "Authors": ["f. yi", "j. wu", "m. zhao", "s. zhou"], "Key Words": ["anime character identification", "multimodal network", "dataset", "tag prediction", "curriculum learning"], "Abstract": "in recent years some advances have been achieved in classification and object detection related to animation. however these works do not take full advantage of the tags and text description content attached to the anime data when they are created which restricts both the related methods and data to unimodality consequently leading to unsatisfactory performance. in this paper we propose a novel multimodal deep learning network for anime character identification and tag prediction by exploiting multimodal data. considering that in many realistic scenarios text annotations accompanying anime may be missing we introduce the concept of curriculum learning in transformers to enable inference with only one modality. another challenge lies in that the existing dataset does not meet our demand for large scale multimodal deep learning. to train the proposed network we construct a new anime dataset dan  mul that contains over 1.6m images spread across more than 14k categories with an average of 24 tags per image. to the best of our knowledge this is the first dataset specifically designed for multimodal anime character identification. with the trained network we can identify the anime characters in images and generate the related tags. experiments show that our method achieves state of the art performance on dan  mul in animation identification.", "Pub Date": "2023-08-02"}