{"Title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models", "Doi": "10.1109/CVPR52729.2023.01046", "Authors": ["j. guo", "j. li", "d. li", "a. m. huat tiong", "b. li", "d. tao", "s. hoi"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "large language models  llms  have demonstrated excellent zero shot generalization to new language tasks. however effective utilization of llms for zero shot visual question answering  vqa  remains challenging primarily due to the modality disconnect and task disconnect between the llm and vqa tasks. end to end training on multimodal data may bridge the disconnects but is inflexible and computationally expensive. to address this issue we propose img2llm a plug and play module that provides llm prompts to enable llms to perform zeroshot vqa tasks without end to end training. we develop llm agnostic models describe image content as exemplar question answer pairs which prove to be effective llm prompts. img2llm offers the following benefits  1  it achieves comparable or better performance than methods relying on end to end training. for example we outperform flamingo  by 5.6% on vqav2. on the challenging a okvqa dataset our method outperforms few shot methods by as much as 20%. 2  it flexibly interfaces with a wide range of llms to perform vqa. 3  it eliminates the need to specialize llms using end to end finetuning and serve highly specialized llms to end users thereby reducing cost. code is available via the lavis  framework at https //github.com salesforce/lavis tree/main projects/img2llm vqa.", "Pub Date": "2023-08-22"}