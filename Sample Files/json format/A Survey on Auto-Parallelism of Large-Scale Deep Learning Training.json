{"Title": "A Survey on Auto-Parallelism of Large-Scale Deep Learning Training", "Doi": "10.1109/TPDS.2023.3281931", "Authors": ["p. liang", "y. tang", "x. zhang", "y. bai", "t. su", "z. lai", "l. qiao", "d. li"], "Key Words": ["auto-parallelism", "large-scale deep learning model", "training technique", "parallel and distributed training"], "Abstract": "deep learning  dl  has gained great success in recent years leading to state of the art performance in research community and industrial fields like computer vision and natural language processing. one of the reasons for this success is the huge amount parameters adopted in dl models. however it is impractical to train a moderately large model with a large number of parameters on a typical single device. thus it is necessary to train dl models in clusters with distributed training algorithms. however traditional distributed training algorithms are usually sub optimal and highly customized which owns the drawbacks to train large scale dl models in varying computing clusters. to handle the above problem researchers propose auto parallelism which is promising to train large scale dl models efficiently and practically in various computing clusters. in this survey we perform a broad and thorough investigation on challenges basis and strategy searching methods of auto parallelism in dl training. first we abstract basic parallelism schemes with their communication cost and memory consumption in dl training. further we analyze and compare a series of current auto parallelism works and investigate strategies and searching methods which are commonly used in practice. at last we discuss several trends in auto parallelism which are promising in further research.", "Pub Date": "2023-07-03"}