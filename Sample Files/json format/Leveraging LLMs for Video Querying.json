{"Title": "Leveraging LLMs for Video Querying", "Doi": "10.1109/CERA59325.2023.10455658", "Authors": ["a. g. jangam", "a. p. mohite", "d. u. nayak", "a. v. nimkar"], "Key Words": ["language models", "video caption querying", "bimodal transformer", "vid2seq", "ucf crime dataset", "fine-tuning", "prompt comparison"], "Abstract": "in the pursuit of enhancing video search capabilities this paper delves into the application of large language models  large language model . the focal point is the utilization of the bi modal transformer  bmt  model and the vid2seq model for video captioning employing the ucf crime dataset for experimentation. by employing various large language model such as gpt3.5 gpt4 and llama the objective is to create an efficient search system that can pinpoint specific moments within videos. the paper outlines a methodology involving the generation of text based captions using the bmt and vid2seq model. furthermore it conducts experiments using different large language model and prompts with evaluation metrics defined to assess system performance and event occurrence accuracy. the findings acquired provide insights into the capabilities and constraints of large language model and prompts illustrating their potential to enhance video captioning and transform video search functionalities. this research makes a valuable contribution by exploring large language model based video querying techniques and their potential to improve video search efficiency.", "Pub Date": "2024-03-05"}