{"Title": "Aspect-Level Sentiment Analysis Based on Disentangled Attention and Aspect Proximity Weights", "Doi": "10.23919/CCC58697.2023.10240529", "Authors": ["j. chen", "z. wang"], "Key Words": ["absa", "deberta", "disentangled attention", "aspect proximity weights"], "Abstract": "aspect based sentiment analysis  absa  aims to analyze the sentiment polarity of different entities and it is a fine grained sentiment classification task. the advent of pre trained language models has significantly improved the performance of many natural languages processing tasks and bert based pre trained language models have been successfully applied to tasks that require a deep understanding of language such as sentiment classification. however the previous approaches rely on the semantic relevance of a word with a large range of its context. this may lead to an undesirable result. one aspect will go to focus on another aspect context and confuse the judgment of the model. although the best absa models have achieved significant performance they still have problems with robustness. considering this we choose deberta  decoding enhanced bert with disentangled attention  which uses disentangled attention in the calculation of attention score and pays more attention to the relationship between text content and position. besides we propose aspect proximity weights  apw  which are designed for the relative distance between aspects and their context. the aspect proximity weight is a kind of weight designed for the relative distance between aspects and their context. the experimental results on the semeval2014 dataset and the arts dataset show that the deberta combining aspect proximity weights has better performance and robustness compared with other baseline models.", "Pub Date": "2023-09-18"}