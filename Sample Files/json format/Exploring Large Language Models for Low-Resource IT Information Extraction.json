{"Title": "Exploring Large Language Models for Low-Resource IT Information Extraction", "Doi": "10.1109/ICDMW60847.2023.00157", "Authors": ["b. bhavya", "p. t. isaza", "y. deng", "m. nidd", "a. p. azad", "l. shwartz", "c. zhai"], "Key Words": ["llm", "information extraction", "it domain"], "Abstract": "information extraction  ie  in it is an important foundational task that is needed for many aiops applications. a major challenge of ie in it is that we often do not have sufficient labelled data for training machine learning algorithms since acquiring labels is labor intensive and costly. in this paper we propose to leverage large language models  llms  to address this challenge of low resources and study two data augmentation strategies i.e. using llms to generate pseudo labels and generate synthetic data. we use multiple ie tasks and datasets including a new semantic troubleshooting segment extraction task and named entity recognition to evaluate the benefits of llms. our experiment results suggest that data augmentation using llms specifically using seqmix model that combines active labeling with synthetic data samples generated in the embedding vector space is a promising approach for it domain ie. our study also shows that although data augmentation and direct labeling with the state of the art chatgpt model achieves a high performance on general domain ie there is a need to adapt it for ie from it text data. moreover our initial exploration of two label weighting and selection strategies  confidence and consistency based  suggests that they could be used to improve data augmentation with chatgpt for it domain ie. finally we also suggest directions for future research on the new stse task including developing better evaluation metrics. 1", "Pub Date": "2024-02-06"}