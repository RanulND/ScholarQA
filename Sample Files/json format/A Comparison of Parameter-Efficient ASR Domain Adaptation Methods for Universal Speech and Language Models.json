{"Title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "Doi": "10.1109/ICASSP48485.2024.10445894", "Authors": ["k. c. sim", "z. huo", "t. munkhdalai", "n. siddhartha", "a. stooke", "z. meng", "b. li", "t. sainath"], "Key Words": ["parameter-efficient adaptation", "foundation model", "universal speech model"], "Abstract": "a recent paradigm shift in artificial intelligence has seen the rise of foundation models such as the large language models and the universal speech models. with billions of model parameters and trained with a wide range of data these foundation models are expected to have a better generalization to different downstream tasks. efficient adaptation is the key to leveraging these foundation models in a new task or domain. in this paper we compare several popular parameter efficient tuning methods such as vector adaptation residual adapters low rank adapter  lora  and prompt tuning for automatic speech recognition  asr  domain adaptation. we use the connectionist temporal classification  ctc  model with conformer encoder and fused it with a universal language model. we study the effect of adapting either or both of the conformer encoder and the universal language model. we carry out extensive experiments to study these methods under different hyper parameter settings and the effect of combining some of these methods. we find that combining vector adaptation and residual adapters with increasing bottleneck dimension achieved the best performance.", "Pub Date": "2024-03-18"}