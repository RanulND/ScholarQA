{"Title": "Trainable Weighted Pooling Method for Text Classification with BERT", "Doi": "10.1109/IIAI-AAI-Winter61682.2023.00056", "Authors": ["h. yamato", "m. okada", "n. mori"], "Key Words": ["text classification", "pooling", "bert", "natural language processing"], "Abstract": "text classification is one of the central challenges in natural language processing encompassing techniques for cate gorizing large amounts of text data into meaningful categories. this field plays an important role in many applications such as information retrieval sentiment analysis and recommendation systems. in recent years the remarkable development of deep learning technology has led to the proposal of large language models which have achieved high performance in various tasks. bert is one of the large language models widely recognized for its potential in text classification. although bert can effectively learn context dependent word representations an appropriate pooling strategy is necessary to obtain a representation of the entire document. in this study we propose a pooling method called cls average pooling  cap  that combines the commonly used the [cls] embedding and the average pooling method in bert for text classification. we obtain the sentence representations by taking the weighted sum of the embeddings obtained from the [cls] embedding and the average pooling. at this time we treat the weights used in cap as trainable parameters to automatically acquire appropriate weights for text classification. we demonstrated that the proposed method is more effective than conventional pooling methods in text classification tasks by applying it to a dataset for text classification.", "Pub Date": "2024-04-09"}