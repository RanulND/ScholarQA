{"Title": "Analyzing the Scaling Characteristics of Transformer Feed-Forward Networks for the Trillion-Parameter Era and Beyond", "Doi": "10.1109/ICEIC61013.2024.10457210", "Authors": ["t. kim", "h. -j. lee"], "Key Words": ["transformers", "large language model (llm)", "feed-forward network", "scaling"], "Abstract": "many transformers based tasks such as language translation question answering and code generation have shown increasingly high task quality over the recent years. this has been made possible by unprecedented model size i.e. the number of parameters that consisting each dnn model. until recently much research has been centered on optimizing the attention module whose cost increases quadratically to sequence length and the cost of the feed forward network has been relatively less explored. to this end this paper analyzes the cost profile of large transformer models with regard to embedding size and suggests optimization directions.", "Pub Date": "2024-03-19"}