{"Title": "Building an Inference Server Platform for Large Language Models Using Dataflow PIM Platform", "Doi": "10.1109/ICEIC61013.2024.10457213", "Authors": ["k. h. choi", "t. hwang"], "Key Words": ["pim", "large language model", "inference server", "datacenter", "ai"], "Abstract": "processing in memory  pim  has garnered attention as a platform for large language model inference due to its ability to perform computations within memory leveraging the internal bandwidth of memory components. in data center environments to execute artificial intelliegence models across multiple nodes an inference server is typically deployed at the data center frontend. this server orchestrates the assignment of artificial intelliegence inference tasks to the appropriate nodes. this paper presents the construction of an open source based inference server designed for easy deployment of a pim platform grounded in data flow architecture within a data center setting. we have conducted operational tests on large language models to validate the efficacy of our approach.", "Pub Date": "2024-03-19"}