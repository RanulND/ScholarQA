{"Title": "MediBioDeBERTa: Biomedical Language Model With Continuous Learning and Intermediate Fine-Tuning", "Doi": "10.1109/ACCESS.2023.3341612", "Authors": ["e. kim", "y. jeong", "m. -s. choi"], "Key Words": ["language model", "fine-tuning", "domain-specific modeling", "natural language processing"], "Abstract": "the emergence of large language models  large language model  has marked a significant milestone in the evolution of natural language processing. with the expanded use of large language model in multiple fields the development of domain specific pre trained language models  plms  has become a natural progression and requirement. developing domain specific plms requires careful design considering not only differences in training methods but also various factors such as the type of training data and hyperparameters. this paper proposes medibiodeberta a specialized language model  lm  for biomedical applications. first we present several practical analyses and methods for improving the performance of lms in specialized domains. as the initial step we developed scideberta v2 an lm specialized in the scientific domain. in the scierc dataset evaluation scideberta v2 achieves the state of the art model performance in the named entity recognition  ner  task. we then provide an in depth analysis of the datasets and training methods used in the biomedical field. based on these analyses medibiodeberta was continually trained on scideberta v2 to specialize in the biomedical domain. utilizing the biomedical language understanding and reasoning benchmark  blurb  we analyzed factors that degrade task performance and proposed additional improvement methods based on intermediate fine tuning. the results demonstrate improved performance in three categories  named entity recognition  ner  semantic similarity  ss  and question answering  qna  as well as in the chemprot relation extraction  re  task on blurb compared with existing state of the art lms.", "Pub Date": "2023-12-19"}