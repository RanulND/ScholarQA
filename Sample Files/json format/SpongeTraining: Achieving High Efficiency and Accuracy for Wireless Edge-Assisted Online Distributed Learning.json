{"Title": "SpongeTraining: Achieving High Efficiency and Accuracy for Wireless Edge-Assisted Online Distributed Learning", "Doi": "10.1109/TMC.2022.3154644", "Authors": ["z. guo", "j. wang", "s. liu", "j. ren", "y. xu", "y. wang"], "Key Words": ["edge computing", "federated learning", "learning rate", "batch size"], "Abstract": "edge assisted distributed learning  edl  is a popular machine learning paradigm that uses a set of distributed edge nodes to collaboratively train a machine learning model using training data. most of existing works implicitly assume that the fixed amount of training data is pre collected and dispatched from user devices to edge nodes. in real world however training data in edge nodes are collected from user devices through wireless networks and the volume and distribution of training data in edge nodes could exhibit temporal and spatial fluctuations due to varying wireless situations  e.g. network congestion link capacity variation . in this way existing solutions suffer from slow convergence and low accuracy. in this paper we propose spongetraining to achieve high efficiency and accuracy for online edl. to accommodate to fluctuations in training data spongetraining uses a buffer at each worker to store received training data and adaptively adjusts training batch size and learning rate of each worker based on training data extracted from the buffer. experiment results based on real world datasets show that spongetraining outperforms existing solutions by accelerating the training process up to 50% for reaching the same training accuracy.", "Pub Date": "2023-06-30"}