{"Title": "Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models", "Doi": "10.1109/WACV57701.2024.00538", "Authors": ["d. luo", "j. huang", "s. gong", "h. jin", "y. liu"], "Key Words": ["algorithms", "vision + language and/or other modalities", "algorithms", "video recognition and understanding"], "Abstract": "accurate video moment retrieval  vmr  requires universal visual textual correlations that can handle unknown vocabulary and unseen scenes. however the learned correlations are likely either biased when derived from a limited amount of moment text data which is hard to scale up because of the prohibitive annotation cost  fully supervised  or unreliable when only the video text pairwise relationships are available without fine grained temporal annotations  weakly supervised . recently the vision language models  vlm  demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual textual correlations derived from large scale vision language pairwise web data which has also shown benefits to vmr by fine tuning in the target domains.in this work we propose a zero shot method for adapting generalisable visual textual priors from arbitrary vlm to facilitate moment text alignment without the need for accessing the vmr data. to this end we devise a conditional feature refinement module to generate boundary aware visual features conditioned on text queries to enable better moment boundary understanding. additionally we design a bottom up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex query retrieval tasks into individual action retrievals thereby maximizing the benefits of vlm. extensive experiments conducted on three vmr benchmark datasets demonstrate the notable performance advantages of our zero shot algorithm especially in the novel word and novel location out of distribution setups.", "Pub Date": "2024-04-09"}