{"Title": "Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator", "Doi": "10.1109/TASLP.2022.3224286", "Authors": ["g. sun", "c. zhang", "p. c. woodland"], "Key Words": ["contextual speech recognition", "end-to-end", "language model discounting", "minimum bayes' risk", "pointer generator"], "Abstract": "contextual knowledge is essential for reducing speech recognition errors on high valued long tail words. this paper proposes a novel tree constrained pointer generator  tcpgen  component that enables end to end asr models to bias towards a list of long tail words obtained using external contextual information. with only a small overhead in memory use and computation cost tcpgen can structure thousands of biasing words efficiently into a symbolic prefix tree and creates a neural shortcut between the tree and the final asr output to facilitate the recognition of the biasing words. to enhance tcpgen we further propose a novel minimum biasing word error  mbwe  loss that directly optimises biasing word errors during training along with a biasing word driven language model discounting  blmd  method during the test. all contextual asr systems were evaluated on the public librispeech audiobook corpus and the data from the dialogue state tracking challenges  dstc  with the biasing lists extracted from the dialogue system ontology. consistent word error rate  wer  reductions were achieved with tcpgen which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. mbwe and blmd further improved the effectiveness of tcpgen and achieved more significant wer reductions on the biasing words. tcpgen also achieved zero shot learning of words not in the audio training set with large wer reductions on the out of vocabulary words in the biasing list.", "Pub Date": "2022-12-06"}