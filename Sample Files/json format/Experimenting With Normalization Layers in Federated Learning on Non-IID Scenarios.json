{"Title": "Experimenting With Normalization Layers in Federated Learning on Non-IID Scenarios", "Authors": ["b. casella", "r. esposito", "a. sciarappa", "c. cavazzoni", "m. aldinucci"], "Pub Date": "2024-04-05", "Abstract": "training deep learning  dl  models require large high quality datasets often assembled with data from different institutions. federated learning  fl  has been emerging as a method for privacy preserving pooling of datasets employing collaborative training from different institutions by iteratively globally aggregating locally trained models. one critical performance challenge of fl is operating on datasets not independently and identically distributed  non iid  among the federation participants. even though this fragility cannot be eliminated it can be debunked by a suitable optimization of two hyper parameters  layer normalization methods and collaboration frequency selection. in this work we benchmark five different normalization layers for training neural networks  nns  two families of non iid data skew and two datasets. results show that batch normalization widely employed for centralized dl is not the best choice for fl whereas group and layer normalization consistently outperform batch normalization with a performance gain of up to about 15 % in the most challenging non iid scenario. similarly frequent model aggregation decreases convergence speed and mode quality.", "Doi": "10.1109/ACCESS.2024.3383783", "Key Words": ["batch normalization", "epochs per round", "federated averaging", "federated learning", "neural networks", "non-iid data", "normalization layers"]}