{"Title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning", "Doi": "10.1109/CVPR52729.2023.01044", "Authors": ["y. yu", "j. chung", "h. yun", "j. hessel", "j. s. park", "x. lu", "r. zellers", "p. ammanabrolu", "r. l. bras", "g. kim", "y. choi"], "Key Words": ["vision", "language", "and reasoning"], "Abstract": "language models are capable of commonsense reasoning  while domain specific models can learn from explicit knowledge  e.g. commonsense graphs  ethical norms   and larger models like gpt-3  mani fest broad commonsense reasoning capacity. can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? in this work we propose \u201a\u00e4\u00b0esper  extending sensory perception with reinforcement learning  which enables text only pretrained models to address multimodal tasks such as visual commonsense reasoning. our key novelty is to use rein forcement learning to align multimodal inputs to language model generations without direct supervision  for example our reward optimization relies only on cosine similarity derived from clip  and requires no additional paired  image text  data. experiments demonstrate that esper outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning  these include a new benchmark we collect and release the esp dataset which tasks models with generating the text of several different domains for each image. our code and data are publicly released at https //github.com jiwanchung/esper.", "Pub Date": "2023-08-22"}