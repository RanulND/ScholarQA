{"Title": "Nuclear-Norm Maximization for Low-Rank Updates", "Doi": "10.1109/ICASSP48485.2024.10448410", "Authors": ["h. liu", "y. zhai", "k. xu", "d. feng", "y. li"], "Key Words": ["language understanding", "low-rank adaptation", "subspace", "nuclear-norm"], "Abstract": "pre trained large language models exhibit significant potential in speech and language processing. fine tuning all parameters becomes impractical when confronted with numerous downstream tasks. to address this challenge various low rank adaptation techniques have been introduced for parameter efficient fine tuning which freeze the over parametrized models and learn incremental parameter updates within smaller subspaces. however our observation reveals that most directions of the learned subspace play a minor role in the incremental updates. consequently fine tuned models may not achieve optimal performance. to bridge this gap we introduce nnm lora which strives to harness more meaningful singular directions. through nuclear norm maximization  nnm  we can better regulate the allocation of singular values. accordingly we propose a parameter free plug and play regularizer for low rank updates. this innovative approach allows us to utilize as many singular directions of the subspace as possible during the training of low rank updates. to validate the effectiveness of nnm lora we conduct extensive experiments involving different pre trained models on various natural language understanding tasks. results demonstrate that nnm lora exhibits significant improvements compared to baseline methods.", "Pub Date": "2024-03-18"}