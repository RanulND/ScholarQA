{"Title": "SMT Solver Validation Empowered by Large Pre-Trained Language Models", "Doi": "10.1109/ASE56229.2023.00180", "Authors": ["m. sun", "y. yang", "y. wang", "m. wen", "h. jia", "y. zhou"], "Key Words": ["smt solver", "fuzzing", "large language model", "retrain-finetune", "data augmentation"], "Abstract": "smt solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains including software verification test case generation and program synthesis. however bugs hidden in smt solvers can lead to severe consequences causing erroneous results in these domains. therefore ensuring the reliability and robustness of smt solvers is of critical importance. despite several testing approaches proposed for smt solvers generating effective test formulas to comprehensively test smt solvers remains a challenge. to address this challenge in this study we propose to port large language models  large language model  to generate smt formulas for fuzzing solvers. specifically the study presents a novel retrain finetune pipeline to unleash the potential of language models to generate effective smt formulas and improve their generation performance through data augmentation. we implemented our approach as a practical fuzzing tool named lastand then extensively tested the state of the art smt solvers namely z3 cvc5 and bitwuzla. to date last has successfully uncovered 65 genuine bugs for the solvers of which 45 have been fixed by the developers.", "Pub Date": "2023-11-08"}