{"Title": "Self Evaluation Using Zero-shot Learning", "Doi": "10.1109/ICRCV59470.2023.10329149", "Authors": ["a. banwasi", "x. sun", "r. ravindranath", "m. vazquez"], "Key Words": ["large language models", "visual language models", "language grounding for vision and video understanding", "robotics"], "Abstract": "with recent advances in large language models  large language model  and visual language models  vlm  there has been great interest in extending these models to robotics and task execution. recent studies have bridged the gap between large language models and robotic language processing by providing a real world grounding to help the robot better understand and execute tasks. a major drawback of existing approaches is that they still require a human to monitor and evaluate the robot\u201a\u00e4\u00f4s performance. as robots expand into a wide range of industries from agriculture to healthcare it is important that they can self monitor their performance and identify when any mistake occurs. we propose a model that combines large language models and visual language models to self evaluate task execution based on video data. we utilize large language model to generate outcome descriptions for each step and then provide these as input prompts to a vlm to determine whether a step has been completed. based on video data our solution identifies if each step in a task has been executed correctly and notices when a mistake has been made. we design a score metric that assesses the status of completion of each step in the task. upon testing the model on a variety of household related tasks it demonstrated successful evaluation of task completion. our model could be implemented in robots to self evaluate task execution leading to more accurate and efficient robotic systems.", "Pub Date": "2023-11-30"}