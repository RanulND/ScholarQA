{"Title": "Revalidating the Encoder-Decoder Depths and Activation Function to Find Optimum Vanilla Transformer Model", "Doi": "10.1109/ICCoSITE57641.2023.10127790", "Authors": ["y. heryadi", "b. d. wijanarko", "d. fitria murad", "c. tho", "k. hashimoto"], "Key Words": ["transformer model", "neural machine translation", "sundanese language"], "Abstract": "the transformer model has become a state of the art model in natural language processing. the initial transformer model known as the vanilla transformer model is designed to improve some prominent models in sequence modeling and transduction problems such as language modeling and machine translation. the initial transformer model has 6 stacks of identical encoder decoder layers with an attention mechanism whose aim is to push limitations of common recurrent language models and encoder decoder architectures. its outstanding performance has inspired many researchers to extend the architecture to improve its performance and computation efficiency. despite many extensions to the vanilla transformer there is no clear explanation of the encoder decoder set out depth in the vanilla transformer model. this paper presents exploration results on the effect of combination encoder decoder layer depth and activation function in the feed forward layer of the vanilla transformer model on its performance. the model is tested to address a downstream task  text translation from bahasa indonesia to the sundanese language. although the value difference is not significantly large the empirical results show that the combination of depth = 2 with sigmoid tanh and relu activation function  and d = 6 with relu activation shows the highest average training accuracy. interestingly d = 6 and relu show the lowest average training and validation loss. however statistically there is no significant difference between depth and activation functions.", "Pub Date": "2023-05-23"}