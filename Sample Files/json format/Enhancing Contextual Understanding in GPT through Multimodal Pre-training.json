{"Title": "Enhancing Contextual Understanding in GPT through Multimodal Pre-training", "Doi": "10.1109/IC-RVITM60032.2023.10435352", "Authors": ["s. kumar", "simran", "n. t. singh"], "Key Words": ["generative pre-trained transformers (gpt)", "pre-training model", "graph neural networks", "natural language processing (nlp)", "ulip"], "Abstract": "natural language processing  nlp  tasks that generative pre trained transformers  gpt  have proven to be extremely effective at include text production machine translation and question answering. these models mostly rely on textual data despite the fact that there is a lot of information available in other modalities such as images audio and video. in order to enhance the contextual comprehension of gpt models this study will look into the benefits of multimodal pretraining. these massive models simply receive training on simple texts without any linguistic or global comprehension regardless of their achievement. furthermore the bulk of large scale models were trained using auto regression. because of this this traditional way of fine tuning performs terribly on future language comprehension exams.", "Pub Date": "2024-02-21"}