{"Title": "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles", "Doi": "10.1109/CVPR52729.2023.00259", "Authors": ["s. ye", "y. xie", "d. chen", "y. xu", "l. yuan", "c. zhu", "j. liao"], "Key Words": ["multi-modal learning"], "Abstract": "this paper focuses on analyzing and improving the commonsense ability of recent popular vision language  vl  models. despite the great success we observe that existing vl models still lack commonsense knowledge reasoning ability  e.g. \u201a\u00e4\u00falemons are sour\u201a\u00e4\u00f9  which is a vital component towards artificial general intelligence. through our analysis we find one important reason is that existing large scale vl datasets do not contain much commonsense knowledge which motivates us to improve the commonsense of vl models from the data perspective. rather than collecting a new vl training dataset we propose a more scalable strategy i.e. \u201a\u00e4\u00fadata augmentation with knowledge graph linearization for commonsense capability\u201a\u00e4\u00f9  dance . it can be viewed as one type of data augmentation technique which can inject commonsense knowledge into existing vl datasets on the fly during training. more specifically we leverage the commonsense knowledge graph  e.g. conceptnet  and create variants of text description in vl datasets via bidirectional sub graph sequentialization. for better commonsense evaluation we further propose the first retrieval based commonsense diagnostic benchmark. by conducting extensive experiments on some representative vl models we demonstrate that our dance technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks. the code and data are available at https //github.com pleaseconnectwifi/dance.", "Pub Date": "2023-08-22"}