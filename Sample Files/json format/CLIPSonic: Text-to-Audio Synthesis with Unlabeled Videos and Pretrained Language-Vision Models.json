{"Title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models", "Doi": "10.1109/WASPAA58266.2023.10248160", "Authors": ["h. -w. dong", "x. liu", "j. pons", "g. bhattacharya", "s. pascual", "j. serr\u221a\u2020", "t. berg-kirkpatrick", "j. mcauley"], "Key Words": ["sound synthesis", "audio generation", "multimodal learning", "diffusion models", "neural networks", "machine learning"], "Abstract": "recent work has studied text to audio synthesis using large amounts of paired text audio data. however audio recordings with high quality text annotations can be difficult to acquire. in this work we approach text to audio synthesis using unlabeled videos and pre trained language vision models. we propose to learn the desired text audio correspondence by leveraging the visual modality as a bridge. we train a conditional diffusion model to generate the audio track of a video given a video frame encoded by a pretrained contrastive language image pretraining  clip  model. at test time we first explore performing a zero shot modality transfer and condition the diffusion model with a clip encoded text query. however we observe a noticeable performance drop with respect to image queries. to close this gap we further adopt a pretrained diffusion prior model to generate a clip image embedding given a clip text embedding. our results show the effectiveness of the proposed method and that the pretrained diffusion prior can reduce the modality transfer gap. while we focus on text to audio synthesis the proposed model can also generate audio from image queries and it shows competitive performance against a state of the art image to audio synthesis model in a subjective listening test. this study offers a new direction of approaching text to audio synthesis that leverages the naturally occurring audio visual correspondence in videos and the power of pretrained language vision models.", "Pub Date": "2023-09-15"}