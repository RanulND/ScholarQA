{"Title": "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos", "Doi": "10.1109/CVPR52729.2023.02221", "Authors": ["z. yu", "l. zheng", "z. zhao", "f. wu", "j. fan", "k. ren", "j. yu"], "Key Words": ["vision", "language", "and reasoning"], "Abstract": "building benchmarks to systemically analyze different capabilities of video question answering  videoqa  models is challenging yet crucial. existing benchmarks often use non compositional simple questions and suffer from language biases making it difficult to diagnose model weaknesses incisively. a recent benchmark agqa  poses a promising paradigm to generate qa pairs automatically from pre annotated scene graphs enabling it to measure diverse reasoning abilities with granular control. however its questions have limitations in reasoning about the fine grained semantics in videos as such information is absent in its scene graphs. to this end we present anetqa a large scale benchmark that supports fine grained compositional reasoning over the challenging untrimmed videos from activitynet . similar to agqa the qa pairs in anetqa are automatically generated from annotated video scene graphs. the fine grained properties of anetqa are reflected in the following   i  untrimmed videos with fine grained semantics   ii  spatio temporal scene graphs with fine grained taxonomies  and  iii  diverse questions generated from fine grained templates. anetqa attains 1.4 billion unbalanced and 13.4 million balanced qa pairs which is an order of magnitude larger than agqa with a similar number of videos. comprehensive experiments are performed for state of the art methods. the best model achieves 44.5% accuracy while human performance tops out at 84.5% leaving sufficient room for improvement.", "Pub Date": "2023-08-22"}