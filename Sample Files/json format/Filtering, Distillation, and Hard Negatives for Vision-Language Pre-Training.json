{"Title": "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training", "Doi": "10.1109/CVPR52729.2023.00673", "Authors": ["f. radenovic", "a. dubey", "a. kadian", "t. mihaylov", "s. vandenhende", "y. patel", "y. wen", "v. ramanathan", "d. mahajan"], "Key Words": ["vision", "language", "and reasoning"], "Abstract": "vision language models trained with contrastive learning on large scale noisy data are becoming increasingly popular for zero shot recognition problems. in this paper we improve the following three aspects of the contrastive pre training pipeline  dataset noise model initialization and the training objective. first we propose a straightforward filtering strategy titled complexity action and text spotting  cat  that significantly reduces dataset size while achieving improved performance across zero shot vision language tasks. next we propose an approach titled concept distillation to leverage strong unimodal representations for contrastive training that does not increase training complexity while outperforming prior work. finally we modify the traditional contrastive alignment objective and propose an importance sampling approach to up sample the importance of hard negatives without adding additional complexity. on an extensive zero shot benchmark of 29 tasks our distilled and hard negative training  diht  approach improves on 20 tasks compared to the baseline. furthermore for few shot linear probing we propose a novel approach that bridges the gap between zero shot and few shot performance substantially improving over prior work. models are available at github.com facebookresearch/diht.", "Pub Date": "2023-08-22"}