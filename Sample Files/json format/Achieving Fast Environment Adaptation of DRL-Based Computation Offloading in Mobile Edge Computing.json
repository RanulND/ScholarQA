{"Title": "Achieving Fast Environment Adaptation of DRL-Based Computation Offloading in Mobile Edge Computing", "Doi": "10.1109/TMC.2023.3320253", "Authors": ["z. hu", "j. niu", "t. ren", "m. guizani"], "Key Words": ["computation offloading", "deep reinforcement learning", "environment adaptation", "mobile edge computing"], "Abstract": "one of the key issues in mobile edge computing  mec  is computation offloading most policies of which are developed based on mathematical programming  mp . due to the high computational complexity of iterative programming in mp based policies recent years have seen a popular trend to develop offloading policies based on deep reinforcement learning  drl . however on account of the poor generalization ability of drl models in mec environments with different network sizes and settings it is difficult to directly apply drl based offloading policies in unseen mec environments. motivated by this we propose a drl based environment adaptive offloading framework  deat  including a size adaptive scheme  sied  and setting adaptive component  seal . sied leverages the idea of \u201a\u00e4\u00f2time division multiplexing\u201a\u00e4\u00f4 to adapt to varying mec network sizes and order unaware feature extraction to mitigate impacts of different size changing orders. seal adopts system dynamics embedding and offloading policy embedding which guide the finding of the closest pre training mec environment and offloading policy respectively to achieve fast setting adaptation with only few exploring interactions in unseen mec environments. extensive experiments are conducted via both simulation and testbed to demonstrate the adaptation performance advantages of deat in unseen mec environments compared to the state of the art offloading approaches.", "Pub Date": "2024-04-04"}