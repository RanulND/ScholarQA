{"Title": "FastTextDodger: Decision-Based Adversarial Attack Against Black-Box NLP Models With Extremely High Efficiency", "Doi": "10.1109/TIFS.2024.3350376", "Authors": ["x. hu", "g. liu", "b. zheng", "l. zhao", "q. wang", "y. zhang", "m. du"], "Key Words": ["adversarial attacks", "black-box attacks", "natural language processing"], "Abstract": "recently achieving query efficient adversarial example attacks targeting black box natural language models has attracted widespread attention from researchers. this task is considered difficult due to the discrete nature of texts limited knowledge of the target model and strict query access limitations in real world systems. however existing attacks often require a large number of queries or result in low attack success rates having not met practical requirements. to address this we propose fasttextdodger a simple and compact decision based black box textual adversarial attack that generates grammatically correct adversarial texts with high attack success rates and few queries. experimental results show that fasttextdodger achieves an impressive 97.4% attack success rate on benchmark datasets and models and only needs about 200 queries. compared to state of the art attacks fasttextdodger only requires one tenth of the number of queries in text classification and entailment tasks while maintaining comparable attack success rates and perturbed word rates.", "Pub Date": "2024-01-12"}