{"Title": "Accelerating Deep Learning Inference via Model Parallelism and Partial Computation Offloading", "Doi": "10.1109/TPDS.2022.3222509", "Authors": ["h. zhou", "m. li", "n. wang", "g. min", "j. wu"], "Key Words": ["mobile edge computing", "fused-layer", "dnn inference", "partial offloading", "model parallelism"], "Abstract": "with the rapid development of internet of things  iot  and the explosive advance of deep learning there is an urgent need to enable deep learning inference on iot devices in mobile edge computing  mec . to address the computation limitation of iot devices in processing complex deep neural networks  dnns  computation offloading is proposed as a promising approach. recently partial computation offloading is developed to dynamically adjust task assignment strategy in different channel conditions for better performance. in this paper we take advantage of intrinsic dnn computation characteristics and propose a novel fused layer based  fl based  dnn model parallelism method to accelerate inference. the key idea is that a dnn layer can be converted to several smaller layers in order to increase partial computation offloading flexibility and thus further create the better computation offloading solution. however there is a trade off between computation offloading flexibility as well as model parallelism overhead. then we investigate the optimal dnn model parallelism and the corresponding scheduling and offloading strategies in partial computation offloading. in particular we propose a particle swarm optimization with minimizing waiting  psomw  method which explores and updates the fl strategy path scheduling strategy and path offloading strategy to reduce time complexity and avoid invalid solutions. finally we validate the effectiveness of the proposed method in commonly used dnns. the results show that the proposed method can reduce the dnn inference time by an average of 12.75 times compared to the legacy no fl  nfl  algorithm and is very close to the optimal solution achieved by the brute force  bf  algorithm with the difference of less than 0.04%.", "Pub Date": "2022-12-21"}