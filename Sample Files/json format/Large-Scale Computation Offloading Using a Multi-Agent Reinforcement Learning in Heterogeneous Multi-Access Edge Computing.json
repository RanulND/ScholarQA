{"Title": "Large-Scale Computation Offloading Using a Multi-Agent Reinforcement Learning in Heterogeneous Multi-Access Edge Computing", "Doi": "10.1109/TMC.2022.3141080", "Authors": ["z. gao", "l. yang", "y. dai"], "Key Words": ["large-scale computation offloading", "multi-access edge computing", "multi-agent reinforcement learning (marl)", "recurrent multi-agent actor-critic", "attention mechanism"], "Abstract": "recently existing computation offloading methods have provided extremely low service latency for mobile users  mus  in multi access edge computing  mec . however this remains a challenge in large scale mixed cooperative competitive mus heterogeneous mec environments. moreover existing methods focus more on all offloaded tasks handled by static resource allocation mec servers  ess  within a time interval ignoring on demand requirements of heterogeneous tasks resulting in many tasks being dropped or wasting resources especially for latency sensitive tasks. to address these issues we present a decentralized computation offloading solution based on the attention weighted recurrent multi agent actor critic  armaac . first we design a recurrent actor critic framework to assist mu agents in remembering historical resource allocation information of ess to better understand the future state of ess especially in dynamic resource allocation. second an attention mechanism is introduced to compress the joint observation space dimension of all mus agent to adapt to large scale mus. finally the actor critic framework with double centralized critics and dueling network is redesigned considering the instability and convergence difficulties caused by the sensitive relationship between the actor and critic networks. the experiments show that armaac improves task completion rates and reduces average system cost by 11.01%$\\sim$\u201a\u00e0\u00ba14.03% and 10.45%$\\sim$\u201a\u00e0\u00ba15.56% compared with baselines.", "Pub Date": "2023-05-05"}