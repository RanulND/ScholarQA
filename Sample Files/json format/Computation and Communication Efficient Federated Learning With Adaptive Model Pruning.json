{"Title": "Computation and Communication Efficient Federated Learning With Adaptive Model Pruning", "Doi": "10.1109/TMC.2023.3247798", "Authors": ["z. jiang", "y. xu", "h. xu", "z. wang", "j. liu", "q. chen", "c. qiao"], "Key Words": ["adaptive model pruning", "edge computing", "federated learning", "heterogeneity"], "Abstract": "federated learning  fl  has emerged as a promising distributed learning paradigm that enables a large number of mobile devices to cooperatively train a model without sharing their raw data. the iterative training process of fl incurs considerable computation and communication overhead. the workers participating in fl are usually heterogeneous and the workers with poor capabilities may become the bottleneck of model training. to address the challenges of resource overhead and system heterogeneity this article proposes an efficient fl framework called fedmp that improves both computation and communication efficiency over heterogeneous workers through adaptive model pruning. we theoretically analyze the impact of pruning ratio on training performance and employ a multi armed bandit based online learning algorithm to adaptively determine different pruning ratios for heterogeneous workers even without any prior knowledge of their capabilities. as a result each worker in fedmp can train and transmit the sub model that fits its own capabilities accelerating the training process without hurting model accuracy. to prevent the diverse structures of pruned models from affecting the training convergence we further present a new parameter synchronization scheme called residual recovery synchronous parallel  r2sp . besides our proposed framework can be extended to the peer to peer  p2p  setting. extensive experiments on physical devices demonstrate that fedmp is effective for different heterogeneous scenarios and data distributions and can provide up to 4.1\u221a\u00f3 speedup compared to the existing fl methods.", "Pub Date": "2024-02-05"}