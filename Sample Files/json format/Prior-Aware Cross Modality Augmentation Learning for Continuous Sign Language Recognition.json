{"Title": "Prior-Aware Cross Modality Augmentation Learning for Continuous Sign Language Recognition", "Doi": "10.1109/TMM.2023.3268368", "Authors": ["h. hu", "j. pu", "w. zhou", "h. fang", "h. li"], "Key Words": ["cross modality augmentation learning", "editing with prior incorporated", "continuous sign language recognition"], "Abstract": "continuous sign language recognition  cslr  aims to map a sign video into a sentence of text words in the same order as the signs. generally word error rate  wer  i.e. editing distance is adopted as the main evaluation metric. since this metric is not differentiable current deep learning based cslr methods usually resort to connectionist temporal classification  ctc  loss during optimization which maximizes the posterior probability over the sequential alignment. due to the optimization gap between ctc loss and wer the decoded sequence with the maximum probability in ctc may not be the one with the lowest wer. to tackle this issue we propose a novel prior aware cross modality augmentation learning method. in our approach we first generate the pseudo video text pair by cross modality editing i.e. substitution deletion and insertion on the paired real video text data. to ensure the pseudo data quality we guide the editing with both textual grammar prior and visual pose transition consistency prior. in this way the generated pseudo video and text sentence follow the underlying distribution of the sign language data and sever as more genuine hard examples for the cross modality representation learning of our cslr task. based on the real and generated pseudo data we optimize our cslr framework with three loss terms. we evaluate our approach on popular large scale cslr datasets and extensive experiments demonstrate the effectiveness of our method.", "Pub Date": "2024-01-08"}