{"Title": "Recovering from Privacy-Preserving Masking with Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10448234", "Authors": ["a. vats", "z. liu", "p. su", "d. paul", "y. ma", "y. pang", "z. ahmed", "o. kalinli"], "Key Words": ["privacy-preserving machine learning", "language modeling", "large language models", "automatic speech recognition"], "Abstract": "model adaptation is crucial to handle the discrepancy between proxy training data and actual users\u201a\u00e4\u00f4 data received. to effectively perform adaptation textual data of users is typically stored on servers or their local devices where downstream natural language processing  nlp  models can be directly trained using such in domain data. however this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. replacing identifying information in textual data with a generic marker has been recently explored. in this work we leverage large language models  large language model  to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. specifically we propose multiple pre trained and fine tuned large language model based approaches and perform empirical studies on various datasets for the comparison of these methods. experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy preserving token masking.", "Pub Date": "2024-03-18"}