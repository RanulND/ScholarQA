{"Title": "Task Residual for Tuning Vision-Language Models", "Doi": "10.1109/CVPR52729.2023.01049", "Authors": ["t. yu", "z. lu", "x. jin", "z. chen", "x. wang"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "large scale vision language models  vlms  pre trained on billion level data have learned general visual representations and broad visual concepts. in principle the welllearned knowledge structure of the vlms should be inherited appropriately when being transferred to downstream tasks with limited data. however most existing efficient transfer learning  etl  approaches for vlms either damage or are excessively biased towards the prior knowledge e.g. prompt tuning  pt  discards the pre trained text based classifier and builds a new one while adapter style tuning  at  fully relies on the pre trained features. to address this we propose a new efficient tuning approach for vlms named task residual tuning  taskres  which performs directly on the text based classifier and explicitly decouples the prior knowledge of the pre trained models and new knowledge regarding a target task. specifically taskres keeps the original classifier weights from the vlms frozen and obtains a new classifier for the target task by tuning a set of prior independent parameters as a residual to the original one which enables reliable prior knowledge preservation and flexible task specific knowledge exploration. the proposed taskres is simple yet effective which significantly outperforms previous etl methods  e.g. pt and at  on 11 benchmark datasets while requiring minimal effort for the implementation. our code is available at https //github.com geekyutao/taskres.", "Pub Date": "2023-08-22"}