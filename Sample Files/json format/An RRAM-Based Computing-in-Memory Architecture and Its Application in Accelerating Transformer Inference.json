{"Title": "An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference", "Doi": "10.1109/TVLSI.2023.3345651", "Authors": ["z. lu", "x. wang", "m. t. arafin", "h. yang", "z. liu", "j. zhang", "g. qu"], "Key Words": ["accelerator", "computing-in-memory (cim)", "energy efficiency", "resistive random access memory (rram)", "scalability", "transformer"], "Abstract": "deep neural network  dnn  based transformer models have demonstrated remarkable performance in natural language processing  nlp  applications. unfortunately the unique scaled dot product attention mechanism and intensive memory access pose a significant challenge during inference on power constrained edge devices. one emerging solution to this challenge is computing in memory  cim  which uses memory cells for logic computation to reduce data movement and overcome the memory wall. however existing cim designs do not support high precision computations such as floating point operations which are essential for nlp applications. furthermore cim architectures require complex control modules and costly peripheral circuits to harness the full potential of in memory computation. hence this article proposes a scalable rram based in memory floating point computation architecture  rime  that uses single cycle nor nand and minority logic to implement in memory floating point operations. rime features efficient parallel and pipeline capabilities with a centralized control module and a simplified peripheral circuit to eliminate data movement during computation. furthermore the article proposes pipelined implementations of matrix\u201a\u00e4\u00ecmatrix multiplication  matmul  and softmax functions enabling the construction of a transformer accelerator based on rime. extensive experimental results show that compared with gpu based implementation the rime based transformer accelerator improves timing efficiency by  $2.3\\times $  and energy efficiency by  $1.7\\times $  without compromising inference accuracy.", "Pub Date": "2024-02-26"}