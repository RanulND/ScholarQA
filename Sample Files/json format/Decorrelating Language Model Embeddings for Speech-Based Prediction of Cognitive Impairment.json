{"Title": "Decorrelating Language Model Embeddings for Speech-Based Prediction of Cognitive Impairment", "Doi": "10.1109/ICASSP49357.2023.10097265", "Authors": ["l. xu", "k. d. mueller", "j. liss", "v. berisha"], "Key Words": ["language modeling", "clinical speech analytics", "decorrelated features"], "Abstract": "training robust clinical speech based models that generalize requires large sample sizes because speech is variable and high dimensional. researchers have turned to foundational models such as the bidirectional encoder representations from transformers  bert  to generate lower dimensional embeddings and then finetuned the models for a specific down stream clinical task. while there is empirical evidence that this approach is helpful a recent study reveals that the embeddings generated by bert models tend to be highly correlated which makes the downstream models difficult to fine tune particularly in the small sample size regime. in this work we propose a new regularization scheme to penalize correlated embeddings during fine tuning of bert and apply the approach to speech based assessment of cognitive impairment. compared to existing methods the proposed method yields lower estimation errors and smaller false alarm rates in a mini mental state examination  mmse  score regression task.", "Pub Date": "2023-05-05"}