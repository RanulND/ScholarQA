{"Title": "A Corporate Public Opinion Classification Method Based on Multi-teacher Knowledge Distillation", "Doi": "10.1109/ICPECA56706.2023.10076112", "Authors": ["c. xiaoxiao", "x. wenjue", "l. weiping"], "Key Words": ["corporate public opinion classification", "pretrained large-scale model", "knowledge distillation", "multi-teacher distillation", "model compression"], "Abstract": "with the raising of media convergence network public opinion monitoring system has become a necessary tool for government enterprises and financial institutions to respond to public opinion events and create a favorable development environment. in recent years pre trained large scale language models have established significant improvement in performance on a variety of natural language processing tasks such as text classification etc. however the parameters of these large scale models are at least hundreds of millions of which makes it difficult to popularize. therefore knowledge distillation technology has attracted more and more attention as one of the ways to accomplish model compression. based on public sentiment data of enterprises this paper builds a three teacher knowledge distillation deep learning framework to achieve the training of public opinion classification models with the same performance as large models at low computing costs as well as to pre train large models in enterprise public opinion detection systems. an experiment in china shows that the model learned by knowledge distillation has slightly improved performance  acc 2.45%  even under the premise of reducing the number of parameters by 41.58% which illustrates the effectiveness of multi teacher knowledge distillation network framework.", "Pub Date": "2023-03-29"}