{"Title": "Beyond the Limit of Weight-Sharing: Pioneering Space-Evolving NAS with Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10448075", "Authors": ["x. su", "s. you", "h. xu", "x. li", "j. long", "y. chen", "c. xu"], "Key Words": ["nas", "space evolving", "llm"], "Abstract": "large language models  large language model  offer impressive performance across diverse fields but their increasing complexity raises both design costs and the need for specialized expertise. these challenges are intensified for neural architecture search  nas  methods reliant on weight sharing techniques. this paper introduces gnas a new nas method that boosts the search process with the aid of large language model for efficient model discovery. with insights from existing architectures gnas swiftly identifies superior models that can adapt to changing resource constraints. we provide a mathematical framework to facilitate the transfer of knowledge across different model sizes thereby improving search efficiency. our experiments conducted on imagenet nas bench macro and channelbench macro confirm the effectiveness of gnas across both cnn and transformer architectures.", "Pub Date": "2024-03-18"}