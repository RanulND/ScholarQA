{"Title": "Prefix Conditioning Unifies Language and Label Supervision", "Doi": "10.1109/CVPR52729.2023.00280", "Authors": ["k. saito", "k. sohn", "x. zhang", "c. -l. li", "c. -y. lee", "k. saenko", "t. pfister"], "Key Words": ["vision", "language", "reasoning"], "Abstract": "pretraining visual models on web scale image caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classification data. image caption datasets are more \u201a\u00e4\u00faopendomain \u201a\u00e4\u00f9 containing broader scene types and vocabulary words and result in models that have strong performance in fewand zero shot recognition tasks. however large scale classification datasets can provide fine grained categories with a balanced label distribution. in this work we study a pretraining strategy that uses both classification and caption datasets to unite their complementary benefits. first we show that naively unifying the datasets results in sub optimal performance in downstream zero shot recognition tasks as the model is affected by dataset bias  the coverage of image domains and vocabulary words is different in each dataset. we address this problem with novel prefix conditioning a simple yet effective method that helps disentangle dataset biases from visual concepts. this is done by intro ducing prefix tokens that inform the language encoder of the input data type  e.g. classification vs caption  at training time. our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. prefix conditioning is generic and can be easily integrated into existing vl pretraining objectives such as clip or unicl. in experiments we show that it improves zero shot image recognition and robustness to image level distribution shift.", "Pub Date": "2023-08-22"}