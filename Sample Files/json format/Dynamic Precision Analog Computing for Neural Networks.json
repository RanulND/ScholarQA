{"Title": "Dynamic Precision Analog Computing for Neural Networks", "Authors": ["s. garg", "j. lou", "a. jain", "z. guo", "b. j. shastri", "m. nahmias"], "Pub Date": "2022-11-23", "Abstract": "analog electronic and optical computing exhibit tremendous advantages over digital computing for accelerating deep learning when operations are executed at low precision. although digital architectures support programmable precision to increase efficiency analog computing architectures today only support a single static precision. in this work we characterize the relationship between the effective number of bits  enob  of precision of analog processors which is limited by noise and digital bit precision for quantized neural networks. we propose extending analog computing architectures to support dynamic levels of precision by repeating operations and averaging the result decreasing the impact of noise. to utilize dynamic precision we propose a method for learning the precision of each layer of a pre trained model without retraining network weights. we evaluate this method on analog architectures subject to shot noise thermal noise and weight noise and find that employing dynamic precision reduces energy consumption by up to 89% for computer vision models such as resnet50 and by 24% for natural language processing models such as bert. in one example we apply dynamic precision to a shot noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aj mac for resnet50 and 1.6 aj mac for bert with ${< }2\\%$ accuracy degradation implying that the optical energy consumption is unlikely to be the dominant cost.", "Doi": "10.1109/JSTQE.2022.3218019", "Key Words": ["neural networks", "signal-to-noise ratio", "analog computing", "optical computing"]}