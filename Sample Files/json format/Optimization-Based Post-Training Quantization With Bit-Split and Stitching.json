{"Title": "Optimization-Based Post-Training Quantization With Bit-Split and Stitching", "Authors": ["p. wang", "w. chen", "x. he", "q. chen", "q. liu", "j. cheng"], "Pub Date": "2023-01-06", "Abstract": "deep neural networks have shown great promise in various domains. meanwhile problems including the storage and computing overheads arise along with these breakthroughs. to solve these problems network quantization has received increasing attention due to its high efficiency and hardware friendly property. nonetheless most existing quantization approaches rely on the full training dataset and the time consuming fine tuning process to retain accuracy. post training quantization does not have these problems however it has mainly been shown effective for 8 bit quantization. in this paper we theoretically analyze the effect of network quantization and show that the quantization loss in the final output layer is bounded by the layer wise activation reconstruction error. based on this analysis we propose an optimization based post training quantization framework and a novel bit split optimization approach to achieve minimal accuracy degradation. the proposed framework is validated on a variety of computer vision tasks including image classification object detection instance segmentation with various network architectures. specifically we achieve near original model performance even when quantizing fp32 models to 3 bit without fine tuning.", "Doi": "10.1109/TPAMI.2022.3159369", "Key Words": ["deep neural networks", "compression", "quantization", "post-training quantization"]}