{"Title": "Adaptive Block-Wise Regularization and Knowledge Distillation for Enhancing Federated Learning", "Doi": "10.1109/TNET.2023.3301972", "Authors": ["j. liu", "q. zeng", "h. xu", "y. xu", "z. wang", "h. huang"], "Key Words": ["federated learning", "edge computing", "heterogeneity", "regularization", "knowledge distillation"], "Abstract": "federated learning  fl  is a distributed model training framework that allows multiple clients to collaborate on training a global model without disclosing their local data in edge computing  ec  environments. however fl usually faces statistical heterogeneity  e.g. non iid data  and system heterogeneity  e.g. computing and communication capabilities  resulting in poor model training performance. to deal with the above two challenges we propose an efficient fl framework named fedbr which integrates the idea of block wise regularization and knowledge distillation  kd  into the pioneering fl algorithm fedavg for resource constrained edge computing. specifically we first divide the model into multiple blocks according to the layer order of deep neural network  dnn . the server only sends some consecutive model blocks instead of an entire model to clients for communication efficiency. then the clients make use of knowledge distillation to absorb the knowledge of global model blocks to alleviate statistical heterogeneity during local training. we provide a theoretical convergence guarantee for fedbr and show that the convergence bound will decrease as the increasing number of model blocks sent by the server. besides since the increasing number of model blocks brings more computing and communication costs we design a heuristic algorithm  gmbs  to determine the appropriate number of model blocks for clients according to their varied data distributions computing and communication capabilities. extensive experimental results show that fedbr can reduce the bandwidth consumption by about 31% and achieve an average accuracy improvement of around 5.6% compared with the baselines under heterogeneous settings.", "Pub Date": "2024-02-19"}