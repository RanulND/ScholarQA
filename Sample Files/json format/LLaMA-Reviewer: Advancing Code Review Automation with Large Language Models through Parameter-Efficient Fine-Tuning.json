{"Title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning", "Doi": "10.1109/ISSRE59848.2023.00026", "Authors": ["j. lu", "l. yu", "x. li", "l. yang", "c. zuo"], "Key Words": ["code review automation", "large language models (llms)", "parameter-efficient fine-tuning (peft)", "deep learning", "llama", "software quality assurance"], "Abstract": "the automation of code review activities a long standing pursuit in software engineering has been primarily addressed by numerous domain specific pre trained models. despite their success these models frequently demand extensive resources for pre training from scratch. in contrast large language models  llms  provide an intriguing alternative given their remarkable capabilities when supplemented with domain specific knowledge. however their potential for automating code review tasks remains largely unexplored.in response to this research gap we present llama reviewer an innovative framework that leverages the capabilities of llama a popular llm in the realm of code review. mindful of resource constraints this framework employs parameter efficient fine tuning  peft  methods delivering high performance while using less than 1% of trainable parameters.an extensive evaluation of llama reviewer is conducted on two diverse publicly available datasets. notably even with the smallest llama base model consisting of 6.7b parameters and a limited number of tuning epochs llama reviewer equals the performance of existing code review focused models.the ablation experiments provide insights into the influence of various fine tuning process components including input representation instruction tuning and different peft methods. to foster continuous progress in this field the code and all peft weight plugins have been made open source.", "Pub Date": "2023-11-02"}