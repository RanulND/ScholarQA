{"Title": "From English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition", "Doi": "10.1109/ICASSP49357.2023.10094903", "Authors": ["c. -h. h. yang", "b. li", "y. zhang", "n. chen", "r. prabhavalkar", "t. n. sainath", "t. strohman"], "Key Words": ["cross-lingual speech recognition", "model reprogramming", "pre-trained adaptation", "and foundation speech models"], "Abstract": "in this work we propose a new parameter efficient learning framework based on neural model reprogramming for cross lingual speech recognition which can re purpose well trained english automatic speech recognition  asr  models to recognize the other languages. we design different auxiliary neural architectures focusing on learnable pre trained feature enhancement that for the first time empowers model reprogramming on asr. specifically we investigate how to select trainable components  i.e. encoder  of a conformer based rnn transducer as a frozen pre trained backbone. experiments on a seven language multilingual librispeech speech  mls  task show that model reprogramming only requires 4.2%  11m out of 270m  to 6.8%  45m out of 660m  of its original trainable parameters from a full asr model to perform competitive results in a range of 11.9% to 8.1% wer averaged across different languages. in addition we discover different setups to make large scale pre trained asr succeed in both monolingual and multilingual speech recognition. our methods outperform existing asr tuning architectures and their extension with self supervised losses  e.g. w2v bert  in terms of lower wer and better training efficiency.", "Pub Date": "2023-05-05"}