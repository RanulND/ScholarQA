{"Title": "Joint Task Offloading and Resource Allocation for Quality-Aware Edge-Assisted Machine Learning Task Inference", "Doi": "10.1109/TVT.2023.3235520", "Authors": ["w. fan", "z. chen", "z. hao", "f. wu", "y. liu"], "Key Words": ["task offloading", "data quality", "edge computing", "edge intelligence", "inference"], "Abstract": "edge computing is essential to enhance delay sensitive and computation intensive machine learning  ml  task inference services. quality of inference results which is mainly impacted by the task data and ml models is an important indicator impacting the system performance. in this paper we consider a quality aware edge assisted ml task inference scenario and propose a resource management scheme to minimize the total task processing delay while guaranteeing the stability of all the task queues and the inference accuracy requirements of all the tasks. in our scheme the task offloading task data adjustment computing resource allocation and wireless channel allocation are jointly optimized. the lyapunov optimization technique is adopted to transform the original optimization problem into a deterministic problem for each time slot. considering the high complexity of the optimization problem we design an algorithm that decomposes the problem into a task offloading and channel allocation  toca  sub problem a task data adjustment sub problem and a computing resource allocation sub problem and then solves them iteratively. a low complexity heuristic algorithm is also designed to solve the toca sub problem efficiently. extensive simulations are conducted by varying different crucial parameters. the results demonstrate the superiority of our scheme in comparison with 4 other schemes.", "Pub Date": "2023-05-15"}