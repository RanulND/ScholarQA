{"Title": "Universal Cross-Lingual Data Generation for Low Resource ASR", "Doi": "10.1109/TASLP.2023.3345150", "Authors": ["w. wang", "y. qian"], "Key Words": ["low-resource speech recognition", "text-to-seech", "data splicing", "self-supervised learning"], "Abstract": "significant advances in end to end  e2e  automatic speech recognition  asr  have primarily been concentrated on languages rich in annotated data. nevertheless a large proportion of languages worldwide which are typically low resource continue to pose significant challenges. to address this issue this study presents a novel speech synthesis framework based on data splicing that leverages self supervised learning  ssl  units from hidden unit bert  hubert  as universal phonetic units. in our framework the ssl phonetic units serve as crucial bridges between speech and text across different languages. by leveraging these units we successfully splice speech fragments from high resource languages into synthesized speech that maintains acoustic coherence with text from low resource languages. to further enhance the practicality of the framework we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. the application of this confidence sampling strategy in data splicing significantly accelerates asr model convergence and enhances overall asr performance. experimental results on the commonvoice dataset show 25 35% relative improvement for four indo european languages and about 20% for turkish using a 4 gram language model for rescoring under a 10 hour low resource setup. furthermore we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing resulting in an additional 10% relative improvement.", "Pub Date": "2024-01-05"}