{"Title": "Empowering Unsupervised Domain Adaptation with Large-scale Pre-trained Vision-Language Models", "Doi": "10.1109/WACV57701.2024.00267", "Authors": ["z. lai", "h. bai", "h. zhang", "x. du", "j. shan", "y. yang", "c. -n. chuah", "m. cao"], "Key Words": ["algorithms", "machine learning architectures", "formulations", "and algorithms", "algorithms", "vision + language and/or other modalities"], "Abstract": "unsupervised domain adaptation  uda  aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. traditional uda methods face the challenge of the tradeoff between domain alignment and semantic class discriminability especially when a large domain gap exists between the source and target domains. the efforts of applying large scale pre training to bridge the domain gaps remain limited. in this work we propose that vision language models  vlms  can empower uda tasks due to their training pattern with language alignment and their large scale pre trained datasets. for example clip and glip have shown promising zero shot generalization in classification and detection tasks. however directly fine tuning these vlms into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. therefore in this work we first study an efficient adaption of vlms to preserve the original knowledge while maximizing its flexibility for learning new knowledge. then we design a domain aware pseudo labeling scheme tailored to vlms for domain disentanglement. we show the superiority of the proposed methods in four uda classification and two uda detection benchmarks with a significant improvement  +9.9%  on domainnet.", "Pub Date": "2024-04-09"}