{"Title": "Multiscale Self-Attention Architecture in Temporal Neural Network for Nonintrusive Load Monitoring", "Authors": ["z. shan", "g. si", "k. qu", "q. wang", "x. kong", "y. tang", "c. yang"], "Pub Date": "2023-05-15", "Abstract": "nonintrusive load monitoring  nilm  constitutes a significant function of the smart grid in the future. the purpose is to ameliorate the consumption and supply of electricity by disaggregating the total load to the appliance level load without intrusive monitoring. recently energy disaggregation is improved with the emergence of deep learning but the imbalanced datasets and long sequences bring multiple difficulties to model training. the distribution of on off states and the limitation of the model lead to massive false positive samples and undetected events. to tackle these problems we proposed a multiscale self attention network  msanet  to utilize the global temporal correlation and local sequential features. specifically the dilated window self attention mechanism is proposed to compute the local attention and the multibranch structure is to exploit sequential features of different scales. furthermore the embedding of global temporal information is introduced to improve global contextual awareness and subtask networks are designed for different tasks respectively to alleviate the effect of imbalance. the proposed model is evaluated on the reference energy disaggregation dataset  redd  and u.k. domestic appliance level electricity  dale  dataset and shows outstanding performance on the mean absolute error  mae  and  $f1$  score compared with baseline algorithms.", "Doi": "10.1109/TIM.2023.3271009", "Key Words": ["deep learning", "energy disaggregation", "nonintrusive load monitoring (nilm)", "self-attention", "sequence-to-sequence"]}