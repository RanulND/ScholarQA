{"Title": "An Efficient 2D Method for Training Super-Large Deep Learning Models", "Doi": "10.1109/IPDPS54959.2023.00031", "Authors": ["q. xu", "y. you"], "Key Words": ["matrix-matrix multiplication", "distributed training", "neural networks", "natural language processing"], "Abstract": "since the rise of transformer  and bert  large language models   have been proposed and shown unprecedented performance in tasks like translation classification and text generation. however due to the memory constraint model parallelism must be used to split the model across multiple processors. inter layer partition intra layer partition and sparse activation are the major approaches to achieve model parallelism. among them inter layer partition   often requires the model to be explicitly expressed as a stack of sub modules the number of which equals to the number of processors and would introduce either gradient staleness or bubble overhead  while the sparse activation  is primarily designed for google tpu cluster and hard to deploy on gpu servers intra layer partition  especially megatron lm  can be easily deployed on gpu servers and has been adopted in subsequent works like turing nlg and m6. though as pioneers of intra layer parallelism they still show memory redundancy and sub optimal communication efficiency which reveals the space for further improvements. in this work we leverage summa  and propose optimus a highly efficient and scalable paradigm for training super large language models. in optimus activations and gradients are partitioned and distributed along processors all the way through forward and backward propagations with hardly any memory redundancy. the isoefficiency of communication in pure model parallelism improves from w ~ p3 for megatron lm to $w\\sim { \\sqrt p \\log p ^3}$ for our optimus. this framework is implemented with open source deep learning framework pytorch and consolidates existing techniques such as mixed precision training  activation checkpointing  and data parallelism. in experiments on tacc frontera supercomputers optimus shows 1.48\u221a\u00f3 the speed for training 1.78\u221a\u00f3 speed for inference and 8\u221a\u00f3 the maximum batch size over megatron lm on 64 gpus in pure model parallelism  and 1.73\u221a\u00f3 speed for training 2.32\u221a\u00f3 speed for inference with data parallelism size equaling 2 on 128 gpus. in pure model parallelism optimus surpasses megatron lm in weak scaling efficiency by a great margin and shows an extraordinary increasing strong scaling efficiency. optimus would facilitate the scaling of language models and serve as a strong thrust in the space exploration of artificial intelligence.", "Pub Date": "2023-07-18"}