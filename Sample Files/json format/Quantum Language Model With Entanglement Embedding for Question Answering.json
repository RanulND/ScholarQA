{"Title": "Quantum Language Model With Entanglement Embedding for Question Answering", "Authors": ["y. chen", "y. pan", "d. dong"], "Pub Date": "2023-05-17", "Abstract": "quantum language models  qlms  in which words are modeled as a quantum superposition of sememes have demonstrated a high level of model transparency and good post hoc interpretability. nevertheless in the current literature word sequences are basically modeled as a classical mixture of word states which cannot fully exploit the potential of a quantum probabilistic description. a quantum inspired neural network  nn  module is yet to be developed to explicitly capture the nonclassical correlations within the word sequences. we propose a nn model with a novel entanglement embedding  ee  module whose function is to transform the word sequence into an entangled pure state representation. strong quantum entanglement which is the central concept of quantum information and an indication of parallelized correlations among the words is observed within the word sequences. the proposed qlm with ee  qlm ee  is proposed to implement on classical computing devices with a quantum inspired nn structure and numerical experiments show that qlm ee achieves superior performance compared with the classical deep nn models and other qlms on question answering  qa  datasets. in addition the post hoc interpretability of the model can be improved by quantifying the degree of entanglement among the word states.", "Doi": "10.1109/TCYB.2021.3131252", "Key Words": ["complex-valued neural network (nn)", "entanglement embedding (ee)", "interpretability", "quantum language model (qlm)"]}