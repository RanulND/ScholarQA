{"Title": "Compressed Collective Sparse-Sketch for Distributed Data-Parallel Training of Deep Learning Models", "Doi": "10.1109/JSAC.2023.3242733", "Authors": ["k. ge", "k. lu", "y. fu", "x. deng", "z. lai", "d. li"], "Key Words": ["distributed training", "deep learning", "sparse", "communication", "sketch"], "Abstract": "distributed data parallel training  ddp  is prevalent in large scale deep learning. to increase the training throughput and scalability high performance collective communication methods such as allreduce have recently proliferated for ddp use. however these approaches require long communication periods with increasing model sizes. collective communication transmits many sparse gradient values that can be efficiently compressed to reduce the required training time. state of the art compression approaches do not provide mergeable compression for allreduce and lack convergence bounds. we present a sparse sketch reducer  s2reducer  a sparsity preserving sketch based collective communication method. s2reducer preserves gradient sparsity and reduces communication costs via a bitmap informed count sketch structure and adapts to efficient allreduce operators. we tune the count sketch organization to minimize the hash conflicts in a fixed size budget. we prove that our method has the same convergence rate as vanilla data parallel training and a much smaller communication overhead than those of state of the art methods. we implement a gpu accelerated s2reducer for the ring allreduce based ddp system. we perform extensive evaluations against four state of the art methods across seven deep learning models. our results show that s2reducer converges to the same accuracy as that of state of the art approaches while reducing the sparse communication overhead by up to 86% and achieving a speedup of up to  $3.5\\times $  in distributed training.", "Pub Date": "2023-03-17"}