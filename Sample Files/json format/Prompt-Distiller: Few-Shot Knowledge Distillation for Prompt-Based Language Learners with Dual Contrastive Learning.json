{"Title": "Prompt-Distiller: Few-Shot Knowledge Distillation for Prompt-Based Language Learners with Dual Contrastive Learning", "Doi": "10.1109/ICASSP49357.2023.10095721", "Authors": ["b. hou", "c. wang", "x. chen", "m. qiu", "l. feng", "j. huang"], "Key Words": ["knowledge distillation", "few-shot learning", "prompt-based learning", "pre-trained language model"], "Abstract": "prompt based learning has improved the few shot learning performance of large scale pre trained language models  plms . yet it is challenging to deploy large scale plms in resource constrained environments for online applications. knowledge distillation  kd  is a promising approach for plm compression. however distilling prompt tuned plms in the few shot learning setting is a non trivial problem due to the lack of task specific training data and kd techniques for the new prompting paradigm. we propose prompt distiller the first few shot kd algorithm for prompt tuned plms which forces the student model to learn from both its pre trained and prompt tuned teacher models to alleviate the model overfitting problem. we further design a contrastive learning technique to learn higher order dependencies from intermediate layer representations of teacher models considering different knowledge capacities of teacher and student models. extensive experiments over various datasets show that prompt distiller consistently outperforms baselines by a large margin.", "Pub Date": "2023-05-05"}