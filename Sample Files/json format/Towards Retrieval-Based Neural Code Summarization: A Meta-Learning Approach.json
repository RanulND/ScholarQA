{"Title": "Towards Retrieval-Based Neural Code Summarization: A Meta-Learning Approach", "Doi": "10.1109/TSE.2023.3238161", "Authors": ["z. zhou", "h. yu", "g. fan", "z. huang", "k. yang"], "Key Words": ["code summarization", "deep learning", "information retrieval", "meta-learning"], "Abstract": "code summarization aims to generate code summaries automatically and has attracted a lot of research interest lately. recent approaches to it commonly adopt neural machine translation techniques which train a seq2seq model on a large corpus and assume it could work on various new code snippets. however codes are highly varied in practice due to different domains businesses or programming styles. therefore it is challenging to learn such a variety of patterns into a single model. in this paper we propose a brand new framework for code summarization based on meta learning and code retrieval named mlcs to tackle this issue. in this framework the summarization of each target code is formalized as a few shot learning task where its similar examples are used as training data and the testing example is itself. we retrieve examples similar to the target code in a rank and filter manner. given a neural code summarizer we optimize it into a meta learner via model agnostic meta learning  maml . during inference the meta learner first adapts to the retrieved examples and yields an exclusive model for the target code and then generates its summary. extensive experiments on real world datasets show   1  utilizing mlcs a standard seq2seq model is able to outperform previous state of the art approaches including both neural models and retrieval based neural models   2  mlcs can flexibly adapt to existing neural code summarizers without modifying their architecture and could significantly improve their performance with the relative gain of up to 112.7% on bleu 4 23.2% on rouge l and 31.5% on meteor   3  compared to the existing retrieval based neural approaches mlcs can better leverage multiple similar examples and shows better generalization ability on different retrievers unseen retrieval corpus and low frequency words.", "Pub Date": "2023-04-18"}