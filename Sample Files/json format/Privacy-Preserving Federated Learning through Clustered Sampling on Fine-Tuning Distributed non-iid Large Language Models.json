{"Title": "Privacy-Preserving Federated Learning through Clustered Sampling on Fine-Tuning Distributed non-iid Large Language Models", "Doi": "10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00101", "Authors": ["s. yun", "z. a. bhuiyan", "m. t. a. h. sadi", "s. su"], "Key Words": ["federated learning", "large language models", "clustered sampling", "model similarity"], "Abstract": "recently large language models  large language model  have been a phenomenal trend in the artificial intelligence field. however training and fine tuning can be challenging because of privacy concerns and limited computing resources. federated learning  fl  has emerged as a novel machine learning framework offering privacy protection. the challenges in applying fl to real world applications include dealing with heterogeneous data poor client updates and client selection. this paper introduces privacy preserving federated learning through clustered sampling on large language model  fclm  a framework that clusters models by their distribution similarity. it helps the model group similar models to improve text data heterogeneity handling and privacy concerns in distributed machine learning environments. the fclm framework is implemented and evaluated using popular language models and text data. the framework shows a robust performance over the heterogeneous text data which can further extend to the use of more complex large language model.", "Pub Date": "2024-04-11"}