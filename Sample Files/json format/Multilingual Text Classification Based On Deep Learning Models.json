{"Title": "Multilingual Text Classification Based On Deep Learning Models", "Doi": "10.1109/ITAIC58329.2023.10409100", "Authors": ["l. lin"], "Key Words": ["multilingual texts classification", "large language models", "model distillation", "pre-trained models", "knowledge transferring"], "Abstract": "in an era characterized by global interconnectedness the imperative for seamless communication across diverse languages is more pronounced than ever. the present research introduces a sophisticated pre trained model denoted as multilingual bert  mbert  meticulously engineered to execute text classification with efficacy across 21 european languages. this model is pivotal in augmenting performance uniformity amongst a myriad of european language families. significantly mbert showcases enhanced parallelization and a notable reduction in processing time for extensive documents standing out in comparison to its contemporaries. in addition a distilled model is cultivated under the tutelage of mbert aiming to delve into the intricacies of knowledge transfer between the two entities. the empirical results corroborate the superior proficiency of mbert in the realm of language type classification thereby furnishing a robust methodology for language categorization. concurrently the distilled model adeptly assimilates knowledge from its precursor mbert thereby validating the efficacy of the proposed methodology.", "Pub Date": "2024-02-01"}