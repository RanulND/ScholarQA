{"Title": "Data-Driven Pruning Algorithm Based on Result Orientation", "Doi": "10.1109/ICNLP58431.2023.00043", "Authors": ["j. wu", "z. zhang", "b. zhao", "y. wang"], "Key Words": ["model compression", "pruning algorithm", "result orientation", "data-driven", "feature map"], "Abstract": "with their outstanding performance in natural language processing tasks such as machine translation and semantic recognition deep neural networks have attracted great attention from both academia and industry. for more complex nlp tasks people try to add more parameters to the network expand more layers input larger data samples to produce a large model to solve the complex task. however it is not the case that the deeper the layers the better the parameters. there is a large amount of redundant information in the parameters which not only contributes nothing to the results but also increases the computational burden of the model and the storage burden of the hardware. eliminating a small amount of redundant information often has no effect on the recognition rate of the model but slightly improves it  so the neural network model needs to be compressed. existing compression methods include model pruning parameter quantization tensor decomposition knowledge distmation etc.  in this paper model pruning algorithm is selected to implement a result oriented data driven pruning algorithm by introducing the propagation characteristics and inter layer correlation of neural networks and automatic decision making based on feature map information. finally the effectiveness of the result   oriented data   driven pruning algorithm is proved by comparative experiments.", "Pub Date": "2023-09-06"}