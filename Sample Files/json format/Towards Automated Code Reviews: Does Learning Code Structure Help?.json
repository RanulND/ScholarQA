{"Title": "Towards Automated Code Reviews: Does Learning Code Structure Help?", "Doi": "10.1109/SANER56733.2023.00075", "Authors": ["h. y. lin", "p. thongtanunam"], "Key Words": ["code review", "neural machine translation"], "Abstract": "code review is a crucial ingredient to quality software development but requires a large amount of time and effort for developers. to optimise this manual process recent research on automated code review seeks to leverage neural machine translation  nmt  models to perform tasks such as automated code improvement. a recent work had pretrained the nmt model for automated code review in order to equip the model with general coding knowledge. however their pretraining approach is generic to natural languages which does not leverage the unique properties of coding languages. therefore we set out to explore two state of the art pretrained nmt models  i.e. codet5 and graphcodebert  that were designed to learn code structure. we studied the models\u201a\u00e4\u00f4 abilities to generate correct code improvement through an empirical evaluation based on five different datasets. our results showed that in terms of generating correct code sequences codet5 graphcodebert and the prior work achieved an average accuracy of 22% 18% and 10% respectively. in terms of generating correct dataflow structures they achieved an average accuracy of 33% 30% and 22% respectively. the results suggested that the code structure focused approaches could outperform the generic pretraining approach. this work contributes towards enhancing automated code review techniques by understanding the effectiveness of code structure focused nmt models.", "Pub Date": "2023-05-15"}