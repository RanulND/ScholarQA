{"Title": "Graft: Efficient Inference Serving for Hybrid Deep Learning With SLO Guarantees via DNN Re-Alignment", "Doi": "10.1109/TPDS.2023.3340518", "Authors": ["j. wu", "l. wang", "q. jin", "f. liu"], "Key Words": ["deep learning systems", "edge computing", "hybrid deep learning", "gpu sharing"], "Abstract": "deep neural networks  dnns  have been widely adopted for various mobile inference tasks yet their ever increasing computational demands are hindering their deployment on resource constrained mobile devices. hybrid deep learning partitions a dnn into two parts and deploys them across the mobile device and a server aiming to reduce inference latency or prolong battery life of mobile devices. however such partitioning produces  non uniform  dnn fragments which are hard to serve efficiently on the server. this article presents graft\u201a\u00e4\u00eean efficient inference serving system for hybrid deep learning with latency service level objective  slo  guarantees. our main insight is to mitigate the non uniformity by a core concept called dnn re alignment allowing multiple heterogeneous dnn fragments to be restructured to share layers. to fully exploit the potential of dnn re alignment graft employs fine grained gpu resource sharing. based on that we propose efficient algorithms for merging grouping and re aligning dnn fragments to maximize request batching opportunities minimizing resource consumption while guaranteeing the inference latency slo. we implement a graft prototype and perform extensive experiments with five types of widely used dnns and real world network traces. our results show that graft improves resource efficiency by up to 70% compared with the state of the art inference serving systems.", "Pub Date": "2023-12-27"}