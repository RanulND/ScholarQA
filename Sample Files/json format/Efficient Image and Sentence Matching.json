{"Title": "Efficient Image and Sentence Matching", "Doi": "10.1109/TPAMI.2022.3178485", "Authors": ["y. huang", "y. wang", "l. wang"], "Key Words": ["efficient image and sentence matching", "cross-modal similarity distillation", "vision and language"], "Abstract": "recently the accuracy of image and sentence matching has been continuously improved by larger and larger models. however such large models not only need huge storage space but also slow down inference speed which are not very suitable for low cost devices in real world applications. to our knowledge this work makes the first attempt to improve the model efficiency in the context of image and sentence matching and accordingly proposes a simple yet effective whitened similarity distillation  wsd  method which can distill cross modal knowledge from a large teacher model to a small student model of both high efficiency and accuracy. the high efficiency is achieved by performing  1  feature representation based on efficient backbone networks  and 2  similarity measurement in a fast n to n manner. however the accuracy of such a student model is much worse than that of teacher model because there exists very large variation inconsistency between two cross modal similarity matrices of teacher and student models which is hard to reduce during the similarity distillation. by performing two whitening like transformations in the orthogonal space the proposed wsd can reduce the large variation inconsistency more isotropically and is able to improve the accuracy of student model. we perform extensive experiments on two benchmark datasets and demonstrate the effectiveness of the proposed wsd. compared with the teacher model our distilled student model is 7\u221a\u00f3 smaller  in model size  and 9\u221a\u00f3 faster  in testing speed  only at the cost of $< $<2% accuracy decrease.", "Pub Date": "2023-02-03"}