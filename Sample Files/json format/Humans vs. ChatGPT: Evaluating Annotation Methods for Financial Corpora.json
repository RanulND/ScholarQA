{"Title": "Humans vs. ChatGPT: Evaluating Annotation Methods for Financial Corpora", "Doi": "10.1109/BigData59044.2023.10386425", "Authors": ["j. kaikaus", "h. li", "r. j. brunner"], "Key Words": ["large language models", "earnings calls", "emotion recognition", "sentiment analysis"], "Abstract": "given the vast amount of unstructured financial text data available today there is a high demand for reliable quality annotations to facilitate robust model development. however traditional methods can often be expensive and time inefficient. in this study we investigate annotations for emotion sentiment and cognitive dissonance generated by the large language models  llms  gpt 3.5 and gpt 4 for quarterly earnings conference calls and compare them against human annotations obtained via traditional methods. we also investigate different prompt engineering choices on llm annotation quality experimenting with 4 styles of prompts centered around varying the amount of contextual information given and how it is presented to the models. our results show the gpt models are not only more consistent and reliable than human annotators but also provide annotations in a more cost  and time efficient manner.", "Pub Date": "2024-01-22"}