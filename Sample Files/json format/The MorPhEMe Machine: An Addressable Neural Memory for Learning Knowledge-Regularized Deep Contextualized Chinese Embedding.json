{"Title": "The MorPhEMe Machine: An Addressable Neural Memory for Learning Knowledge-Regularized Deep Contextualized Chinese Embedding", "Doi": "10.1109/TASLP.2024.3364610", "Authors": ["z. quan", "c. -m. vong", "w. zeng", "w. yang"], "Key Words": ["representation learning", "pre-trained language model", "contextualized embedding", "external memory", "addressable neural memory"], "Abstract": "deep contextualized embeddings as learned by large pre training models have proven highly effective in various downstream natural language processing tasks. however the embedding space in these large models lacks explicit regularization leading to underfitting and substantial costs during large scale training on huge corpora. in this paper we present a novel approach to learning deep contextualized embeddings introducing linguistic knowledge regularization. specifically our proposed model morpheme  morphology and phonology embedding memory  features an external addressable memory with two additional addressable memories for storing morphology and phonology knowledge. morpheme can be seamlessly stacked into a deep architecture. notably different from existing pre training models morpheme boasts two distinctive features  i  compositional encoding and decompositional decoding facilitated by a dynamic addressing mechanism  and ii  explicit memory embedding regularization through cross layer memory sharing. theoretical analysis suggests that the inclusion of morphology and phonology enables morpheme to reduce the modeling complexity of natural language sequences. we evaluate morpheme across a diverse set of chinese natural language processing tasks including language modeling word similarity computation word analogy reasoning relation extraction and machine reading comprehension. experimental results demonstrate that morpheme in contrast to state of the art models achieves remarkable improvements with fewer parameters and rapid convergence.", "Pub Date": "2024-03-07"}