{"Title": "Twofold Sparsity: Joint Bit- and Network-Level Sparsity for Energy-Efficient Deep Neural Network Using RRAM Based Compute-In-Memory", "Doi": "10.1109/ACCESS.2024.3373709", "Authors": ["f. karimzadeh", "a. raychowdhury"], "Key Words": ["computing-in-memory", "deep learning compression", "edge computing", "quantization", "sparsity"], "Abstract": "on device intelligence and artificial intelliegence powered edge devices require compressed deep learning algorithm and energy efficient hardware. compute in memory  cim  architecture is a more suitable candidate than traditional complementary metal oxide semiconductor  cmos  technology for deep learning applications since computations are performed directly within the memory itself reducing the need for data movement between memory and processing units. however the current deep learning compression techniques are not designed to take advantage of cim architecture. in this work we proposed twofold sparsity a joint bit  and network level sparsity method to highly sparsify the deep leaning models by taking advantage of cim architecture for energy efficient computations. twofold sparsity method sparsify the network during training by adding two regularizations one to sparsify the weights using linear feedback shift register  lfsr  mask and the other one to sparsify the values in the bit level by making bits zero. during inference the same lfsrs is used to choose the correct sparsed weights for multiplication between input and weights and 2bit cell rram based cim is responsible to do the computation. twofold sparsity method achieved 1.3x to 4.35x energy efficiency in different sparsity rates compared to baselines and eventually enabling powerful deep learning models to be run on power constrained edge devices.", "Pub Date": "2024-03-08"}