{"Title": "An Empirical Investigation on the Performance of Domain Adaptation for T5 Code Completion", "Doi": "10.1109/SANER56733.2023.00073", "Authors": ["d. fukumoto", "y. kashiwa", "t. hirao", "k. fujiwara", "h. iida"], "Key Words": ["code completion", "codet5", "domain adaptation", "fine-tuning", "transfer-learning"], "Abstract": "code completion has the benefit of improving coding speed and reducing the chance of inducing bugs. in recent years dl based code completion techniques have been proposed. in particular pre trained models have shown outstanding performance because they can complete code by considering the context before and after it is completed. while the model can generate the set of candidate codes some of those might need to be modified by developers because projects can have different coding rules.in this study to complete code that fits a specific project appropriately we train the codet5 model with additional data from the target project. this fine tuning approach is called do main adaptation and is often used in neural machine translation. our preliminary experiment observes that our domain adapted model improves 5.3% of the perfect prediction rate and 3.4% of the edit distance rate compared to the fine tuned model with the out of domain dataset. furthermore we discover that the improvement is greater with a larger repository size. the model that is trained with a small dataset however hardly improves or performs worse.", "Pub Date": "2023-05-15"}