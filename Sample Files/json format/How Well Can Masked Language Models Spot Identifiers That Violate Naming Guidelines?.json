{"Title": "How Well Can Masked Language Models Spot Identifiers That Violate Naming Guidelines?", "Doi": "10.1109/SCAM59687.2023.00023", "Authors": ["j. villmow", "v. campos", "j. petry", "a. abbad-andaloussi", "a. ulges", "b. weber"], "Key Words": ["masked language models", "source code identfiers", "naming guidelines", "code quality", "identfier quality assessment", "generative approaches", "language model fine-tuning", "code transformers", "ai-based code analysis"], "Abstract": "using meaningful identifiers in source code reduces the risk of errors the cognitive load of developers and speeds up the development process. therefore recent research has looked into an artificial intelliegence based analysis of identifiers for which large scale language models appear to offer great potential. based on tokens\u201a\u00e4\u00f4 probabilities such models can suggest identifiers that are likely to appear in a given context. while current research has used language models to predict the most likely identifier names studies on assessing the quality of given identifiers are scarce. to this end we explore adherence to identifier naming guidelines as a proxy for identifier quality and propose and evaluate two unsupervised approaches for spotting violations  first a generative approach which uses the probability distribution of the language model directly without fine tuning. second a discriminative method which fine tunes the model\u201a\u00e4\u00f4s encoder to discriminate between original identifiers and similar drop in replacements suggested by a weak artificial intelliegence. we demonstrate that the proposed approaches can successfully detect violations of common guidelines for identifier naming. to do so we have developed a dataset built on widely accepted identifier naming guidelines. the manually annotated dataset contains more than 6000 dense annotations of identifiers for 28 common guidelines. using the data we show that the generative approach achieves the best results but that the particular masking strategy and scoring method matter substantially. also we demonstrate our approach to outperform other recent code transformers. in a per guideline analysis we highlight the potential and limitations of language models and provide a blue print for training and evaluating their ability to identify bad identifier names in source code. we make our dataset and models\u201a\u00e4\u00f4 implementation publicly available to encourage future research on artificial intelliegence based identifier quality assessment.", "Pub Date": "2023-12-20"}