{"Title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration", "Doi": "10.1109/ASRU57964.2023.10389705", "Authors": ["j. wu", "y. gaur", "z. chen", "l. zhou", "y. zhu", "t. wang", "j. li", "s. liu", "b. ren", "l. liu", "y. wu"], "Key Words": ["decoder-only", "llama", "lora", "speech translation"], "Abstract": "large language models  llms  have achieved remarkable success in the field of natural language processing enabling better human computer interaction using natural language. however the seamless integration of speech signals into llms has not been explored well. the \u201a\u00e4\u00fadecoder only\u201a\u00e4\u00fa architecture has also not been well studied for speech processing tasks. in this research we introduce speech llama a novel approach that effectively incorporates acoustic information into text based large language models. our method leverages connectionist temporal classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the llm. in addition we further probe the decoder only architecture for speech to text tasks by training a smaller scale randomly initialized speech llama model from speech text paired data alone. we conduct experiments on multilingual speech to text translation tasks and demonstrate a significant improvement over strong baselines highlighting the potential advantages of decoder only models for speech to text conversion.", "Pub Date": "2024-01-19"}