{"Title": "ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation", "Doi": "10.1109/WACV57701.2024.00297", "Authors": ["x. hu", "k. zhang", "l. xia", "a. chen", "j. luo", "y. sun", "k. wang", "n. qiao", "x. zeng", "m. sun", "c. -h. kuo", "r. nevatia"], "Key Words": ["algorithms", "vision + language and/or other modalities", "algorithms", "machine learning architectures", "formulations", "and algorithms"], "Abstract": "large scale pre trained vision language models  vlm  such as clip  have demonstrated noteworthy zero shot classification capability achieving 76.3% top 1 accuracy on imagenet without seeing any examples. however while applying clip to a downstream target domain the presence of visual and text domain gaps and cross modality misalignment can greatly impact the model performance. to address such challenges we propose reclip a novel source free domain adaptation method for vlms which does not require any source data or target labeled data. reclip first learns a projection space to mitigate the misaligned visual text embeddings and learns pseudo labels. then it deploys cross modality self training with the pseudo labels to update visual and text encoders refine labels and reduce domain gaps and misalignment iteratively. with extensive experiments we show that reclip outperforms all the baselines significantly and improves the average accuracy of clip from 69.83% to 74.94% on 22 image classification benchmarks.", "Pub Date": "2024-04-09"}