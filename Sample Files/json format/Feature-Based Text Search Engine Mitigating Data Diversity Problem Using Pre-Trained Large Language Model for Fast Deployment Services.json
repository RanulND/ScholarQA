{"Title": "Feature-Based Text Search Engine Mitigating Data Diversity Problem Using Pre-Trained Large Language Model for Fast Deployment Services", "Doi": "10.1109/ACCESS.2024.3373470", "Authors": ["y. jeong", "j. yang", "i. h. choi", "j. lee"], "Key Words": ["data diversity", "distilbert", "embedded", "gpt2", "kobert", "korean", "language", "local sensitive hashing", "proximity", "semantic", "wikipedia"], "Abstract": "the fairness & bias of narrow coverage of ai becomes another challenge for ai researchers. if a commercial ai trains with a biased dataset there will be severe gender or racial fairness and bias issues. since the researchers use primary language datasets to train ai the broad audience cannot be satisfied if a novel llm  large language model  ai shows a knowledge or creativity limitation on their specific spoken language. narrow coverage of the llms can lead the audience to misinterpretation and confusion if the service involves stt  speech to text . in this paper to overcome this issue of data diversity we propose the idea that the embedded extracted features have captured semantic proximity information that can be useful to mitigate diversity issues. this project focused on the korean language food dataset for stt services where a narrow trained a.i. is prone to show its limitations such as lifestyle related elements. to present our proof of concept we trained a baseline model gpt2 with the korean wikipedia dataset in 2022. then we employed distilbert and kobert for comparison. the extracted hidden state output features from each model were utilized to build feature extraction based text search engines. we used the same idea of local sensitive hashing  lsh  but effectively located a similar hash by applying transposed weights. we also present conventional classification benchmarks for performance comparison using top k measurements times for training and memory & disc consumptions. in the discussion we proposed that our idea can mitigate the diversity problem without re training the model and tokenizer.", "Pub Date": "2024-04-05"}