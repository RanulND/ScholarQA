{"Title": "Empirical Study of LLM Fine-Tuning for Text Classification in Legal Document Review", "Doi": "10.1109/BigData59044.2023.10386911", "Authors": ["f. wei", "r. keeling", "n. huber-fliflet", "j. zhang", "a. dabrowski", "j. yang", "q. mao", "h. qin"], "Key Words": ["llm", "mlm", "fine-tuning", "text classification", "large language model", "predictive modeling", "tar", "predictive coding"], "Abstract": "the increased integration of large language models  llms  across industry sectors is enabling domain experts with new text classification optimization methods. these llms are pretrained on exceedingly large amounts of data  however practitioners can perform additional training or \u201a\u00e4\u00fafine tuning\u201a\u00e4\u00f9 to improve their text classifier\u201a\u00e4\u00f4s results for their own use cases. this paper presents a series of experiments comparing a standard pretrained distilbert model and a fine tuned distilbert model both leveraged for the downstream nlp task of text classification. tuning the model using domain specific data from real world legal matters suggests fine tuning improves the performance of llm text classifiers.to evaluate the performance of text classification models using these two large language models we employed two distinct approaches that 1  score a whole document\u201a\u00e4\u00f4s text for prediction and 2  score snippets  sentence level components of a document  of text for prediction. when comparing the two approaches we found that one prediction method outperforms the other depending on the use case.", "Pub Date": "2024-01-22"}