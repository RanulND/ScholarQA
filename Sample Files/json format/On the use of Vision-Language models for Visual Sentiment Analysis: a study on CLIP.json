{"Title": "On the use of Vision-Language models for Visual Sentiment Analysis: a study on CLIP", "Doi": "10.1109/ACII59096.2023.10388075", "Authors": ["c. bustos", "c. civit", "b. du", "a. sol\u221a\u00a9-ribalta", "a. lapedriza"], "Key Words": ["visual sentiment analysis", "vision-language models", "zero-shot classification"], "Abstract": "this work presents a study on how to exploit the clip embedding space to perform visual sentiment analysis. we experiment with two architectures built on top of the clip embedding space which we denote by clip e. we train the clip e models with webemo the largest publicly available and manually labeled benchmark for visual sentiment analysis and perform two sets of experiments. first we test on webemo and compare the clip e architectures with state of the art  sota  models and with clip zero shot. second we perform cross dataset evaluation and test the clip e architectures trained with webemo on other visual sentiment analysis benchmarks. our results show that the clip e approaches outperform sota models in webemo fine grained categorization and they also generalize better when tested on datasets that have not been seen during training. interestingly we observed that for the fi dataset clip zero shot produces better accuracies than sota models and clip e trained on webemo. these results motivate several questions that we discuss in this paper such as how we should design new benchmarks and evaluate visual sentiment analysis and whether we should keep designing tailored deep learning models for visual sentiment analysis or focus our efforts on better using the knowledge encoded in large vision language models such as clip for this task. our code is available at https //github.com cristinabustos16/clip e.", "Pub Date": "2024-01-15"}