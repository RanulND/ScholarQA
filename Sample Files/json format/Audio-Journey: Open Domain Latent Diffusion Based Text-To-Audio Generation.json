{"Title": "Audio-Journey: Open Domain Latent Diffusion Based Text-To-Audio Generation", "Doi": "10.1109/ICASSP48485.2024.10448220", "Authors": ["j. michaels", "j. b. li", "l. yao", "l. yu", "z. wood-doughty", "f. metze"], "Key Words": ["deep learning", "open domain audio generation", "audio-visual training", "large language models"], "Abstract": "despite recent progress machine learning  ml  models for open domain audio generation need to catch up to generative models for image text speech and music. the lack of massive open domain audio datasets is the main reason for this performance gap  we overcome this challenge through a novel data augmentation approach. we leverage state of the art  sota  large language models  llms  to enrich captions in the weakly labeled audio dataset. we then use a sota video captioning model to generate captions for the videos from which the audio data originated and we again use llms to merge the audio and video captions to form a rich large scale dataset. we experimentally evaluate the quality of our audio visual captions showing a 12.5% gain in semantic score over baselines. using our augmented dataset we train a latent diffusion model to generate in an encodec encoding latent space. our model is novel in the current sota audio generation landscape due to our generation space text encoder noise schedule and attention mechanism. together these innovations provide competitive open domain audio generation. the samples models and implementation will be at https //audiojourney.github.io.", "Pub Date": "2024-03-18"}