{"Title": "PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10446267", "Authors": ["h. yao", "j. lou", "z. qin"], "Key Words": ["prompt learning", "backdoor attacks", "large language model"], "Abstract": "prompts have significantly improved the performance of pre trained large language models  large language model  on various downstream tasks recently making them increasingly indispensable for a diverse range of large language model application scenarios. however the backdoor vulnerability a serious security threat that can maliciously alter the victim model\u201a\u00e4\u00f4s normal predictions has not been sufficiently explored for prompt based large language model. in this paper we present poisonprompt a novel backdoor attack capable of successfully compromising both hard and soft prompt based large language model. we evaluate the effectiveness fidelity and robustness of poisonprompt through extensive experiments on three popular prompt methods using six datasets and three widely used large language model. our findings highlight the potential security threats posed by backdoor attacks on prompt based large language model and emphasize the need for further research in this area.", "Pub Date": "2024-03-18"}