{"Title": "PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10446267", "Authors": ["h. yao", "j. lou", "z. qin"], "Key Words": ["prompt learning", "backdoor attacks", "large language model"], "Abstract": "prompts have significantly improved the performance of pre trained large language models  llms  on various downstream tasks recently making them increasingly indispensable for a diverse range of llm application scenarios. however the backdoor vulnerability a serious security threat that can maliciously alter the victim model\u201a\u00e4\u00f4s normal predictions has not been sufficiently explored for prompt based llms. in this paper we present poisonprompt a novel backdoor attack capable of successfully compromising both hard and soft prompt based llms. we evaluate the effectiveness fidelity and robustness of poisonprompt through extensive experiments on three popular prompt methods using six datasets and three widely used llms. our findings highlight the potential security threats posed by backdoor attacks on prompt based llms and emphasize the need for further research in this area.", "Pub Date": "2024-03-18"}