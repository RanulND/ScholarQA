{"Title": "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model", "Doi": "10.1109/SLT54892.2023.10022954", "Authors": ["y. -j. shih", "h. -f. wang", "h. -j. chang", "l. berry", "h. -y. lee", "d. harwath"], "Key Words": ["visual grounding", "vision and language", "self-supervised learning"], "Abstract": "data driven speech processing models usually perform well with a large amount of text supervision but collecting transcribed speech data is costly. therefore we propose speech clip a novel framework bridging speech and text through images to enhance speech models without transcriptions. we leverage state of the art pre trained hubert and clip aligning them via paired images and spoken captions with minimal fine tuning. speechclip outperforms prior state of the art on image speech retrieval and performs zero shot speech text retrieval without direct supervision from transcriptions. moreover speechclip can directly retrieve semantically related keywords from speech.", "Pub Date": "2023-01-27"}