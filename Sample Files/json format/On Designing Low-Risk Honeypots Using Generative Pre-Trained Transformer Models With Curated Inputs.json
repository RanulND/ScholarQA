{"Title": "On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs", "Doi": "10.1109/ACCESS.2023.3326104", "Authors": ["j. ragsdale", "r. v. boppana"], "Key Words": ["cybersecurity", "threat engagement", "cyber deception", "honeypot", "reinforcement learning", "large language model", "chatgpt"], "Abstract": "honeypots are utilized as defensive tools within a monitored environment to engage attackers and gather artifacts for the development of indicators of compromise. however once these honeypots are deployed they are rarely updated making them obsolete and easier to fingerprint as time passes. furthermore using fully functional computing and networking devices as honeypots presents the risk of an attacker breaking out from the controlled environment. large scale text generating models commonly referred to as large language models  llms  have seen wide implementation using generative pretrained transformer  gpt  models. these models have seen an explosion in popularity and have been tuned for various use cases. this paper investigates the use of these models to simulate honeypots that are adaptive to threat engagement without the risk of unintended breakouts. this investigation finds that the method these models use to generate output has limitations that can reveal the deception to a dedicated attacker in extended sessions. to overcome this challenge this paper presents a method to manage the inputs and outputs to reduce non deterministic output and token usage of a model generating text in a way that simulates a terminal. an example honeypot is evaluated against a traditional low risk honeypot cowrie where greater similarity to an actual machine for single commands is achieved. furthermore in several multi step attack scenarios the proposed architecture reduced the token usage by up to 77% when compared to a baseline scenario that did not manage the inputs to and outputs from an example model. a discussion on the utilization of llms for cyber deception as well as the limitations hindering their broader adoption indicates that llms exhibit promise for cyber deception but necessitate further research before achieving widespread implementation.", "Pub Date": "2023-10-30"}