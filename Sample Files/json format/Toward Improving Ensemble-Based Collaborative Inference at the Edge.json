{"Title": "Toward Improving Ensemble-Based Collaborative Inference at the Edge", "Doi": "10.1109/ACCESS.2024.3351308", "Authors": ["s. kumazawa", "j. yu", "k. kawamura", "t. v. chu", "m. motomura"], "Key Words": ["ensemble", "edge computing", "collaborative inference", "neural networks", "cascade", "test time augmentation"], "Abstract": "ensemble based collaborative inference systems edge ensembles are deep learning edge inference systems that enhance accuracy by aggregating predictions from models deployed on each device. they offer several advantages including scalability based on task complexity and decentralized functionality without dependency on centralized servers. in general ensemble methods effectively improve the accuracy of deep learning and conventional research uses several model integration techniques for deep learning ensembles. some of these existing integration methods are more effective than those used in previous edge ensembles. however it remains uncertain whether these methods can be directly applied in the context of cooperative inference systems involving multiple edge devices. this study investigates the effectiveness of conventional model integration techniques including cascade weighted averaging and test time augmentation  tta  when applied to edge ensembles to enhance their performance. furthermore we propose enhancements of these techniques tailored for edge ensembles. the cascade reduces the number of models required for inference but worsens latency by sequential inference processing. to address this latency issue we propose  $m$  parallel cascade which adjusts the number of models processed simultaneously to  $m$ . we also propose learning tta policies and weights for weighted averaging using ensemble prediction labels instead of ground truth labels. in the experiments we verified the effectiveness of each technique for edge ensembles. the proposed  $m$  parallel cascade achieved a 2.8 times reduction in latency compared to the conventional cascade even with a 1.06 times increase in computational costs. additionally the ensemble label based learning demonstrated comparable effectiveness to the approach using ground truth labels.", "Pub Date": "2024-01-15"}