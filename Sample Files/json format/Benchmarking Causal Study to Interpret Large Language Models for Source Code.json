{"Title": "Benchmarking Causal Study to Interpret Large Language Models for Source Code", "Doi": "10.1109/ICSME58846.2023.00040", "Authors": ["d. rodriguez-cardenas", "d. n. palacio", "d. khati", "h. burke", "d. poshyvanyk"], "Key Words": ["software engineering", "testbeds", "large language models", "dl4se", "interpretability"], "Abstract": "one of the most common solutions adopted by software researchers to address code generation is by training large language models  large language model  on massive amounts of source code. large language model are rooted in the concept of emergent capabilities in which machines statistically learn complex patterns from code data. although a number of studies have shown that large language model have been effectively evaluated on popular accuracy metrics  e.g. bleu codebleu  previous research has largely overlooked the role of causal inference as a fundamental component of the interpretability of large language model\u201a\u00e4\u00f4 performance. existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome but do not take into account confounding variables  e.g. lines of code number of tokens prompt size  that equally influence the accuracy metrics. the fact remains that when dealing with generative software tasks by large language model no benchmark is available to tell researchers how to quantify neither the causal effect of se based treatments nor the correlation of confounders to the model\u201a\u00e4\u00f4s performance. in an effort to bring statistical rigor to the evaluation of large language model this paper introduces a benchmarking strategy named galeras comprised of curated testbeds for three se tasks  i.e. code completion code summarization and commit generation  to help aid the interpretation of large language model\u201a\u00e4\u00f4 performance.we illustrate the insights of our benchmarking strategy by conducting a case study on the performance of chatgpt under distinct prompt engineering methods. the results of the case study demonstrate the positive causal influence of prompt semantics on chatgpt\u201a\u00e4\u00f4s generative performance by an average treatment effect of \u201a\u00e2\u00e0 3%. moreover it was found that confounders such as prompt size are highly correlated with accuracy metrics  \u201a\u00e2\u00e0 0.412 . the end result of our case study is to showcase causal inference evaluations in practice to reduce confounding bias. by reducing the bias we offer an interpretable solution for the accuracy metric under analysis.", "Pub Date": "2023-12-11"}