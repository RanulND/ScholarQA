{"Title": "Design of Analog-AI Hardware Accelerators for Transformer-based Language Models (Invited)", "Doi": "10.1109/IEDM45741.2023.10413767", "Authors": ["g. w. burr", "h. tsai", "w. simon", "i. boybat", "s. ambrogio", "c. . -e. ho", "z. . -w. liou", "m. rasch", "j. b\u221a\u00bachel", "p. narayanan", "t. gordon", "s. jain", "t. m. levin", "k. hosokawa", "m. le gallo", "h. smith", "m. ishii", "y. kohda", "a. chen", "c. mackin", "a. fasoli", "k. elmaghraoui", "r. muralidhar", "a. okazaki", "c. . -t. chen", "m. m. frank", "c. lammie", "a. vasilopoulos", "a. m. friz", "j. luquin", "s. teehan", "i. ahsan", "a. sebastian", "v. narayanan"], "Key Words": ["in-memory computing", "non-volatile memory", "large language models", "analog multiply-accumulate for dnn inference", "analog ai", "deep learning accelerator", "system modeling"], "Abstract": "analog non volatile memory based accelerators offer high throughput and energy efficient multiply accumulate operations for the large fully connected layers that dominate transformer based large language models. we describe architectural wafer scale testing chip demo and hardware aware training efforts towards such accelerators and quantify the unique raw throughput and latency benefits of fully  rather than partially   weight stationary systems.", "Pub Date": "2024-02-07"}