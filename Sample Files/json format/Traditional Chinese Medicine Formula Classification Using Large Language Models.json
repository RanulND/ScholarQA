{"Title": "Traditional Chinese Medicine Formula Classification Using Large Language Models", "Doi": "10.1109/BIBM58861.2023.10385776", "Authors": ["z. wang", "k. li", "q. ren", "k. yao", "y. zhu"], "Key Words": ["large language models", "formula classification", "traditional chinese medicine", "fine-tuning for tcm", "prompt template for tcm"], "Abstract": "objective  in this study we aim to investigate the utilization of large language models  llms  for traditional chinese medicine  tcm  formula classification by fine tuning the llms and prompt template. methods  we refined and cleaned the data from the coding rules for chinese medicinal formulas and their codes  the chinese national medical insurance catalog for proprietary chinese medicines  and textbooks of formulas of chinese medicine  to address the standardization of tcm formula information and finally we extracted 2308 tcm formula data as a dataset in this study. we designed a prompt template for the tcm formula classification task and randomly divided the formula dataset into three subsets  a training set  2000 formulas  a test set  208 formulas  and a validation set  100 formulas . we fine tuned the open source llms such as chatglm 6b and chatglm2 6b. finally we evaluate all selected llms in our study  chatglm 6b  original  chatglm2 6b  original  chatglm 130b internlm 20b chatgpt chatglm 6b  fine tuned  and chatglm2 6b  fine tuned . results  the results showed that chatglm2 6b  fine tuned  and chatglm 6b  fine tuned  achieved the highest accuracy rates of 71% and 70% on the validation set respectively. the accuracy rates of other models were chatglm 130b 58% chatgpt 53% internlm 20b 52% chatglm2 6b  original  41% and chatglm 6b  original  23%. conclusion  llms achieved an impressive 71% accuracy in the formula classification task in our study. this was achieved through fine tuning and the utilization of prompt templates. and provided a novel option for the utilization of llms in the field of tcm.", "Pub Date": "2024-01-18"}