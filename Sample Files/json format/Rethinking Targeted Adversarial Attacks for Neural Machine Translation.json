{"Title": "Rethinking Targeted Adversarial Attacks for Neural Machine Translation", "Doi": "10.1109/ICASSP48485.2024.10447368", "Authors": ["j. wu", "l. liu", "w. bi", "d. -y. yeung"], "Key Words": ["targeted adversarial attack", "neural machine translation", "natural language processing", "robustness"], "Abstract": "targeted adversarial attacks are widely used to evaluate the robustness of neural machine translation systems. unfortunately this paper first identifies a critical issue in the existing settings of nmt targeted adversarial attacks where their attacking results are largely overestimated. to this end this paper presents a new setting for nmt targeted adversarial attacks that could lead to reliable attacking results. under the new setting it then proposes a targeted word gradient adversarial attack  twga  method to craft adversarial examples. experimental results demonstrate that our proposed setting could provide faithful attacking results for targeted adversarial attacks on nmt systems and the proposed twga method can effectively attack such victim nmt systems. in depth analyses on a large scale dataset further illustrate some valuable findings.1 our code and data are available at https //github.com wujunjie1998/twga.", "Pub Date": "2024-03-18"}