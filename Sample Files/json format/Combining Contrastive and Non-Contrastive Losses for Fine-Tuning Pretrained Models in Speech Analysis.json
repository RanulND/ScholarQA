{"Title": "Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis", "Doi": "10.1109/SLT54892.2023.10022897", "Authors": ["f. lux", "c. -y. chen", "n. t. vu"], "Key Words": ["speech embedding", "contrastive", "non-contrastive", "representation adjustment"], "Abstract": "embedding paralinguistic properties is a challenging task as there are only a few hours of training data available for domains such as emotional speech. one solution to this problem is to pretrain a general self supervised speech representation model on large amounts of unlabeled speech. this pretrained model is then finetuned to a specific task. paralinguistic properties however have notoriously high class variance making the finetuning ineffective. in this work we propose a two step approach to this. first we improve the embedding space then we train an adapter to bridge the gap from the embedding space to a classification task. in order to improve the class invariance we use a combination of contrastive and non contrastive losses to explicitly optimize for class invariant yet discriminative features. our approach consistently outperforms baselines that are finetuned end to end on multiple tasks and surpasses a benchmark on state of the art emotion classification.", "Pub Date": "2023-01-27"}