{"Title": "Comparison of Large Language And Vision Models on Representative Downstream Tasks", "Doi": "10.1109/ICICML60161.2023.10424913", "Authors": ["h. chen"], "Key Words": ["deep learning", "neural network", "large language model", "large vision model"], "Abstract": "the success of large model pre training in the fields of natural language processing and computer vision has been remarkable. by pre training on large scale data these models can learn richer semantic and visual representations to achieve better performance on various tasks. performance. in order to enable more researchers and practitioners to fully understand the advantages and applicable fields of different models this paper summarizes the current large model pre training methods including vision transformer  vit  pre trained transformer  gpt  text to text transfer transformer  t5  bidirectional encoder representations from transformers  bert  and contrastive language image pretraining  clip  are all included and discusses their principles and advantages are presented. at the same time compare their results are compared on different datasets and compare the results of combining them with different other techniques. the final research results show that vit has advantages over other models in image classification gpt performs best in generative tasks compared to other models and t5 has achieved significant results in various text tasks compared to other models while bert performs best in various discriminative tasks compared to other models. clip is suitable for image and text related tasks including image classification text retrieval visual question answering etc. finally applications of various models are summarized providing direction for researchers and practitioners.", "Pub Date": "2024-02-13"}