{"Title": "Leveraging Symbolic Knowledge Bases for Commonsense Natural Language Inference Using Pattern Theory", "Doi": "10.1109/TPAMI.2023.3287837", "Authors": ["s. n. aakur", "s. sarkar"], "Key Words": ["multiple choice cnli", "commonsense reasoning", "pattern theory"], "Abstract": "the commonsense natural language inference  cnli  tasks aim to select the most likely follow up statement to a contextual description of ordinary everyday events and facts. current approaches to transfer learning of cnli models across tasks require many labeled data from the new task. this paper presents a way to reduce this need for additional annotated training data from the new task by leveraging symbolic knowledge bases such as conceptnet. we formulate a teacher student framework for mixed symbolic neural reasoning with the large scale symbolic knowledge base serving as the teacher and a trained cnli model as the student. this hybrid distillation process involves two steps. the first step is a symbolic reasoning process. given a collection of unlabeled data we use an abductive reasoning framework based on grenander pattern theory to create weakly labeled data. pattern theory is an energy based graphical probabilistic framework for reasoning among random variables with varying dependency structures. in the second step the weakly labeled data along with a fraction of the labeled data is used to transfer learn the cnli model into the new task. the goal is to reduce the fraction of labeled data required. we demonstrate the efficacy of our approach by using three publicly available datasets  openbookqa swag and hellaswag  and evaluating three cnli models  bert lstm and esim  that represent different tasks. we show that on average we achieve 63% of the top performance of a fully supervised bert model with no labeled data. with only 1000 labeled samples we can improve this performance to 72%. interestingly without training the teacher mechanism itself has significant inference power. the pattern theory framework achieves 32.7% accuracy on openbookqa outperforming transformer based models such as gpt  26.6%  gpt-2  30.2%  and bert  27.1%  by a significant margin. we demonstrate that the framework can be generalized to successfully train neural cnli models using knowledge distillation under unsupervised and semi supervised learning settings. our results show that it outperforms all unsupervised and weakly supervised baselines and some early supervised approaches while offering competitive performance with fully supervised baselines. additionally we show that the abductive learning framework can be adapted for other downstream tasks such as unsupervised semantic textual similarity unsupervised sentiment classification and zero shot text classification without significant modification to the framework. finally user studies show that the generated interpretations enhance its explainability by providing key insights into its reasoning mechanism.", "Pub Date": "2023-10-03"}