{"Title": "Downstream Task-agnostic Transferable Attacks on Language-Image Pre-training Models", "Doi": "10.1109/ICME55011.2023.00481", "Authors": ["y. lv", "j. chen", "z. wei", "k. chen", "z. wu", "y. -g. jiang"], "Key Words": ["transfer-based adversarial attack", "task-agnostic", "visual-language pre-training model"], "Abstract": "vision language pre trained models  e.g. clip  trained on large scale datasets via self supervised learning are drawing increasing research attention since they can achieve superior performances on multi modal downstream tasks. nevertheless we find that the adversarial perturbations crafted on vision language pre trained models can be used to attack different corresponding downstream task models. specifically to investigate such adversarial transferability we introduce a task agnostic method named global and local augmentation  gla  attack to generate highly transferable adversarial examples on clip to attack black box downstream task models. gla adopts random crop and resize at both global and local patch levels to create more diversity and make adversarial noises robust. then gla generates the adversarial perturbations by minimizing the cosine similarity between intermediate features from augmented adversarial and benign examples. extensive experiments on three clip image encoders with different backbones and three different downstream tasks demonstrate the superiority of our method compared with other strong baselines. the code is available at https //github.com yqlvcoding/glaattack.", "Pub Date": "2023-08-25"}