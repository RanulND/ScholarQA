{"Title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts", "Doi": "10.1109/ICCVW60793.2023.00034", "Authors": ["m. maniparambil", "c. vorster", "d. molloy", "n. murphy", "k. mcguinness", "n. e. o\u201a\u00e4\u00f4connor"], "Key Words": ["foundational models", "clip adapter", "gpt 4", "prompt"], "Abstract": "contrastive pretrained large vision language models  vlms  like clip have revolutionized visual representation learning by providing good performance on downstream datasets. vlms are 0 shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. such prompt engineering makes use of domain expertise and a validation dataset. meanwhile recent developments in generative pretrained models like gpt-4 mean they can be used as advanced internet search tools. they can also be manipulated to provide visual information in any structure. in this work we show that gpt-4 can be used to generate text that is visually descriptive and how this can be used to adapt clip to downstream tasks. we show considerable improvements in 0 shot transfer accuracy on specialized fine grained datasets like eurosat  ~7%  dtd  ~ 7%  sun397  ~ 4.6%  and cub   ~3.3%  when compared to clip\u201a\u00e4\u00f4s default prompt. we also design a simple few shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed cocoop by ~2% on average and by over 4% on 4 specialized fine grained datasets. the code prompts and auxiliary text dataset is available at github.com mayug/vdt adapter.", "Pub Date": "2023-12-25"}