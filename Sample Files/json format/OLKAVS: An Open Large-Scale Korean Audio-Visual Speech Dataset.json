{"Title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset", "Doi": "10.1109/ICASSP48485.2024.10446901", "Authors": ["j. park", "j. -w. hwang", "k. choi", "s. -h. lee", "j. h. ahn", "r. -h. park", "h. -m. park"], "Key Words": ["audio-visual speech datasets", "multi-view datasets", "lip reading", "audio-visual speech recognition", "eep learning"], "Abstract": "inspired by humans comprehending speech in a multi modal manner various audio visual datasets have been constructed. however most existing datasets focus on english developed from pre existing videos using various prediction models and have only a small number of multi view videos. to mitigate the limitations we constructed the open large scale korean audio visual speech  olkavs  dataset which is the largest among publicly available audio visual speech datasets. the dataset contains 1150 hours of transcribed audio from 1107 korean speakers in a studio setup with nine different viewpoints and various noise situations. we also provide the pre trained baseline models for two tasks  audiovisual speech recognition and lip reading. we conducted experiments based on the models to verify the effectiveness of multi modal and multi view training over uni modal and frontal view only training. we expect the olkavs dataset to facilitate multi modal research in broader areas.", "Pub Date": "2024-03-18"}