{"Title": "Enriching Source Code with Contextual Data for Code Completion Models: An Empirical Study", "Doi": "10.1109/MSR59073.2023.00035", "Authors": ["t. van dam", "m. izadi", "a. van deursen"], "Key Words": ["code completion", "transformers", "pre-trained language models", "context", "empirical software engineering"], "Abstract": "transformer based pre trained models have recently achieved great results in solving many software engineering tasks including automatic code completion which is a staple in a developer\u201a\u00e4\u00f4s toolkit. while many have striven to improve the code understanding abilities of such models the opposite \u201a\u00e4\u00ec making the code easier to understand \u201a\u00e4\u00ec has not been properly investigated. in this study we aim to answer whether making code easier to understand through using contextual data improves the performance of pre trained code language models for the task of code completion. we consider type annotations and comments as two common forms of additional contextual information that often help developers understand code better. for the experiments we study code completion in two granularity levels  token and line completion and take three recent and large scale language models for source code  unixcoder codegpt and incoder with five evaluation metrics. finally we perform the wilcoxon signed rank test to gauge significance and measure the effect size. contrary to our expectations all models perform better if type annotations are removed  albeit the effect sizes are small . for comments we find that the models perform better in the presence of multi line comments  again with small effect sizes . based on our observations we recommend making proper design choices when training fine tuning or simply selecting such models given the intended data and application. better evaluations and multimodal techniques can also be further investigated to improve the practicality and accuracy of auto completions.", "Pub Date": "2023-07-12"}