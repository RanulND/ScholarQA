{"Title": "Privacy-Preserving Offloading in Edge Intelligence Systems With Inductive Learning and Local Differential Privacy", "Doi": "10.1109/TNSM.2023.3266257", "Authors": ["j. tchaye-kondi", "y. zhai", "j. shen", "l. zhu"], "Key Words": ["edge intelligence", "local differential privacy", "internet of things", "cloud computing", "deep learning"], "Abstract": "we address privacy and latency issues in edge cloud computing environments where the neural network training is centralized. this paper considers the scenario where the edge devices are the only data sources for the deep learning model to be trained on the central server. improper access to the massive amounts of data generated by edge devices could lead to privacy concerns. as a result existing solutions for preserving privacy and reducing network latency in the edge environment rely on auxiliary datasets with no privacy risks or pre trained models to build the client side feature extractor. however finding auxiliary datasets or pre trained models is not always guaranteed and may be challenging. to bridge this gap and eliminate the reliance on auxiliary datasets or pre trained models of existing solutions this paper presents deepguess a privacy preserving and latency aware deep learning framework. deepguess introduces a new learning mechanism enabled by the autoencoder architecture  inductive learning. with inductive learning sensitive data stays on devices and is not explicitly sent to the central server to engage in back propagations. to further enhance privacy we propose a new local differential privacy algorithm that allows edge devices to apply random noise to features extracted from their sensitive data before being transferred to the non trusted central server. the experimental evaluation of deepguess with various datasets and in a real world scenario shows that our solution achieves comparable or even higher accuracy than existing solutions while reducing data transfer over the network by more than 50%.", "Pub Date": "2023-12-12"}