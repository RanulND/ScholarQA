{"Title": "Cross-Modal Language Modeling in Multi-Motion-Informed Context for Lip Reading", "Doi": "10.1109/TASLP.2023.3282109", "Authors": ["x. ai", "b. fang"], "Key Words": ["cross-modal language modeling", "lip reading", "pre-training"], "Abstract": "we observe that for lip reading the language is locally transformed instead of globally transformed i.e. speaking and writing follow the same basic grammar rules. in this work we present a cross modal language model to tackle the lip reading challenge on silent videos. compared to previous works we consider multi motion informed contexts composed of multiple lip motion representations from different subspaces to guide decoding via the source target attention mechanism. we present a piece wise pre training strategy inspired by multi task learning to pre train a visual module to generate multi motion informed contexts for cross modality and pre train a decoder to generate texts for language modeling. our final large scale model outperforms baseline models on four datasets  lrs2 lrs3 lrw and grid. we will open our source code on github.", "Pub Date": "2023-06-16"}