{"Title": "Exploring Parameter-Efficient Fine-Tuning of a Large-Scale Pre-Trained Model for scRNA-seq Cell Type Annotation", "Doi": "10.1109/BIBM58861.2023.10385599", "Authors": ["y. liu", "t. li", "z. wang", "g. zhu", "y. zhang", "q. zou"], "Key Words": ["single-cell rna-seq", "cell type annotation", "parameter-efficient fine-tuning", "pre-trained language model"], "Abstract": "accurate identification of cell types is a pivotal and intricate task in scrna seq data analysis. recently significant strides have been made in cell type annotation of scrna seq data using pre trained language models  plms . this method has surmounted the constraints of conventional approaches regarding precision robustness and generalization. however the fine tuning process of large scale pre trained models incurs substantial computational expenses. to tackle this issue a promising avenue of research has emerged proposing parameter efficient fine tuning techniques for plms. these techniques concentrate on fine tuning only a small portion of the model parameters while attaining comparable performance. in this study we extensively research parameter efficient fine tuning methods for scrna seq cell type annotation employing scbert as the backbone. we scrutinize the performance and compatibility of various parameter efficient fine tuning methodologies across multiple datasets. through comprehensive analysis we demonstrate the remarkable performance of parameter efficient fine tuning methods in cell type annotation. hopefully this study can inspire new thinking in analyzing scrna seq data.", "Pub Date": "2024-01-18"}