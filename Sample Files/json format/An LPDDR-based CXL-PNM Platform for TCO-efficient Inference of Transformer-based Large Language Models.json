{"Title": "An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models", "Doi": "10.1109/HPCA57654.2024.00078", "Authors": ["s. -s. park", "k. kim", "j. so", "j. jung", "j. lee", "k. woo", "n. kim", "y. lee", "h. kim", "y. kwon", "j. kim", "j. lee", "y. cho", "y. tai", "j. cho", "h. song", "j. h. ahn", "n. s. kim"], "Key Words": ["cxl-pnm", "llm", "lpddr", "cxl"], "Abstract": "transformer based large language models  llms  such as generative pre trained transformer  gpt  have become popular due to their remarkable performance across diverse applications including text generation and translation. for llm training and inference the gpu has been the predominant accelerator with its pervasive software development ecosystem and powerful computing capability. however as the size of llms keeps increasing for higher performance and or more complex applications a single gpu cannot efficiently accelerate llm training and inference due to its limited memory capacity which demands frequent transfers of the model parameters needed by the gpu to compute the current layer s  from the host cpu memory storage. a gpu appliance may provide enough aggregated memory capacity with multiple gpus but it suffers from frequent transfers of intermediate values among gpu devices each accelerating specific layers of a given llm. as the frequent transfers of these model parameters and intermediate values are performed over relatively slow device to device interconnects such as pcie or nvlink they become the key bottleneck for efficient acceleration of llms. focusing on accelerating llm inference which is essential for many commercial services we develop cxl pnm a processing near memory  pnm  platform based on the emerging interconnect technology compute express link  cxl . specifically we first devise an lpddr5x based cxl memory architecture with 512gb of capacity and 1.1tb s of bandwidth which boasts 16\u221a\u00f3 larger capacity and 10\u221a\u00f3 higher bandwidth than gddr6and ddr5 based cxl memory architectures respectively under a module form factor constraint. second we design a cxlpnm controller architecture integrated with an llm inference accelerator exploiting the unique capabilities of such cxl memory to overcome the disadvantages of competing technologies such as hbm pim and axdimm. lastly we implement a cxlpnm software stack that supports seamless and transparent use of cxl pnm for python based llm programs. our evaluation shows that a cxl pnm appliance with 8 cxl pnm devices offers 23% lower latency 31% higher throughput and 2.8\u221a\u00f3 higher energy efficiency at 30% lower hardware cost than a gpu appliance with 8 gpu devices for an llm inference service.", "Pub Date": "2024-04-02"}