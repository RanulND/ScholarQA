{"Title": "Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation", "Doi": "10.1109/ICACS60934.2024.10473289", "Authors": ["l. jayawardena", "p. yapa"], "Key Words": ["paraphrase generation", "natural language processing", "knowledge distillation", "deep learning", "large language models"], "Abstract": "over the past year the field of natural language generation  nlg  has experienced an exponential surge largely due to the introduction of large language models  llms . these models have exhibited the most effective performance in a range of domains within the natural language processing and generation domains. however their application in domain specific tasks such as paraphrasing presents significant challenges. the extensive number of parameters makes them difficult to operate on commercial hardware and they require substantial time for inference leading to high costs in a production setting. in this study we tackle these obstacles by employing llms to develop three distinct models for the paraphrasing field applying a method referred to as sequence level knowledge distillation. these distilled models are capable of maintaining the quality of paraphrases generated by the llm. they demonstrate faster inference times and the ability to generate diverse paraphrases of comparable quality. a notable characteristic of these models is their ability to exhibit syntactic diversity while also preserving lexical diversity features previously uncommon due to existing data quality issues in datasets and not typically observed in neural based approaches. human evaluation of our models shows that there is only a 4% drop in performance compared to the llm teacher model used in the distillation process despite being 1000 times smaller. this research provides a significant contribution to the nlg field offering a more efficient and cost effective solution for paraphrasing tasks.", "Pub Date": "2024-03-21"}