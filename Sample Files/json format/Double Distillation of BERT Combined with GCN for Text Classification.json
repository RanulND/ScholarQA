{"Title": "Double Distillation of BERT Combined with GCN for Text Classification", "Doi": "10.1109/ICMLC58545.2023.10328013", "Authors": ["j. -h. wang", "j. -y. zhao", "c. feng"], "Key Words": ["natural language processing", "bert", "graph convolutional network", "knowledge distillation", "text classification"], "Abstract": "currently the bert model in the field of natural language processing has better performance for natural language processing tasks compared to other models. however the bert model is limited by the resources of external devices when processing large scale data sets. in this paper we propose a combined gcn and bert which using knowledge distillation technique to optimize the difficulties of bert model to handle large scale data sets limited by resources. the approach combines the advantages of bert models and graph convolutional neural networks incorporating the advantages of large scale data training and conductive learning to perform natural language processing tasks. the model in this paper compresses the bert model in the pre training phase and then fuses it with gcn. the model reduces the model complexity in the pre training phase and optimizes the problem of resource constraints when dealing with large scale data. in this paper four public benchmark datasets are used to test the model. the model accuracy is higher than the original baseline model.", "Pub Date": "2023-11-28"}