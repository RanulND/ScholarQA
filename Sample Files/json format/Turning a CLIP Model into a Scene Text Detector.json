{"Title": "Turning a CLIP Model into a Scene Text Detector", "Doi": "10.1109/CVPR52729.2023.00674", "Authors": ["w. yu", "y. liu", "w. hua", "d. jiang", "b. ren", "x. bai"], "Key Words": ["document analysis and understanding"], "Abstract": "the recent large scale contrastive language image pretraining  clip  model has shown great potential in various downstream tasks via leveraging the pretrained vision and language knowledge. scene text which contains rich textual and visual information has an inherent connection with a model like clip. recently pretraining approaches based on vision language models have made effective progresses in the field of text detection. in contrast to these works this paper proposes a new method termed tcm focusing on turning the clip model directly for text detection without pretraining process. we demonstrate the advantages of the proposed tcm as follows   1  the underlying principle of our framework can be applied to improve existing scene text detector.  2  it facilitates the few shot training capability of existing methods e.g. by using 10% of labeled data we significantly improve the performance of the baseline method with an average of 22% in terms of the f measure on 4 benchmarks.  3  by turning the clip model into existing scene text detection methods we further achieve promising domain adaptation ability. the code will be publicly released at https //github.com wenwenyu/tcm.", "Pub Date": "2023-08-22"}