{"Title": "Learning Meta Soft Prompt for Few-Shot Language Models", "Doi": "10.1109/APSIPAASC58517.2023.10317500", "Authors": ["j. -t. chien", "m. -y. chen", "j. -h. xue"], "Key Words": ["meta learning", "few-shot learning", "prompt tuning", "domain adaptation", "language model"], "Abstract": "prompt based learning is powerful to utilize the large scaled pre trained language model  plm  for language understanding where the input sentences are augmented by either adding the hard prompt using word tokens or the soft prompt in a form of trainable tokens. however the learned soft prompt in training domain may not really help a frozen plm to handle domain shift in test domain. this paper presents an approach to incorporate meta learning into domain adaptation to train new soft prompt which sufficiently generalizes the frozen plm to a number of domains. the meta soft prompt is then developed for few shot unsupervised domain adaptation where a frozen plm can be quickly adapted to a target domain. this soft prompt is optimized according to meta learning where the domain adaptation loss and the prompt based classification loss are jointly minimized. the experiments on multi domain natural language understanding show the benefits of the proposed meta soft prompt in pre trained language model by using bert under the few shot setting.", "Pub Date": "2023-11-20"}