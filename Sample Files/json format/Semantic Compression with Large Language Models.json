{"Title": "Semantic Compression with Large Language Models", "Doi": "10.1109/SNAMS60348.2023.10375400", "Authors": ["h. gilbert", "m. sandborn", "d. c. schmidt", "j. spencer-smith", "j. white"], "Key Words": ["large language models", "data compression", "prompt engineering", "text generation"], "Abstract": "the rise of large language models  llms  is revolutionizing information retrieval question answering summarization and code generation tasks. however in addition to confidently presenting factually inaccurate information at times  known as \u201a\u00e4\u00fahallucinations\u201a\u00e4\u00f9  llms are also inherently limited by the number of input and output tokens that can be processed at once making them potentially less effective on tasks that require processing a large set or continuous stream of information. a common approach to reducing the size of data is through lossless or lossy compression. yet in some cases it may not be strictly necessary to perfectly recover every detail from the original data as long as a requisite level of semantic precision or intent is conveyed. this paper presents three contributions to research on llms. first we present the results from experiments exploring the viability of \u201a\u00e4\u00faapproximate compression\u201a\u00e4\u00f9 using llms focusing specifically on gpt 3.5 and gpt-4 via chatgpt interfaces. second we investigate and quantify the capability of llms to compress text. third we present two novel metrics exact reconstructive effectiveness  ere  and semantic reconstruction effectiveness  sre  that quantify the level of preserved intent between text compressed and decompressed by the llms we studied. our initial results indicate that gpt-4 can effectively compress and reconstruct text while preserving the semantic essence of the original text providing a path to leverage more tokens than current limits allow.", "Pub Date": "2024-01-02"}