{"Title": "Semantic Compression with Large Language Models", "Doi": "10.1109/SNAMS60348.2023.10375400", "Authors": ["h. gilbert", "m. sandborn", "d. c. schmidt", "j. spencer-smith", "j. white"], "Key Words": ["large language models", "data compression", "prompt engineering", "text generation"], "Abstract": "the rise of large language models  large language model  is revolutionizing information retrieval question answering summarization and code generation tasks. however in addition to confidently presenting factually inaccurate information at times  known as \u201a\u00e4\u00fahallucinations\u201a\u00e4\u00f9  large language model are also inherently limited by the number of input and output tokens that can be processed at once making them potentially less effective on tasks that require processing a large set or continuous stream of information. a common approach to reducing the size of data is through lossless or lossy compression. yet in some cases it may not be strictly necessary to perfectly recover every detail from the original data as long as a requisite level of semantic precision or intent is conveyed. this paper presents three contributions to research on large language model. first we present the results from experiments exploring the viability of \u201a\u00e4\u00faapproximate compression\u201a\u00e4\u00f9 using large language model focusing specifically on gpt 3.5 and gpt-4 via chatgpt interfaces. second we investigate and quantify the capability of large language model to compress text. third we present two novel metrics exact reconstructive effectiveness  ere  and semantic reconstruction effectiveness  sre  that quantify the level of preserved intent between text compressed and decompressed by the large language model we studied. our initial results indicate that gpt-4 can effectively compress and reconstruct text while preserving the semantic essence of the original text providing a path to leverage more tokens than current limits allow.", "Pub Date": "2024-01-02"}