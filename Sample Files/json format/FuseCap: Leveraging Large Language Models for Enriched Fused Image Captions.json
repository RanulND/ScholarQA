{"Title": "FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions", "Doi": "10.1109/WACV57701.2024.00559", "Authors": ["n. rotstein", "d. bensa\u221a\u00f8d", "s. brody", "r. ganz", "r. kimmel"], "Key Words": ["algorithms", "vision + language and/or other modalities", "algorithms", "datasets and evaluations", "algorithms", "image recognition and understanding"], "Abstract": "the advent of vision language pre training techniques enhanced substantial progress in the development of models for image captioning. however these models frequently produce generic captions and may omit semantically important image details. this limitation can be traced back to the image text datasets  while their captions typically offer a general description of image content they frequently omit salient details. considering the magnitude of these datasets manual reannotation is impractical emphasizing the need for an automated approach. to address this challenge we leverage existing captions and explore augmenting them with visual details using \"frozen\" vision experts including an object detector an attribute recognizer and an optical character recognizer  ocr . our proposed method fusecap fuses the outputs of such vision experts with the original captions using a large language model  large language model  yielding comprehensive image descriptions. we automatically curate a training set of 12m image enriched caption pairs. these pairs undergo extensive evaluation through both quantitative and qualitative analyses. subsequently this data is utilized to train a captioning generation blip based model. this model outperforms current state of the art approaches producing more precise and detailed descriptions demonstrating the effectiveness of the proposed data centric approach. we release this large scale dataset of enriched image caption pairs for the community.", "Pub Date": "2024-04-09"}