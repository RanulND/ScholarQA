{"Title": "Simultaneously Training and Compressing Vision-and-Language Pre-Training Model", "Doi": "10.1109/TMM.2022.3233258", "Authors": ["q. qi", "a. zhang", "y. liao", "w. sun", "y. wang", "x. li", "s. liu"], "Key Words": ["vision-and-language", "model compression", "pre-training model"], "Abstract": "model compression is an essential step for large scale pre training models toward practical application and deployment on the edge device. however when conventional compression methods following \u201a\u00e4\u00f2pre training then compressing\u201a\u00e4\u00f4 two phase pipeline are applied to vision and language pre training  vlp  models it will lead to a high calculation and memory overhead. in this work we break the two phase pipeline and propose an efficient and effective one phase vlp model compression mechanism named reducer which stands for \u201a\u00e4\u00f2simultaneously training and compressing\u201a\u00e4\u00f4 vlp model via progressive module replacing and network rewiring. specifically reducer consists of three insightful designs. firstly we design a one phase compression framework to train and compress the vlp model simultaneously to avoid the extra calculation and memory cost caused by an isolated model compression phase in the conventional two phase pipeline. secondly we propose an adaptive progressive module replacing mechanism to compress the model depth free from explicit knowledge distillation losses relieving the multi task optimization problems. thirdly we integrate pruning techniques into vlp model compression to simultaneously compress the model in width and depth. overall we obtain a lightweight vlp model with only one pre training phase and it is the first one phase compression method for vlp models. extensive experiments have been conducted on representative vlp models i.e. clipbert and victor and the experimental results show a superior trade off between performance and efficiency.", "Pub Date": "2023-12-12"}