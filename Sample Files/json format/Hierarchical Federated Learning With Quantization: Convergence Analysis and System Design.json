{"Title": "Hierarchical Federated Learning With Quantization: Convergence Analysis and System Design", "Doi": "10.1109/TWC.2022.3190512", "Authors": ["l. liu", "j. zhang", "s. song", "k. b. letaief"], "Key Words": ["federated learning", "convergence analysis", "edge learning"], "Abstract": "federated learning  fl  is a powerful distributed machine learning framework where a server aggregates models trained by different clients without accessing their private data. hierarchical fl with a client edge cloud aggregation hierarchy can effectively leverage both the cloud server\u201a\u00e4\u00f4s access to many clients\u201a\u00e4\u00f4 data and the edge servers\u201a\u00e4\u00f4 closeness to the clients to achieve a high communication efficiency. neural network quantization can further reduce the communication overhead during model uploading. to fully exploit the advantages of hierarchical fl an accurate convergence analysis with respect to the key system parameters is needed. unfortunately existing analysis is loose and does not consider model quantization. in this paper we derive a tighter convergence bound for hierarchical fl with quantization. the convergence result leads to practical guidelines for important design problems such as the client edge aggregation and edge client association strategies. based on the obtained analytical results we optimize the two aggregation intervals and show that the client edge aggregation interval should slowly decay while the edge cloud aggregation interval needs to adapt to the ratio of the client edge and edge cloud propagation delay. simulation results shall verify the design guidelines and demonstrate the effectiveness of the proposed aggregation strategy.", "Pub Date": "2023-01-06"}