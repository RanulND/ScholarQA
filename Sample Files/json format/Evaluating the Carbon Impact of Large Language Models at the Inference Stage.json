{"Title": "Evaluating the Carbon Impact of Large Language Models at the Inference Stage", "Doi": "10.1109/IPCCC59175.2023.10253886", "Authors": ["b. everman", "t. villwock", "d. chen", "n. soto", "o. zhang", "z. zong"], "Key Words": ["energy efficiency", "large language models", "gpt", "software carbon intensity", "carbon footprint"], "Abstract": "large language models  large language model  such as gpt 3 chatgpt and gpt 4 have demonstrated enormous potential across a range of tasks and attracted over 100 million users globally in recent months. however these large language model are resource intensive and contribute significantly to carbon emissions. currently our understanding of their carbon impact remains insufficient due to the lack of reliable measurement tools standard methodologies and evaluation metrics. to bridge this gap this paper conducts a thorough study on the carbon impact of various open source large language model including gpt j 6b gpt neo 2.7b gptneo 1.3b and gpt-2 at the inference stage utilizing the software carbon intensity  sci  specification released by the green software foundation. the primary contributions of our research are   1  we propose a quantitative framework that measures and contrasts the environmental impacts of different large language model   2  we illustrate that high carbon large language model do not necessarily provide superior model quality than their low carbon counterparts  and  3  we find that the carbon emissions are primarily driven by embodied carbon in large language model and that employing gpus as opposed to cpus can substantially reduce carbon emissions.", "Pub Date": "2023-10-18"}