{"Title": "Guided Transformer for Machine Translation: English to Hindi", "Doi": "10.1109/INDICON59947.2023.10440876", "Authors": ["a. bisht", "d. gupta", "s. parida"], "Key Words": ["machine translation", "transformers", "encoder", "decoder", "self-attention", "multi head attention", "positional encoding", "blue-score", "english-hindi translation", "explicit knowledge", "dependency graph", "dependency parser"], "Abstract": "in the area of machine translation the objective is to develop a model which can translate a piece of text from one language to another language. large sized neural networks requires a large amount of training data to achieve a good performance level. the objective of this study is to build an explicit knowledge from the dependency parser which can be integrated into transformer architecture. adding an explicit knowledge to a neural machine translation network can enhance its learning process. the baseline transformer model and the proposed guided transformer model were tested on the same test data set of pmindia man ki baat session. with an addition of explicit knowledge the proposed guided transformer model surpass the performance of baseline transformer model by +1.0 bleu  approx.  +1.0 sacrebleu  approx.  +1.48 chrf +1.44 chrf+ and +1.39 chrf++.", "Pub Date": "2024-02-27"}