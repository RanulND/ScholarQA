{"Title": "Racial Skew in Fine-Tuned Legal AI Language Models", "Doi": "10.1109/ICDMW60847.2023.00037", "Authors": ["v. q. malic", "a. kumari", "x. liu"], "Key Words": ["legal ai", "large language models", "case law", "bias", "fairness"], "Abstract": "rapid growth in the application of large language models to an immense variety of use case scenarios has occurred alongside increasing concern that such models learn encode and perpetuate harmful social biases. this concern merits particular scrutiny in legal contexts which involve search research and reasoning tasks that stand to benefit greatly from large language model but also demand adherence to rigorous standards of fairness and equality before the law. in this paper we operationalize legal equality in the u.s. legal context as a \"blind\" language model that when prompted to select a race to fill in a blank assigns equal probability to all choices. we then fine tune a pretrained gpt-2 model on multiple subsets of american case law texts accounting for time and the political lean of each court\u201a\u00e4\u00f4s host state. we identify and measure the degree to which these models are unfair deviating from legal equality by learning to associate different races with different legal contexts.", "Pub Date": "2024-02-06"}