{"Title": "Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems", "Doi": "10.1109/BigData59044.2023.10386182", "Authors": ["c. kosten", "p. cudr\u221a\u00a9-mauroux", "k. stockinger"], "Key Words": ["benchmark for question answering over knowledge graphs", "language models", "performance evaluation"], "Abstract": "with the recent spike in the number and availability of large language models  llms  it has become increasingly important to provide large and realistic benchmarks for evaluating knowledge graph question answering  kgqa  systems. so far the majority of benchmarks rely on pattern based sparql query generation approaches. the subsequent natural language  nl  question generation is conducted through crowdsourcing or other automated methods such as rule based paraphrasing or nl question templates. although some of these datasets are of considerable size their pitfall lies in their pattern based generation approaches which do not always generalize well to the vague and linguistically diverse questions asked by humans in real world contexts. in this paper we introduce spider4sparql  a new sparql benchmark dataset featuring 9693 previously existing manually generated nl questions and 4721 unique novel and complex sparql queries of varying complexity. in addition to the nl sparql pairs we also provide their corresponding 166 knowledge graphs and ontologies which cover 138 different domains. our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern kgqa systems. we evaluate the system with state of the art kgqa systems as well as llms which achieve only up to 45% execution accuracy demonstrating that spider4sparql is a challenging benchmark for future research.", "Pub Date": "2024-01-22"}