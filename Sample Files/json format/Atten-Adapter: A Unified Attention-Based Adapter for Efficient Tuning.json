{"Title": "Atten-Adapter: A Unified Attention-Based Adapter for Efficient Tuning", "Doi": "10.1109/ICIP49359.2023.10223170", "Authors": ["k. li", "w. gu", "m. xue", "j. xiao", "d. shi", "x. wei"], "Key Words": ["parameter-efficient tuning", "attention", "adapter", "prompt", "large pre-trained model"], "Abstract": "recently more and more large pre trained models have emerged. several parameter efficient tuning methods have been studied to transfer the prior knowledge of the pre trained models to specific downstream tasks and achieve promising results. this paper proposes a simple yet effective method called atten adapter. to the best of our knowledge this is the first work that utilizes attention with learnable parameters as the internal structure of the adapter in the field of fine tuning. the attention based adapter can provide better information fusion ability and pay more attention to the global features compared to the mlp based adapter. as a plug and play module atten adapter can be easily adapted to different types of vision models such as convnets and transformer architectures in different tasks like classification and segmentation. moreover we demonstrate the generality of our proposed adapters by conducting experiments on language models. with small amounts of tunable parameters our method achieves significant improvements compared to the previous state of the art methods.", "Pub Date": "2023-09-11"}