{"Title": "Reinforcement Learning Based Energy-Efficient Collaborative Inference for Mobile Edge Computing", "Doi": "10.1109/TCOMM.2022.3229033", "Authors": ["y. xiao", "l. xiao", "k. wan", "h. yang", "y. zhang", "y. wu", "y. zhang"], "Key Words": ["collaborative inference", "computation partition", "mobile edge computing", "multi-agent reinforcement learning"], "Abstract": "collaborative inference in mobile edge computing  mec  enables mobile devices to offload the computation tasks for the computation intensive perception services and the inference policy determines the inference latency and energy consumption. the optimal inference policy depends on the inference performance model of deep learning the data generation model and the network model that are rarely known by mobile devices in time. in this paper we propose a multi agent reinforcement learning  rl  based energy efficient mec collaborative inference scheme which enables each mobile device to choose both the partition point of deep learning and the collaborative edge of each mobile device based on the image quantity the channel conditions and the previous inference performance. a learning experience exchange mechanism exploits the q values of the neighboring mobile devices to accelerate the inference policy optimization with less energy consumption. we also provide a deep multi agent rl based inference scheme to accelerate learning for large scale mec networks in which an actor network yields the collaborative inference policy probability distribution and a critic network guides the weight update of the actor network to enhance sample efficiency. we provide the inference performance bound and analyze the computational complexity. both simulation and experimental results show that our proposed schemes reduce the inference latency and save the mec energy consumption.", "Pub Date": "2023-02-15"}