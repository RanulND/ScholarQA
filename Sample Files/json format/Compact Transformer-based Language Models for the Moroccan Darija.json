{"Title": "Compact Transformer-based Language Models for the Moroccan Darija", "Doi": "10.1109/CiSt56084.2023.10409912", "Authors": ["m. aghzal", "m. a. e. bouni", "s. driouech", "a. mourhir"], "Key Words": ["darija", "language modeling", "deep learning", "low resource language", "transformer"], "Abstract": "over the past few years pre trained language models based on transformer architectures revolutionized the field of natural language processing achieving state of the art performance on various tasks. however due to these models\u201a\u00e4\u00f4 dependability on enormous corpora for training very few such models were trained for underresourced languages and dialects such as moroccan darija. in this work we introduce darroberta and darelectra which are two transformer based language models for darija. we evaluate the language models on the extrinsic tasks of text summarization and topic classification. as for the text summarization task the results show that darelectra achieves state of the art results with a score of 19.25 for rouge 1 5.79 for rouge 2 and 18.01 for rouge l. for the topic classification task darroberta achieved an f1 score of 0.84 and accuracy of 0.86. while our daija language models achieved results close the arabic language models they are much smaller and more efficient.", "Pub Date": "2024-02-05"}