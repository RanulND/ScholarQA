{"Title": "POP-FL: Towards Efficient Federated Learning on Edge Using Parallel Over-Parameterization", "Doi": "10.1109/TSC.2024.3376194", "Authors": ["x. lu", "h. zheng", "w. liu", "y. jiang", "h. wu"], "Key Words": ["distributed machine learning", "edge computing", "federated learning", "model sparse training"], "Abstract": "federated learning  fl  is a promising paradigm for mining massive data while respecting users\u201a\u00e4\u00f4 privacy. however the deployment of fl on resource constrained edge devices remains elusive due to its high resource demand. in this paper unlike existing works that use expensive dense models we propose to utilize dynamic sparse training in fl and design a novel sparse to sparse fl framework named as pop fl. the framework can reduce both computation and communication overheads while maintaining the performance of the global model. specifically pop fl partitions massive clients into groups and performs parallel parameter exploration i.e. parallel over parameterization over the collaboration between these groups. this exploration can greatly improve the expressibility and generalizability of sparse training in fl  especially for extreme sparsity levels  through reliably covering sufficient parameters and dynamically updating the global sparse network structure during the training process. experimental results show that compared with existing sparse to sparse training methods in both iid and non iid data distribution pop fl achieves the best inference accuracy on various representative networks.", "Pub Date": "2024-04-09"}