{"Title": "Improving Multilingual and Code-Switching ASR Using Large Language Model Generated Text", "Doi": "10.1109/ASRU57964.2023.10389644", "Authors": ["k. hu", "t. n. sainath", "b. li", "y. zhang", "y. cheng", "t. wang", "y. zhang", "f. liu"], "Key Words": ["text injection", "large language model", "prompt tuning", "multilingual", "code-switching"], "Abstract": "we investigate using large language models  llms  to generate text only training data for improving multilingual and code switching automatic speech recognition  asr  through a text injection method. in a multilingual setup or a low resource scenario such as code switching we propose to generate text data using the state of the art palm 2. to better match the generated text data with specific tasks we use prompt tuning to adapt palm 2 to generate domain relevant multilingual or code switched text data for text injection. we can achieve significant improvements in word error rate  wer  in both multilingual and code switching scenarios. the multilingual experiment shows a $6.2 \\%$ relative wer reduction on average i.e. from $11.25 \\%$ to $10.55 \\%$ compared to a baseline without text injection. the improvement is up to $23.1 \\%$ improvement for certain languages. while in the code switching scenario we use english only prompts to generate mandarin english code switching text and achieve a 3.6% relative wer reduction for a code switching test set as well as wer reductions in both english and mandarin monolingual scenarios $5.3 \\%$ and $8.5 \\%$ relative respectively. our findings demonstrate that leveraging llms for text generation and then injection benefits multilingual or code switching asr tasks.", "Pub Date": "2024-01-19"}