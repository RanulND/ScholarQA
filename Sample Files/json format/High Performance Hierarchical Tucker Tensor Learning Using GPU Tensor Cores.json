{"Title": "High Performance Hierarchical Tucker Tensor Learning Using GPU Tensor Cores", "Authors": ["h. huang", "x. -y. liu", "w. tong", "t. zhang", "a. walid", "x. wang"], "Pub Date": "2023-01-18", "Abstract": "extracting information from large scale high dimensional data is a fundamentally important task in high performance computing where the hierarchical tucker  ht  tensor learning approach  learning a tensor tree structure  has been widely used in many applications. however ht tensor learning algorithms are compute intensive due to the \u201a\u00e4\u00facurse of dimensionality\u201a\u00e4\u00f9 i.e. the time complexity grows exponentially with the order of the data tensor. the computation of ht tensor learning algorithms boils down to tensor primitives which are amenable to computing on gpu tensor cores. existing work does not support ht tensor learning using gpu tensor cores. there are three main challenges to address  1  to accelerate tensor learning primitives using gpu tensor cores  2  to implement the tensor learning algorithms using gpu tensor cores and multiple gpus  3  to support large scale data tensors exceeding the gpu memory capacity. in this paper we present efficient ht tensor learning primitives using gpu tensor cores and demonstrate three applications. first we utilize gpu tensor cores to optimize ht tensor learning primitives including tensor contractions tensor matricizations and tensor singular value decomposition  svd . we employ the optimized primitives to optimize ht tensor decomposition algorithms for big data analysis. second we propose a novel ht tensor layer for deep neural networks whose training process only involves a forward pass without back propagation. the forward pass consists of tensor operations thus further exploiting the computing power of gpu tensor cores. third we apply the optimized primitives to develop a tensor tree structured quantum machine learning algorithm tree tensor network  ttn . compared with tensorly and tensornetwork on nvidia a100 gpus our third order ht tensor decomposition algorithm achieves up to $8.92 \\times$8.92\u221a\u00f3 and $6.42 \\times$6.42\u221a\u00f3 speedups respectively and our high order case achieves up to $32.67 \\times$32.67\u221a\u00f3 and $23.97 \\times$23.97\u221a\u00f3 speedups respectively. our ht tensor layer for a fully connected neural network achieves $49.2 \\times$49.2\u221a\u00f3 compression at the cost of 0.5% drops in accuracy and $1.42 \\times$1.42\u221a\u00f3 speedup compared with the implementation on cuda cores  for the alexnet our ht tensor layer achieves $9.45 \\times$9.45\u221a\u00f3 compression at the cost of 0.8% drops in accuracy and $1.87 \\times$1.87\u221a\u00f3 speedup compared with the implementation on cuda cores. our ttn algorithm achieves up to $11.17\\times$11.17\u221a\u00f3 speedup compared with tensornetwork indicating the potential of optimized tensor learning primitives for the classical simulation of quantum machine learning algorithms.", "Doi": "10.1109/TC.2022.3172895", "Key Words": ["gpu tensor cores", "hierarchical tucker", "big data", "ht tensor layer", "quantum machine learning"]}