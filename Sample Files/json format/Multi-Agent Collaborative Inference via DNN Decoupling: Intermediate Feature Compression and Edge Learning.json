{"Title": "Multi-Agent Collaborative Inference via DNN Decoupling: Intermediate Feature Compression and Edge Learning", "Doi": "10.1109/TMC.2022.3183098", "Authors": ["z. hao", "g. xu", "y. luo", "h. hu", "j. an", "s. mao"], "Key Words": ["deep reinforcement learning", "mobile edge computing", "multi-user", "collaborative inference", "hybrid action space"], "Abstract": "recently deploying deep neural network  dnn  models via collaborative inference which splits a pre trained model into two parts and executes them on user equipment  ue  and edge server respectively becomes attractive. however the large intermediate feature of dnn impedes flexible decoupling and existing approaches either focus on the single ue scenario or simply define tasks considering the required cpu cycles but ignore the indivisibility of a single dnn layer. in this article we study the multi agent collaborative inference scenario where a single edge server coordinates the inference of multiple ues. our goal is to achieve fast and energy efficient inference for all ues. to achieve this goal we design a lightweight autoencoder based method to compress the large intermediate feature at first. then we define tasks according to the inference overhead of dnns and formulate the problem as a markov decision process  mdp . finally we propose a multi agent hybrid proximal policy optimization  mahppo  algorithm to solve the optimization problem with a hybrid action space. we conduct extensive experiments with different types of networks and the results show that our method can reduce up to 56% of inference latency and save up to 72% of energy consumption.", "Pub Date": "2023-08-31"}