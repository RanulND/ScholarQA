{"Title": "Knowledge Distillation with Source-free Unsupervised Domain Adaptation for BERT Model Compression", "Doi": "10.1109/CSCWD57460.2023.10152760", "Authors": ["j. tian", "j. chen", "n. j. chen", "l. bai", "s. huang"], "Key Words": ["knowledge distillation", "unsupervised domain adaptation", "bert model", "model compression"], "Abstract": "the pre training language model bert has brought significant performance improvements to a series of natural language processing tasks but due to the large scale of the model it is difficult to be applied in many practical application scenarios. with the continuous development of edge computing deploying the models on resource constrained edge devices has become a trend. considering the distributed edge environment how to take into account issues such as data distribution differences labeling costs and privacy while the model is shrinking is a critical task. the paper proposes a new bert distillation method with source free unsupervised domain adaptation. by combining source free unsupervised domain adaptation and knowledge distillation for optimization and improvement the performance of the bert model is improved in the case of cross domain data. compared with other methods our method can improve the average prediction accuracy by up to around 4% through the experimental evaluation of the cross domain sentiment analysis task.", "Pub Date": "2023-06-22"}