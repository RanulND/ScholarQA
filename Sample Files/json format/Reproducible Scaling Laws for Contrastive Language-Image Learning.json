{"Title": "Reproducible Scaling Laws for Contrastive Language-Image Learning", "Doi": "10.1109/CVPR52729.2023.00276", "Authors": ["m. cherti", "r. beaumont", "r. wightman", "m. wortsman", "g. ilharco", "c. gordon", "c. schuhmann", "l. schmidt", "j. jitsev"], "Key Words": ["multi-modal learning"], "Abstract": "scaling up neural networks has led to remarkable performance across a wide range of tasks. moreover performance often follows reliable scaling laws as a function of training set size model size and compute which offers valuable guidance as large scale experiments are becoming increasingly expensive. however previous work on scaling laws has primarily used private data & models or focused on uni modal language or vision learning. to address these limitations we investigate scaling laws for contrastive language image pre training  clip  with the public laion dataset and the open source openclip repository. our large scale experiments involve models trained on up to two billion image text pairs and identify power law scaling for multiple downstream tasks including zero shot classification retrieval linear probing and end to end fine tuning. we find that the training distribution plays a key role in scaling laws as the openai and openclip models exhibit different scaling behavior despite identical model architectures and similar training recipes. we open source our evaluation workflow and all models including the largest public clip models to ensure reproducibility and make scaling laws research more accessible. source code and instructions to reproduce this study is available at https //github.eom laion artificial intelliegence sealing laws openelip.", "Pub Date": "2023-08-22"}