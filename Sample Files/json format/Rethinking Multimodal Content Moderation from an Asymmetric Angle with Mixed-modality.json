{"Title": "Rethinking Multimodal Content Moderation from an Asymmetric Angle with Mixed-modality", "Doi": "10.1109/WACV57701.2024.00834", "Authors": ["j. yuan", "y. yu", "g. mittal", "m. hall", "s. sajeev", "m. chen"], "Key Words": ["applications", "social good", "algorithms", "vision + language and/or other modalities"], "Abstract": "there is a rapidly growing need for multimodal content moderation  cm  as more and more content on social media is multimodal in nature. existing unimodal cm systems may fail to catch harmful content that crosses modalities  e.g. memes or videos  which may lead to severe consequences. in this paper we present a novel cm model asymmetric mixed modal moderation  am3  to target multimodal and unimodal cm tasks. specifically to address the asymmetry in semantics between vision and language am3 has a novel asymmetric fusion architecture that is designed to not only fuse the common knowledge in both modalities but also to exploit the unique information in each modality. unlike previous works that focus on representing the two modalities into a similar feature space while overlooking the intrinsic difference between the information conveyed in multimodality and in unimodality  asymmetry in modalities  we propose a novel cross modality contrastive loss to learn the unique knowledge that only appears in multimodality. this is critical as some harmful intent may only be conveyed through the intersection of both modalities. with extensive experiments we show that am3 outperforms all existing state of the art methods on both multimodal and unimodal cm benchmarks.", "Pub Date": "2024-04-09"}