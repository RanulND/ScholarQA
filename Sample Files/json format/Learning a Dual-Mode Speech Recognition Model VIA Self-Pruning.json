{"Title": "Learning a Dual-Mode Speech Recognition Model VIA Self-Pruning", "Doi": "10.1109/SLT54892.2023.10022446", "Authors": ["c. liu", "y. shangguan", "h. yang", "y. shi", "r. krishnamoorthi", "o. kalinli"], "Key Words": ["neural network pruning", "sparsity optimization", "supernet", "recurrent neural network transducer"], "Abstract": "there is growing interest in unifying the streaming and full context automatic speech recognition  asr  networks into a single end to end asr model to simplify the model training and deployment for both use cases. while in real world asr applications the streaming asr models typically operate under more storage and computational constraints   e.g. on embedded devices   than any server side full context models. motivated by the recent progress in omni sparsity supernet training where multiple subnetworks are jointly optimized in one single model this work aims to jointly learn a compact sparse on device streaming asr model and a large dense server non streaming model in a single supernet. next we present that performing supernet training on both wav2vec 2.0 self supervised learning and supervised asr fine tuning can not only substantially improve the large non streaming model as shown in prior works and also be able to improve the compact sparse streaming model.", "Pub Date": "2023-01-27"}