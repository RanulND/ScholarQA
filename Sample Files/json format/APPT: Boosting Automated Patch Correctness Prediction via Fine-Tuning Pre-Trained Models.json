{"Title": "APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models", "Doi": "10.1109/TSE.2024.3354969", "Authors": ["q. zhang", "c. fang", "w. sun", "y. liu", "t. he", "x. hao", "z. chen"], "Key Words": ["automated program repair", "patch correctness", "pre-trained model"], "Abstract": "automated program repair  automated program repair  aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. despite the recent significant progress in the number of fixed bugs automated program repair is still challenged by a long standing overfitting problem  i.e. the generated patch is plausible but overfitting . various techniques have thus been proposed to address the overfitting problem. recently researchers have employed bert to extract code features which are then used to train a classifier for patch correctness prediction indicating the potential of such pre trained models in reasoning about patch correctness. however bert is restricted to feature extraction for classifier training without benefiting from the training process potentially generating sub optimal vector representations for patched code snippets. in this paper we propose appt a pre trained model based automated patch correctness assessment technique by both pre training and fine tuning. appt adopts a pre trained model as the encoder stack followed by an lstm stack and a deep learning classifier. more importantly the pre trained model is fine tuned in conjunction with other components as a whole pipeline to fully adapt it specifically for reasoning about patch correctness. although our idea is general and can be built on various existing pre trained models we have implemented appt based on the bert model. we conduct an extensive experiment on 1183 defects4j patches and the experimental results show that appt achieves prediction accuracy of 79.7% and recall of 83.2% outperforming the state of the art technique cache by 4.3% and 6.7%. our additional investigation on 49694 real world patches shows that appt achieves the optimum performance  exceeding 99% in five common metrics for assessing patch classification techniques  compared with existing representation learning techniques. we further investigate the impact of each component and find that they all positively contribute to appt e.g. the fine tuning process and the lstm stack increase f1 score by 10.22% and 4.11% respectively. we also prove that adopting advanced pre trained models can further provide substantial advancement  e.g. graphcodebert based appt improves bert based appt by 2.8% and 3.3% in precision and auc respectively  highlighting the generalizability of appt. overall our study highlights the promising future of fine tuning pre trained models to assess patch correctness and reduce the manual inspection effort of debugging experts when deploying automated program repair tools in practice.", "Pub Date": "2024-03-18"}