{"Title": "Large-Scale Language Model Rescoring on Long-Form Data", "Doi": "10.1109/ICASSP49357.2023.10096429", "Authors": ["t. chen", "c. allauzen", "y. huang", "d. park", "d. rybach", "w. r. huang", "r. cabrera", "k. audhkhasi", "b. ramabhadran", "p. j. moreno", "m. riley"], "Key Words": ["large-scale language models", "n-best rescoring", "fine-tuning"], "Abstract": "in this work we study the impact of large scale language models  large language model  on automated speech recognition  asr  of youtube videos which we use as a source for long form asr. we demonstrate up to 8% relative reduction in word error eate  wer  on us english  en us  and code switched indian english  en in  long form asr test sets and a reduction of up to 30% relative on salient term error rate  ster  over a strong first pass baseline that uses a maximum entropy based language model. improved lattice processing that results in a lattice with a proper  non tree  digraph topology and carrying context from the 1 best hypothesis of the previous segment s  results in significant wins in rescoring with large language model. we also find that the gains in performance from the combination of large language model trained on vast quantities of available data  such as c4   and conventional neural lms is additive and significantly outperforms a strong first pass baseline with a maximum entropy lm.", "Pub Date": "2023-05-05"}