{"Title": "Compression Models via Meta-Learning and Structured Distillation for Named Entity Recognition", "Doi": "10.1109/IALP61005.2023.10336991", "Authors": ["q. zhang", "z. gao", "m. zhang", "j. duan", "h. wang", "l. he"], "Key Words": ["named entity recognition", "knowledge distillation", "meta-learning"], "Abstract": "this paper addresses the issue of high resource consumption in named entity recognition  ner  under large models by utilizing meta learning and structured distillation to generate lightweight models. knowledge distillation from commonly used models in ner tasks poses challenges because of the exponentially large output space. previous work treated it as a structured prediction task for distillation but did not consider utilizing the feedback from the student model to optimize the student itself. therefore this paper proposes meta structured distillation  msd . specifically this paper incorporates meta learning into structured distillation updating the teacher parameters based on the student performance feedback on the dataset to obtain a better student model. experimental results demonstrate the effectiveness of this approach showing improvement over previous work in structured distillation.", "Pub Date": "2023-12-12"}