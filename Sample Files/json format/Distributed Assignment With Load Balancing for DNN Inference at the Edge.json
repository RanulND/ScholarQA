{"Title": "Distributed Assignment With Load Balancing for DNN Inference at the Edge", "Doi": "10.1109/JIOT.2022.3205410", "Authors": ["y. xu", "t. mohammed", "m. di francesco", "c. fischione"], "Key Words": ["assignment problems", "distributed inference", "deep neural network (dnn) offloading", "edge computing"], "Abstract": "inference carried out on pretrained deep neural networks  dnns  is particularly effective as it does not require retraining and entails no loss in accuracy. unfortunately resource constrained devices such as those in the internet of things may need to offload the related computation to more powerful servers particularly at the network edge. however edge servers have limited resources compared to those in the cloud  therefore inference offloading generally requires dividing the original dnn into different pieces that are then assigned to multiple edge servers. related approaches in the state of the art either make strong assumptions on the system model or fail to provide strict performance guarantees. this article specifically addresses these limitations by applying distributed assignment to dnn inference at the edge. in particular it devises a detailed model of dnn based inference suitable for realistic scenarios involving edge computing. optimal inference offloading with load balancing is also defined as a multiple assignment problem that maximizes proportional fairness. moreover a distributed algorithm for dnn inference offloading is introduced to solve such a problem in polynomial time with strong optimality guarantees. finally extensive simulations employing different data sets and dnn architectures establish that the proposed solution significantly improves upon the state of the art in terms of inference time  1.14 to 2.62 times faster  load balance  with jain\u201a\u00e4\u00f4s fairness index of 0.9  and convergence  one order of magnitude less iterations .", "Pub Date": "2023-01-05"}