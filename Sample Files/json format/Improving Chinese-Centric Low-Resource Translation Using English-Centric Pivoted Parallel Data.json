{"Title": "Improving Chinese-Centric Low-Resource Translation Using English-Centric Pivoted Parallel Data", "Doi": "10.1109/IALP61005.2023.10337297", "Authors": ["x. wang", "l. mu", "h. xu"], "Key Words": ["mnmt", "low-resource translation", "pre-trained models"], "Abstract": "the good performance of neural machine trans lation  nmt  normally relies on a large amount of parallel data while the bilingual data between languages are usually insufficient. mbart improves the performance of low resource translation by pre training on multilingual monolingual data and then fine tuning on bilingual data but does not leverage parallel data which contains crucial alignment information between languages. in this paper we propose to use english centric parallel data in a multilingual nmt  mnmt  manner with english as the pivot to provide translation and alignment information for the translation between chinese and other languages. we conduct experiments on the ccmt 2023 low resource machine translation task between chinese and the languages among \u201a\u00e4\u00fathe belt and road\u201a\u00e4\u00f9. our method improves the zh $\\rightarrow \\text{vi}$ vi $\\rightarrow \\text{zh}$ zh $\\rightarrow \\text{mn} \\text{mn} \\rightarrow \\text{zh}$ zh $\\rightarrow \\text{cs}$ and cs $\\rightarrow \\text{zh}$ tasks by $+1.65 +0.24+0.91 +3.47 +2.88 +6.35$ bleu respectively over the strong mbart baseline showing the effectiveness of our approach and the importance of english centric parallel data.", "Pub Date": "2023-12-12"}