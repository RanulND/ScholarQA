{"Title": "Controllable Text-to-Image Synthesis for Multi-Modality MR Images", "Doi": "10.1109/WACV57701.2024.00775", "Authors": ["k. kim", "y. na", "s. -j. ye", "j. lee", "s. s. ahn", "j. eun park", "h. kim"], "Key Words": ["applications", "biomedical / healthcare / medicine", "algorithms", "vision + language and/or other modalities"], "Abstract": "generative modeling has seen significant advancements in recent years especially in the realm of text to image synthesis. despite this progress the medical field has yet to fully leverage the capabilities of large scale foundational models for synthetic data generation. this paper introduces a framework for text conditional magnetic resonance  mr  imaging generation addressing the complexities associated with multi modality considerations. the framework comprises a pre trained large language model a diffusion based prompt conditional image generation architecture and an additional denoising network for input structural binary masks. experimental results demonstrate that the proposed framework is capable of generating realistic high resolution and high fidelity multi modal mr images that align with medical language text prompts. further the study interprets the cross attention maps of the generated results based on text conditional statements. the contributions of this research lay a robust foundation for future studies in text conditional medical image generation and hold significant promise for accelerating advancements in medical imaging research.", "Pub Date": "2024-04-09"}