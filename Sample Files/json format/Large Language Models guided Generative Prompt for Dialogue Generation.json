{"Title": "Large Language Models guided Generative Prompt for Dialogue Generation", "Doi": "10.1109/CyberC58899.2023.00013", "Authors": ["s. liu", "y. fang", "h. cheng", "y. pan", "y. liu", "c. gao"], "Key Words": ["large language models", "prompt learning", "dialogue generation", "few-shot"], "Abstract": "the applications of large language models  large language model  such as chatgpt exhibit impressive comprehension and generative capabilities in dialogue task. large language model require massive high quality data and computational cost which limits their application to low resource tasks. dialogue generation when using smaller language models like gpt-2 encounters difficulties in maintaining context consistency. to address the problem of dialogue generation under resource constraints we propose an large language model guided generative prompt method  lgp . lgp enhances the relevance and coherence of generated dialogues through a smaller model gpt-2 and generative prompt  gp . gp is produced by the proposed prompt network which leverages prompt encoder to learn dialogue history features and utilizes lstm to extract contextual temporal features. therefore gp shown as the simple fixed length learnable embeddings can replace the original complex and redundant context in gpt 2. the few shot training of gp is guided by the large language model\u201a\u00e4\u00f4s responses which facilitates gpt-2 in generating more contextually consistent and comprehensive responses. experiments on the dailydialog and multiwoz datasets show that lgp achieves high improvements in bleu nist meteor and rouge l metrics. remarkably lgp achieves these results with approximately 18% of the training data surpassing other full data finetuning methods in automatic evaluation metrics.", "Pub Date": "2024-02-21"}