{"Title": "FashionVLM - Fashion Captioning Using Pretrained Vision Transformer and Large Language Model", "Doi": "10.1109/ESCI59607.2024.10497350", "Authors": ["g. g. computer science", "p. s. computer division"], "Key Words": ["image captioning", "fashion captioning", "fashion captioning dataset", "ms coco dataset", "transformer", "large language model", "vision transformer"], "Abstract": "image captioning models automatically generate image descriptions using semantics of the input image. improvements in image captioning have paved the way for fashion image captioning to generate more expressive descriptions capturing more attributes of the fashion item. in our current research work we focus on designing and developing a fashion image captioning model for automating the generation of descriptive captions for fashion items. we call it the fashion vision language model  fashionvlm  to capture the multi modality nature of the model. we utilize a frozen large language model as a text decoder and a vision transformer as an image encoder connecting these models with a comparatively smaller querying transformer. fashion captioning dataset  facad  is one of the biggest datasets of fashion items. for fine tuning on f acad we utilize blip-2 pretrain stage two and ms coco fine tuned models in three different stages. in stage one we use opt 2.7 and opt 6.7 based blip-2 pretrain stage two models as base models. in stage two we utilize blip-2 opt 2.7 and opt 6.7 based ms coco fine tuned models as base models. in stage three we use stage one models as the base models for fine tuning. the opt 6.7 based stage three fashionvlm achieves the best performance compared to the state of the art for fashion captioning on facad providing +4.281 points + 39.015 points +5.667 points and + 3.519 points improvements for bleu 4 cider rouge l and meteor performance metrics respectively.", "Pub Date": "2024-04-17"}