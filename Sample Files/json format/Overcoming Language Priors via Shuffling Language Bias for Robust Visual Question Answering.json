{"Title": "Overcoming Language Priors via Shuffling Language Bias for Robust Visual Question Answering", "Doi": "10.1109/ACCESS.2023.3304415", "Authors": ["j. zhao", "z. yu", "x. zhang", "y. yang"], "Key Words": ["visual question answering", "language prior", "data balance", "data augmentation"], "Abstract": "recent research has revealed the notorious language prior problem in visual question answering  vqa  tasks based on visual textual interaction which indicates that well developed vqa models rely on learning shortcuts from questions without fully considering visual evidence.to tackle this problem most existing methods focus on decreasing the incentive to learn prior knowledge by adding a question only branch and becoming complacent by mechanically improving accuracy. however these methods over correct positive biases useful for generalization leading to the degradation of performance on the vqa v2 dataset when cumulating their methods into other vqa architecture. in this paper we propose a robust shuffling language bias  slb  approach to explicitly balance the prediction distribution hopefully alleviating the language prior by increasing training opportunities for vqa models.experiment results demonstrate that our method is cumulative with data augmentation and large scale pre training vqa architectures and achieves competitive performance on both the in domain benchmark vqa v2 and out of distribution benchmark vqa cp v2.", "Pub Date": "2023-08-17"}