{"Title": "Overcoming Language Priors via Shuffling Language Bias for Robust Visual Question Answering", "Doi": "10.1109/ACCESS.2023.3304415", "Authors": ["j. zhao", "z. yu", "x. zhang", "y. yang"], "Key Words": ["visual question answering", "language prior", "data balance", "data augmentation"], "Abstract": "recent research has revealed the notorious language prior problem in visual question answering  visual question answering  tasks based on visual textual interaction which indicates that well developed visual question answering models rely on learning shortcuts from questions without fully considering visual evidence.to tackle this problem most existing methods focus on decreasing the incentive to learn prior knowledge by adding a question only branch and becoming complacent by mechanically improving accuracy. however these methods over correct positive biases useful for generalization leading to the degradation of performance on the visual question answering v2 dataset when cumulating their methods into other visual question answering architecture. in this paper we propose a robust shuffling language bias  slb  approach to explicitly balance the prediction distribution hopefully alleviating the language prior by increasing training opportunities for visual question answering models.experiment results demonstrate that our method is cumulative with data augmentation and large scale pre training visual question answering architectures and achieves competitive performance on both the in domain benchmark visual question answering v2 and out of distribution benchmark visual question answering cp v2.", "Pub Date": "2023-08-17"}