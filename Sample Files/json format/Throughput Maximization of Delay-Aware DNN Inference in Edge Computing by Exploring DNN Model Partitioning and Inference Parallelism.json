{"Title": "Throughput Maximization of Delay-Aware DNN Inference in Edge Computing by Exploring DNN Model Partitioning and Inference Parallelism", "Doi": "10.1109/TMC.2021.3125949", "Authors": ["j. li", "w. liang", "y. li", "z. xu", "x. jia", "s. guo"], "Key Words": ["mobile edge computing (mec)", "dnn model inference provisioning", "throughput maximization", "intelligent iot devices", "approximation and online algorithms", "delay-aware dnn inference", "dnn partitioning", "inference parallelism", "computing and bandwidth resource allocation and optimization", "algorithm design and analysis"], "Abstract": "mobile edge computing  mec  has emerged as a promising paradigm catering to overwhelming explosions of mobile applications by offloading compute intensive tasks to mec networks for processing. the surging of deep learning brings new vigor and vitality to shape the prospect of intelligent internet of things  iot  and edge intelligence arises to provision real time deep neural network  dnn  inference services for users. to accelerate the processing of the dnn inference of a user request in an mec network the dnn inference model usually can be partitioned into two connected parts  one part is processed in the local iot device of the request and another part is processed in a cloudlet  edge server  in the mec network. also the dnn inference can be further accelerated by allocating multiple threads of the cloudlet to which the request is assigned. in this paper we study a novel delay aware dnn inference throughput maximization problem with the aim to maximize the number of delay aware dnn service requests admitted by accelerating each dnn inference through jointly exploring dnn partitioning and multi thread execution parallelism. specifically we consider the problem under both offline and online request arrival settings  a set of dnn inference requests is given in advance and a sequence of dnn inference requests arrives one by one without the knowledge of future arrivals respectively. we first show that the defined problems are np hard. we then devise a novel constant approximation algorithm for the problem under the offline setting. we also propose an online algorithm with a provable competitive ratio for the problem under the online setting. we finally evaluate the performance of the proposed algorithms through experimental simulations. experimental results demonstrate that the proposed algorithms are promising", "Pub Date": "2023-04-04"}