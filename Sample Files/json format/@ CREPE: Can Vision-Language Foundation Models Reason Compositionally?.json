{"Title": "@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?", "Doi": "10.1109/CVPR52729.2023.01050", "Authors": ["z. ma", "j. hong", "m. o. gul", "m. gandhi", "i. gao", "r. krishna"], "Key Words": ["vision", "language", "and reasoning"], "Abstract": "a fundamental characteristic common to both human vision and natural language is their compositional nature. yet despite the performance gains contributed by large vision and language pretraining we find that across 7 architectures trained with 4 algorithms on massive datasets they struggle at compositionality. to arrive at this conclusion we introduce a new compositionality evaluation benchmark @ crepe which measures two important aspects of compositionality identified by cognitive science literature  system aticity and productivity. to measure systematicity crepe consists of a test dataset containing over 370k image text pairs and three different seen unseen splits. the three splits are designed to test models trained on three popular training datasets  cc 12m yfcc 15m and laion 400m. we also generate 325k 316k and 309k hard negative captions for a subset of the pairs. to test productivity crepe contains 17 k image text pairs with nine different complexities plus 278k hard negative captions with atomic swapping and negation foils. the datasets are generated by repurposing the visual genome scene graphs and region descriptions and applying handcrafted templates and gpt 3. for systematicity we find that model performance decreases consistently when novel compositions dominate the retrieval set with recall@1 dropping by up to 9%. for productivity models' retrieval success decays as complexity increases frequently nearing random chance at high complexity. these results hold regardless of model and training dataset size.", "Pub Date": "2023-08-22"}