{"Title": "FedLC: Accelerating Asynchronous Federated Learning in Edge Computing", "Doi": "10.1109/TMC.2023.3307610", "Authors": ["y. xu", "z. ma", "h. xu", "s. chen", "j. liu", "y. xue"], "Key Words": ["asynchronous federated learning", "edge computing", "non-i.i.d", "local collaboration"], "Abstract": "federated learning  fl  has been widely adopted to process the enormous data in the application scenarios like edge computing  ec . however the commonly used synchronous mechanism in fl may incur unacceptable waiting time for heterogeneous devices leading to a great strain on the devices\u201a\u00e4\u00f4 constrained resources. in addition the alternative asynchronous fl is known to suffer from the model staleness which will lead to performance degradation of the trained model especially on non i.i.d. data. in this paper we design a novel asynchronous fl mechanism named fedlc to handle the non i.i.d. issue in ec by enabling the local collaboration among edge devices. specifically apart from uploading the local model directly to the server each device will transmit its gradient to the other devices with different data distributions for local collaboration which can improve the model generality. we theoretically analyze the convergence rate of fedlc and obtain the quantitative relationship between convergence bound and local collaboration. we design an efficient algorithm utilizing demand list to determine the set of devices receiving gradients from each device. to handle the model staleness we further assign different learning rates for various devices according to their participation frequency. the extensive experimental results demonstrate the effectiveness of our proposed mechanism.", "Pub Date": "2024-04-04"}