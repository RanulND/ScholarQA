{"Title": "Real-Time Monocular Depth Estimation Merging Vision Transformers on Edge Devices for AIoT", "Authors": ["x. liu", "w. wei", "c. liu", "y. peng", "j. huang", "j. li"], "Pub Date": "2023-04-13", "Abstract": "depth estimation is requisite to build the 3 d perceiving capability of artificial intelligence of things  aiot . real time inference with extremely low computing resource consumption is critical on edge devices. however most single view depth estimation networks focus on the improvement of accuracy when running on high end gpus which goes against the real time requirement on edge devices. to address this issue this article proposed a novel encoder\u201a\u00e4\u00ecdecoder network to realize real time monocular depth estimation on edge devices. the proposed network merges semantic information at the global field via an efficient transformer based module to provide more details of the object for depth assignment. the transformer based module is integrated into the lowest level resolution of an encoder\u201a\u00e4\u00ecdecoder architecture to largely reduce the parameters of the vision transformer  vit . in particular we proposed a novel patch convolutional layer for low latency feature extraction in the encoder and an sconv5 layer for effective depth assignment in the decoder. the proposed network achieves an outstanding balance between the accuracy and speed of the nyu depth v2 dataset. a low root mean square error  rmse  of 0.554 and a fast speed of 58.98 fps on nvidia jetson nano device with tensorrt optimization are obtained on nyu depth v2 outperforming most state of the art real time results.", "Doi": "10.1109/TIM.2023.3264039", "Key Words": ["artificial intelligence of things (aiot)", "attention", "monocular depth estimation", "real-time", "transformers"]}