{"Title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction", "Doi": "10.1109/MODELS-C59198.2023.00097", "Authors": ["b. chen", "f. yi", "d. varr\u221a\u2265"], "Key Words": ["taxonomy construction", "domain-specific constraints", "large language models", "few-shot learning", "fine-tuning"], "Abstract": "taxonomies represent hierarchical relations between entities frequently applied in various software modeling and natural language processing  nlp  activities. they are typically subject to a set of structural constraints restricting their content. however manual taxonomy construction can be time consuming incomplete and costly to maintain. recent studies of large language models  large language model  have demonstrated that appropriate user inputs  called prompting  can effectively guide large language model such as gpt 3 in diverse nlp tasks without explicit  re  training. however existing approaches for automated taxonomy construction typically involve fine tuning a language model by adjusting model parameters. in this paper we present a general framework for taxonomy construction that takes into account structural constraints. we subsequently conduct a systematic comparison between the prompting and fine tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. our result reveals the following   1  even without explicit training on the dataset the prompting approach outperforms fine tuning based approaches. moreover the performance gap between prompting and fine tuning widens when the training dataset is small. however  2  taxonomies generated by the fine tuning approach can be easily post processed to satisfy all the constraints whereas handling violations of the taxonomies produced by the prompting approach can be challenging. these evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.", "Pub Date": "2023-12-22"}