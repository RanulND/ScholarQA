{"Title": "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation", "Doi": "10.1109/TSE.2023.3267446", "Authors": ["f. cassano", "j. gouwar", "d. nguyen", "s. nguyen", "l. phipps-costin", "d. pinckney", "m. -h. yee", "y. zi", "c. j. anderson", "m. q. feldman", "a. guha", "m. greenberg", "a. jangda"], "Key Words": ["b.2.3 reliability, testing, and fault-tolerance", "i.5.1.d neural nets"], "Abstract": "large language models have demonstrated the ability to generate both natural language and programming language text. although contemporary code generation models are trained on corpora with several programming languages they are tested using benchmarks that are typically monolingual. the most widely used code generation benchmarks only target python so there is little quantitative evidence of how code generation models perform on other programming languages. we propose multipl e a system for translating unit test driven code generation benchmarks to new languages. we create the first massively multilingual code generation benchmark by using multipl e to translate two popular python code generation benchmarks to 18 additional programming languages. we use multipl e to extend the humaneval benchmark  chen et al. 2021  and mbpp benchmark  austin et al. 2021  to 18 languages that encompass a range of programming paradigms and popularity. using these new parallel benchmarks we evaluate the multi language performance of three state of the art code generation models  codex  chen et al. 2021  codegen  nijkamp et al. 2022  and incoder  fried et al. 2022 . we find that codex matches or even exceeds its performance on python for several other languages. the range of programming languages represented in multipl e allow us to explore the impact of language frequency and language features on model performance. finally the multipl e approach of compiling code generation benchmarks to new programming languages is both scalable and extensible making it straightforward to evaluate new models benchmarks and languages.", "Pub Date": "2023-07-17"}