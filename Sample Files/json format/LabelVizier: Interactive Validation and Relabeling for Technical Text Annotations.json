{"Title": "LabelVizier: Interactive Validation and Relabeling for Technical Text Annotations", "Doi": "10.1109/PacificVis56936.2023.00026", "Authors": ["x. zhang", "x. xuan", "a. dima", "t. sexton", "k. -l. ma"], "Key Words": ["workflow design", "technical language processing", "data annotation", "model interpretation"], "Abstract": "with the rapid accumulation of text data produced by data driven techniques the task of extracting \"data annotations\"\u201a\u00e4\u00eeconcise high quality data summaries from unstructured raw text\u201a\u00e4\u00eehas become increasingly important. the recent advances in weak supervision and crowd sourcing techniques provide promising solutions to efficiently create annotations  labels  for large scale technical text data. however such annotations may fail in practice because of the change in annotation requirements application scenarios and modeling goals where label validation and relabeling by domain experts are required. to approach this issue we present labelvizier a human in the loop workflow that incorporates domain knowledge and user specific requirements to reveal actionable insights into annotation flaws then produce better quality labels for large scale multi label datasets. we implement our workflow as an interactive notebook to facilitate flexible error profiling in depth annotation validation for three error types and efficient annotation relabeling on different data scales. we evaluated our workflow in assisting the validation and relabelling of technical text annotation with two use cases and four expert reviews. the results show that labelvizier is applicable in various application scenarios and users with different knowledge backgrounds have diverse preferences for the tool usage.", "Pub Date": "2023-06-14"}