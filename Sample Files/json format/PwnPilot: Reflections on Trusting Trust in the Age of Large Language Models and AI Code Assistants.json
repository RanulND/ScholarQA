{"Title": "PwnPilot: Reflections on Trusting Trust in the Age of Large Language Models and AI Code Assistants", "Doi": "10.1109/CSCE60160.2023.00396", "Authors": ["d. horne"], "Key Words": ["trusting trust", "artificial intelligence (ai) and machine learning (ml) security", "ai code assistant", "github copilot", "chatgpt", "pwnpilot", "ai driven n-version programming (aid-nvp)"], "Abstract": "at the dawn of a new era in software engineering one defined by large language models  llms  and ai code assistants like github copilot new meaning can be found from a historic turing award lecture that concluded one cannot trust source code they \u201a\u00e4\u00fadid not totally create\u201a\u00e4\u00f9 themselves. in this paper a targeted systematic survey of the latest research results from 2019 to early 2023 highlights the possible risks of using ai code assistants that produce substantial source code contributions and the potential for an ai copilot to unknowingly become pwnpilot a malevolent digital actor that introduces vulnerabilities and compromises trust. during a period of explosive growth for generative ai renewed reflections on trusting trust point to conclusions similar to the original assertions of ken thompson in 1984. but despite a recent theoretical roadblock from proof of ability to plant undetectable backdoors in machine learning models the potential for enhanced productivity from ai code assistants may still be realizable with an acceptable level of risk perhaps even for safety critical and sensitive security relevant contexts. in support of that goal a number of near term risk management options and longer term research paths are identified as enablers for practitioners and inputs to potential research roadmaps toward more secure and trusted ai code generation.", "Pub Date": "2024-04-09"}