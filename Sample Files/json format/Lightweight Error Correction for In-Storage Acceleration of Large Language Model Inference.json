{"Title": "Lightweight Error Correction for In-Storage Acceleration of Large Language Model Inference", "Doi": "10.1109/ICEIC61013.2024.10457117", "Authors": ["j. jeong", "b. ahn", "d. shin", "j. choi"], "Key Words": ["large language model", "error correction code", "nand flash errors"], "Abstract": "as large language models  llms  expand their sizes conventional gpu based llm inference systems face memory bandwidth and capacity limitations. an llm inference accelerator using nand flash storage has been proposed to overcome these challenges. however this necessitates a significant expansion of flash channels to ensure adequate bandwidth for inference subsequently escalating error correction code  ecc  costs. this paper examines the impact of flash memory errors on llm inference accuracy and explores the possibility of lightweight ecc by leveraging llm inherent error resilience. we analyze the impact of 1  high order bit indices masking for fp32 llm parameters 2  clipping and 3  a dependency by parameter type of error robustness and show that a combination of them can reduce ecc bandwidth by up to 9.38%.", "Pub Date": "2024-03-19"}