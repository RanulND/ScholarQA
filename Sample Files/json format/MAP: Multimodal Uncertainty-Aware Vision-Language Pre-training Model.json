{"Title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model", "Doi": "10.1109/CVPR52729.2023.02228", "Authors": ["y. ji", "j. wang", "y. gong", "l. zhang", "y. zhu", "h. wang", "j. zhang", "t. sakai", "y. yang"], "Key Words": ["multi-modal learning"], "Abstract": "multimodal semantic understanding often has to deal with uncertainty which means the obtained messages tend to refer to multiple targets. such uncertainty is problematic for our interpretation including inter  and intra modal uncertainty. little effort has studied the modeling of this uncertainty particularly in pretraining on unlabeled datasets and fine tuning in task specific downstream datasets. in this paper we project the representations of all modalities as probabilistic distributions via a probability distribution encoder  pde  by utilizing sequence level interactions. compared to the existing deterministic methods such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. furthermore we integrate uncertainty modeling with popular pretraining frameworks and propose suitable pretraining tasks  distribution based vision language contrastive learning  d vlc  distribution based masked language modeling  d mlm  and distribution based image text matching  d itm . the fine tuned models are applied to challenging downstream tasks including image text retrieval visual question answering visual reasoning and visual entailment and achieve state of the art results.", "Pub Date": "2023-08-22"}