{"Title": "Network Pruning for Bit-Serial Accelerators", "Doi": "10.1109/TCAD.2022.3203955", "Authors": ["x. zhao", "y. wang", "c. liu", "c. shi", "k. tu", "l. zhang"], "Key Words": ["ai accelerators", "neural networks (nns)", "nn compression"], "Abstract": "bit serial architectures  bsas  are becoming increasingly popular in low power neural network processor  nnp  designs for edge scenarios. however the performance and energy efficiency of state of the art bsa nnps heavily depends on both the proportion and distribution of ineffectual weight bits in neural networks  nns . to boost the performance of typical bsa accelerators we present bit pruner a software approach to learn bsa favored nns without resorting to hardware modifications. bit pruner not only progressively prunes but also restructures the nonzero bits in weights so that the number of nonzero bits in the model can be reduced and the corresponding computing can be load balanced to suit the target bsa accelerators. on top of bit pruner we further propose a pareto frontier optimization algorithm to adjust the bit pruning rate across network layers and fulfill diverse nn processing requirements in terms of performance and accuracy for various edge scenarios. however an aggressive bit pruner can lead to nontrivial accuracy loss especially for lightweight nns and complex tasks. to this end the alternating direction method of multipliers  admms  is adapted to the retraining phase in bit pruner to smooth the abrupt disturbance due to bit pruning and enhance the resulting model accuracy. according to the experiments bit pruner increases the bit sparsity up to 94.4% with negligible accuracy degradation and achieves an optimized tradeoff between nn accuracy and energy efficiency even under very aggressive performance constraints. when pruned models are deployed onto typical bsa accelerators the average performance is  $2.1\\times $  and  $1.6\\times $  higher than the baseline networks without pruning and those with classical weight pruning respectively.", "Pub Date": "2023-04-20"}