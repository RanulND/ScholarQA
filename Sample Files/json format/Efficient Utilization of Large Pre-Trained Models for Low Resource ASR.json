{"Title": "Efficient Utilization of Large Pre-Trained Models for Low Resource ASR", "Doi": "10.1109/ICASSPW59220.2023.10193570", "Authors": ["p. vieting", "c. l\u221a\u00bascher", "j. dierkes", "r. schl\u221a\u00bater", "h. ney"], "Key Words": ["speech recognition", "medical asr", "unsupervised pre-training"], "Abstract": "unsupervised representation learning has recently helped automatic speech recognition  asr  to tackle tasks with limited labeled data. following this hardware limitations and applications give rise to the question how to take advantage of large pre trained models efficiently and reduce their complexity. in this work we study a challenging low resource conversational telephony speech corpus from the medical domain in vietnamese and german. we show the benefits of using unsupervised techniques beyond simple fine tuning of large pre trained models discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre training and fine tuning. we outperform the project baselines by 22% relative using pre training techniques. further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in domain adaptation data.", "Pub Date": "2023-08-02"}