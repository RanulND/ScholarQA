{"Title": "A Fast, Performant, Secure Distributed Training Framework For LLM", "Doi": "10.1109/ICASSP48485.2024.10446717", "Authors": ["w. huang", "y. wang", "a. cheng", "a. zhou", "c. yu", "l. wang"], "Key Words": ["distributed llm", "security", "tee", "lightweight encryption"], "Abstract": "the distributed  federated  llm is an important method for co training the domain specific llm using siloed data. however maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. in this paper we propose a secure distributed llm based on model slicing. in this case we deploy the trusted execution environment  tee  on both the client and server side and put the fine tuned structure  lora or embedding of p tuning v2  into the tee. then secure communication is executed in the tee and general environments through lightweight encryption. in order to further reduce the equipment cost as well as increase the model performance and accuracy we propose a split fine tuning scheme. in particular we split the llm by layers and place the latter layers in a server side tee  the client does not need a tee . we then combine the proposed sparsification parameter fine tuning  spf  with the lora part to improve the accuracy of the downstream task. numerous experiments have shown that our method guarantees accuracy while maintaining security.", "Pub Date": "2024-03-18"}