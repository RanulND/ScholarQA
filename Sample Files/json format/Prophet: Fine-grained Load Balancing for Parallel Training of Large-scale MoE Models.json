{"Title": "Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models", "Doi": "10.1109/CLUSTER52292.2023.00015", "Authors": ["w. wang", "z. lai", "s. li", "w. liu", "k. ge", "y. liu", "a. shen", "d. li"], "Key Words": ["mixture of experts", "distributed training"], "Abstract": "mixture of expert  moe  has received increasing attention for scaling dnn models to extra large size with negligible increases in computation. the moe model has achieved the highest accuracy in several domains. however a significant load imbalance occurs in the device during the training of a moe model resulting in significantly reduced throughput. previous works on load balancing either harm model convergence or suffer from high execution overhead. to address these issues we present prophet  a fine grained load balancing method for parallel training of large scale moe models which consists of a planner and a scheduler. prophet planner first employs a fine grained resource allocation method to determine the possible scenarios for the expert placement in a fine grained manner and then efficiently searches for a well balanced expert placement to balance the load without introducing additional overhead. prophet scheduler exploits the locality of the token distribution to schedule the resource allocation operations using a layer wise fine grained schedule strategy to hide their overhead. we conduct extensive experiments in four clusters and five representative models. the results indicate that prophet gains up to 2.3x speedup compared to the state of the art moe frameworks including deepspeed moe and fastermoe. additionally prophet achieves a load balancing enhancement of up to 12.06x when compared to fastermoe.", "Pub Date": "2023-11-21"}