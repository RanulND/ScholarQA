{"Title": "STYLECAP: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-Supervised Learning Models", "Doi": "10.1109/ICASSP48485.2024.10445977", "Authors": ["k. yamauchi", "y. ijima", "y. saito"], "Key Words": ["speaking styles", "natural language descriptions", "self-supervised learning model", "large language models"], "Abstract": "we propose stylecap a method to generate natural language descriptions of speaking styles appearing in speech. although most of conventional techniques for para /non linguistic information recognition focus on the category classification or the intensity estimation of pre defined labels they cannot provide the reasoning of the recognition result in an interpretable manner. stylecap is a first step towards an end to end method for generating speaking style prompts from speech i.e. automatic speaking style captioning. stylecap is trained with paired data of speech and natural language descriptions. we train neural networks that convert a speech representation vector into prefix vectors that are fed into a large language model  large language model  based text decoder. we explore an appropriate text decoder and speech feature representation suitable for this new task. the experimental results demonstrate that our stylecap leveraging richer large language model for the text decoder speech self supervised learning  ssl  features and sentence rephrasing augmentation improves the accuracy and diversity of generated speaking style captions. samples of speaking style captions generated by our stylecap are publicly available 1.", "Pub Date": "2024-03-18"}