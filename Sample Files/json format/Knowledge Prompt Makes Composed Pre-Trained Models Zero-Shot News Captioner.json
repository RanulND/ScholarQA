{"Title": "Knowledge Prompt Makes Composed Pre-Trained Models Zero-Shot News Captioner", "Doi": "10.1109/ICME55011.2023.00489", "Authors": ["y. wang", "n. xu", "h. tian", "b. lv", "y. duan", "x. li", "a. -a. liu"], "Key Words": ["news image captioning", "image captioning", "zero-shot inference", "large language model", "transformer", "vision and language"], "Abstract": "news image captioning aims to generate descriptions containing concrete named entities for news images by leveraging relevant news articles. however existing approaches suffer from two shortcomings  1  lack of commonsense knowledge required to understand named entities and 2  limited multimodal context modeling capabilities. in this paper we propose to migrate the ability of large scale pre trained models for news image captioning. to acquire factual knowledge for describing named entities we induce a pre trained language model for commonsense knowledge reasoning using context aware knowledge prompts. to compose a new multimodal context modeling capability we coordinate pre trained models by a unified language representation and constrain joint multimodal context reasoning with cross modal consistency objective. experimental results on goodnews and nytimes datasets show that our proposed method exhibits considerable captioning capabilities even without training on news data.", "Pub Date": "2023-08-25"}