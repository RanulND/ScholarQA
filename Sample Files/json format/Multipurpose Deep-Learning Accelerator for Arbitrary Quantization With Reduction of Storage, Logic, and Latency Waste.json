{"Title": "Multipurpose Deep-Learning Accelerator for Arbitrary Quantization With Reduction of Storage, Logic, and Latency Waste", "Authors": ["s. moon", "h. -g. mun", "h. son", "j. -y. sim"], "Pub Date": "2023-12-28", "Abstract": "various pruning and quantization heuristics have been proposed to compress recent deep learning models. however the rapid development of new optimization techniques makes it difficult for domain specific accelerators to efficiently process various models showing irregularly stored parameters or nonlinear quantization. this article presents a scalable precision deep learning accelerator that supports multiply and accumulate operations  macs  with two arbitrarily quantized data sequences. the proposed accelerator includes three main features. to minimize logic overhead when processing arbitrarily quantized 8 bit precision data a lookup table  lut  based runtime reconfiguration is proposed. the use of bit serial execution without unnecessary computations enables the multiplication of data with non equal precision while minimizing logic and latency waste. furthermore two distinct data formats raw and run length compressed are supported by a zero eliminator  ze  and runtime density detector  rdd  that are compatible with both formats delivering enhanced storage and performance. for a precision range of 1\u201a\u00e4\u00ec8 bit and fixed sparsity of 30% the accelerator implemented in 28 nm low power  lp  cmos shows a peak performance of 0.87\u201a\u00e4\u00ec5.55 tops and a power efficiency of 15.1\u201a\u00e4\u00ec95.9 tops w. the accelerator supports processing with arbitrary quantization  aq  while achieving state of the art  sota  power efficiency.", "Doi": "10.1109/JSSC.2023.3312615", "Key Words": ["arbitrary quantization (aq)", "bit-serial processing", "deep neural network (dnn) accelerator", "lookup table (lut)", "precision scalability", "run-length compression (rlc)"]}