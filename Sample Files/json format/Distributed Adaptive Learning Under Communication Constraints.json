{"Title": "Distributed Adaptive Learning Under Communication Constraints", "Authors": ["m. carpentiero", "v. matta", "a. h. sayed"], "Pub Date": "2024-01-22", "Abstract": "we consider a network of agents that must solve an online optimization problem from continual observation of streaming data. to this end the agents implement a distributed cooperative strategy where each agent is allowed to perform local exchange of information with its neighbors. in order to cope with communication constraints the exchanged information must be compressed to reduce the communication load. we propose a distributed diffusion strategy nicknamed as actc  adapt compress then combine  which implements the following three operations  adaptation where each agent performs an individual stochastic gradient update  compression which leverages a recently introduced class of stochastic compression operators  and combination where each agent combines the compressed updates received from its neighbors. the main elements of novelty of this work are as follows  $i $ adaptive strategies where constant  as opposed to diminishing  step sizes are critical to infuse the agents with the ability of responding in real time to nonstationary variations in the observed model  $ii $ directed i.e. non symmetric combination policies which allow us to enhance the role played by the network topology in the learning performance  $iii $ global strong convexity a condition under which the individual agents might feature even non convex cost functions. under this demanding setting we establish that the iterates of the actc strategy fluctuate around the exact global optimizer with a mean square deviation on the order of the step size achieving remarkable savings of communication resources. comparison against up to date learning strategies with compressed data highlights the benefits of the proposed solution.", "Doi": "10.1109/OJSP.2023.3344052", "Key Words": ["distributed optimization", "adaptation and learning", "diffusion strategies", "stochastic quantizers"]}