{"Title": "Distilling Task Specific Models from Ernie for Chinese Medical Intention Classification", "Doi": "10.1109/SWC57546.2023.10449011", "Authors": ["z. wang", "c. xu", "l. chen", "j. qin", "c. ji"], "Key Words": ["intent classification", "knowledge distillation", "pre-trained language models", "medical nlp"], "Abstract": "the improvement of intention classification of medical query can effectively improve the performance of search engines and question answering systems and further improve medical services. the pre trained language models has achieved good performance in various nlp tasks but the slow reasoning speed and high requirements for storing computing resources of the model make it difficult for medical institutions to deploy relevant models. in this paper we propose a knowledge distillation model based on ernie to solve the chinese medical intention classification task in order to provide a small scale and more efficient intention classification model. we propose a two stage framework which is distilled on specific domain knowledge and tasks and we use data augmentation methods to minimize the loss of accuracy caused by model compression. specifically we distill the embedding layer and transformer layer of the teacher model in the domain knowledge specific distillation stage so that our student model can better capture the general domain and medical specific domain knowledge of the teacher model. in the intent classification task specific distillation stage we also distilled the prediction layer and added soft label and hard label to calculate the prediction layer loss to ensure that the student model acquires knowledge of specific tasks. considering the lack of labeled data available for training in the medical field and intention classification task we use word exchange and whole entity masking methods to augment the labeled data and augment the generalization ability of student models. we conducted experiments on a publicly available cblue dataset and the experimental results showed that our proposed 4 layer student model retained more than 98.6% of the language comprehension capabilities of the original 12 layer teacher model\u201a\u00e4\u00eeernie while reducing the computational resources required and significantly accelerating reasoning speed.", "Pub Date": "2024-03-01"}