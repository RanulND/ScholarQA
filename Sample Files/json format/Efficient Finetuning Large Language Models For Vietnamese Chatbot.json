{"Title": "Efficient Finetuning Large Language Models For Vietnamese Chatbot", "Doi": "10.1109/MAPR59823.2023.10288647", "Authors": ["v. -t. doan", "q. -t. truong", "d. -v. nguyen", "v. -t. nguyen", "t. -n. n. luu"], "Key Words": ["large language model", "instruction fine-tuning", "lora", "vietnamese", "chatbot", "medical"], "Abstract": "large language models  large language model  such as gpt 4 palm and llama have been shown to achieve remarkable performance across a variety of natural language tasks. recent advancements in instruction tuning bring large language model with ability in following user\u201a\u00e4\u00f4s instructions and producing human like responses. however the high costs associated with training and implementing large language model pose challenges to academic research. furthermore the availability of pretrained large language model and instruction tune datasets for vietnamese language is limited. to tackle these concerns we leverage large scale instruction following datasets from open source projects namely alpaca gpt4all and chatdoctor which cover general domain and specific medical domain. to the best of our knowledge these are the first instructional dataset for vietnamese. subsequently we utilize parameter efficient tuning through low rank adaptation  lora  on two open large language model  bloomz  multilingual  and gptj 6b  vietnamese  resulting four models  bloomz chat bloomz doctor gptj chat gptj doctor. finally we assess the effectiveness of our methodology on a per sample basis taking into consideration the helpfulness relevance accuracy level of detail in their responses. this evaluation process entails the utilization of gpt-4 as an automated scoring mechanism. despite utilizing a low cost setup our method demonstrates about 20 30% improvement over the original models in our evaluation tasks.", "Pub Date": "2023-10-26"}