{"Title": "Generalization Algorithm of Multimodal Pre-Training Model Based on Graph-Text Self-Supervised Training", "Doi": "10.1109/ICNLP58431.2023.00066", "Authors": ["x. zhang", "z. tang", "z. long", "x. fu"], "Key Words": ["neural machine translation", "self-supervised training", "natural language processing", "multimodal", "pre-trained language models"], "Abstract": "recently a large number of studies have shown that the introduction of visual information can effectively improve the effect of neural machine translation  nmt . its effectiveness largely depends on the availability of a large number of bilingual parallel sentence pairs and manual image annotation. the lack of images and the effectiveness of images have been difficult to solve. in this paper a multimodal pre training generalization algorithm for self supervised training is proposed which overcomes the lack of visual information and inaccuracy and thus extends the applicability of images on nmt. specifically we will search for many pictures from the existing sentences through the search engine and then through the relationship between visual information and text do the self supervised training task of graphics and text to obtain more effective visual information for text. we show that when the filtered information is used as multimodal machine translation for fine tuning the effect of translation in the global voice dataset is 0.5 bleu higher than the baseline.", "Pub Date": "2023-09-06"}