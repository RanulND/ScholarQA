{"Title": "Fine-Tuning Self-Supervised Models for Language Identification Using Orthonormal Constraint", "Doi": "10.1109/ICASSP48485.2024.10446751", "Authors": ["a. prasad", "a. carofilis", "g. vanderreydt", "d. khalil", "s. madikeri", "p. motlicek", "c. schuepbach"], "Key Words": ["language identification", "transformers", "wav2vec2", "fine-tuning", "low-resource", "out-of-domain"], "Abstract": "self supervised models trained with high linguistic diversity such as the xls r model can be effectively fine tuned for the language recognition task. typically a back end classifier followed by statistics pooling layer are added during training. commonly used back end classifiers require a large number of parameters to be trained which is not ideal in limited data conditions. in this work we explore smaller parameter back ends using factorized time delay neural network  tdnn f . the tdnn f architecture is also integrated into emphasized channel attention propagation and aggregation  tdnn  ecapa tdnn  models termed ecapa tdnn f reducing the number of parameters by 30 to 50% absolute with competitive accuracies and no change in minimum cost. the results show that the ecapa tdnn f can be extended to tasks where ecapa tdnn is suitable. we also test the effectiveness of a linear classifier and a variant the orthonormal linear classifier previously used in x vector type systems. the models are trained with nist lre17 data and evaluated on nist lre17 lre22 and the atco2 lid datasets. both linear classifiers outperform conventional back ends with improvements in accuracy between 0.9% and 9.1%.", "Pub Date": "2024-03-18"}