{"Title": "Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision", "Doi": "10.1109/BigData59044.2023.10386190", "Authors": ["j. su", "p. yu", "j. zhang", "s. h. bach"], "Key Words": ["weak supervision", "large language model", "structure learning"], "Abstract": "prompted weak supervision  promptedws  applies pre trained large language models  large language model  as the basis for labeling functions  lfs  in a weak supervision framework to obtain large labeled datasets. we further extend the use of large language model in the loop to address one of the key challenges in weak supervision  learning the statistical dependency structure among supervision sources. in this work we ask the large language model how similar are these prompted lfs. we propose a structure refining module a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. at the core of structure refining module are labeling function removal  lare  and correlation structure generation  cosgen . compared to previous methods that learn the dependencies from weak labels our method finds the dependencies which are intrinsic to the lfs and less dependent on the data. we show that our structure refining module improves the promptedws pipeline by up to 12.7 points on the benchmark tasks. we also explore the trade offs between efficiency and performance with comprehensive ablation experiments and analysis. code for this project can be found in https //github.com batsresearch/su bigdata23 code.", "Pub Date": "2024-01-22"}