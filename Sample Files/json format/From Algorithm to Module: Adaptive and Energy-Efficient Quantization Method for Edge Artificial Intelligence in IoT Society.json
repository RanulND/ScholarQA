{"Title": "From Algorithm to Module: Adaptive and Energy-Efficient Quantization Method for Edge Artificial Intelligence in IoT Society", "Doi": "10.1109/TII.2022.3223222", "Authors": ["t. li", "y. ma", "t. endoh"], "Key Words": ["deep neural networks (dnns)", "edge artificial intelligence (ai)", "energy-efficient chips", "industrial internet of things (iot)", "quantization-on-multiplier (qom)", "quantization method"], "Abstract": "next generation industrial edge artificial intelligence  artificial intelliegence  applications will undoubtedly emerge on energy efficient highly integrated platforms incorporating various sensors processors and functional modules. embedding the data quantization module onto edge artificial intelliegence chips connecting sensors processors and functional modules is critical to achieving adaptive transformation of diverse data representation formats. this paper proposes a novel adaptive and low power quantization technique and systematically validates its effectiveness from algorithm to hardware module for industrial iot applications covering precise navigation for autonomous vehicles and accurate classification utilizing deep neural networks  dnns . the proposed quantization method merges an adaptive conversion function from floating point to fixed point binaries and an adaptive radix point determination function ensuring adequate resolution and minimal error loss of the fixed point inputs to the edge artificial intelliegence modules. the experimental results demonstrate that the quantization error in the proposed quantization technique contributes negligible errors to the navigation solutions of the strapdown inertial navigation system and the dnns' top 1 and top 5 classification accuracy  on the order of 10$^{ 8}$ and 10$^{ 7}$ . moreover a quantization on multiplier  qom  hardware module is designed synthesized and routed in accordance with the proposed quantization technique. the simulation results indicate that the qom power consumption and area are 0.1 mw and 649.552 $\\mu \\text{m}^{2}$ accounting for 5% and 14.15% of its total energy consumption and area respectively. with our proposed on chip quantization technique the time required to quantize the parameters of dnns is up to 1142 times shorter than with the existing benchmark off chip quantization approaches.", "Pub Date": "2023-07-13"}