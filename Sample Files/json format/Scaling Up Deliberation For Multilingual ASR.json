{"Title": "Scaling Up Deliberation For Multilingual ASR", "Doi": "10.1109/SLT54892.2023.10023028", "Authors": ["k. hu", "b. li", "t. n. sainath"], "Key Words": ["multilingual deliberation", "multilingual automatic speech recognition", "large-scale training"], "Abstract": "multilingual end to end automatic speech recognition models are attractive due to its simplicity in training and deployment. recent work on large scale training of such models has shown promising results compared to monolingual models. however the work often focuses on multilingual models themselves in a single pass setup. in this work we investigate second pass deliberation for multilingual speech recognition. our proposed deliberation is multilingual i.e. the text encoder encodes hypothesis text from multiple languages and the decoder attends to multilingual text and audio. we investigate scaling the deliberation text encoder and decoder and compare scaling the deliberation decoder and the first pass cascaded encoder. we show that deliberation improves the average wer on 9 languages by 4% relative compared to the single pass model. by increasing the size of the deliberation up to 1b parameters the average wer improvement increases to 9% with up to 14% for certain languages. our deliberation rescorer is based on transformer layers and can be parallelized during rescoring.", "Pub Date": "2023-01-27"}