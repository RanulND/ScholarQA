{"Title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models", "Doi": "10.1109/ICASSP48485.2024.10448495", "Authors": ["c. qiang", "h. li", "y. tian", "y. zhao", "y. zhang", "l. wang", "j. dang"], "Key Words": ["minimal supervision", "speech synthesis", "semantic coding", "diffusion model", "ctap"], "Abstract": "text to speech  tts  methods have shown promising results in voice cloning but they require a large number of labeled text speech pairs. minimally supervised speech synthesis decouples tts by combining two types of discrete speech representations semantic & acoustic  and using two sequence to sequence tasks to enable training with minimal supervision. however existing methods suffer from information redundancy and dimension explosion in semantic representation and high frequency waveform distortion in discrete acoustic representation. autoregressive frameworks exhibit typical instability and uncontrollability issues. and non autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. to address these issues we propose a minimally supervised high fidelity speech synthesis method where all modules are constructed based on the diffusion models. the non autoregressive framework enhances controllability and the duration diffusion model enables diversified prosodic expression. contrastive token acoustic pretraining  ctap  is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. mel spectrogram is used as the acoustic representation. both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high frequency fine grained waveform distortion. experimental results show that our proposed method outperforms the baseline method. we provide audio samples on our website.1", "Pub Date": "2024-03-18"}