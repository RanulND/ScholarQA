{"Title": "Impact of Code Language Models on Automated Program Repair", "Doi": "10.1109/ICSE48619.2023.00125", "Authors": ["n. jiang", "k. liu", "t. lutellier", "l. tan"], "Key Words": ["automated program repair", "code language model", "fine-tuning", "deep learning"], "Abstract": "automated program repair  automated program repair  aims to help developers improve software reliability by generating patches for buggy programs. although many code language models  clm  are developed and effective in many software tasks such as code completion there has been little comprehensive in depth work to evaluate clms' fixing capabilities and to fine tune clms for the automated program repair task. firstly this work is the first to evaluate ten clms on four automated program repair benchmarks which shows that surprisingly the best clm as is fixes 72% more bugs than the state of the art deep learning  dl  based automated program repair techniques. secondly one of the four automated program repair benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. thirdly it is the first work to fine tune clms with automated program repair training data which shows that fine tuning brings 31% 1267% improvement to clms and enables them to fix 46% 164 % more bugs than existing dl based automated program repair techniques. fourthly this work studies the impact of buggy lines showing that clms as is cannot make good use of the buggy lines to fix bugs yet fine tuned clms could potentially over rely on buggy lines. lastly this work analyzes the size time and memory efficiency of different clms. this work shows promising directions for the automated program repair domain such as fine tuning clms with automated program repair specific designs and also raises awareness of fair and comprehensive evaluations of clms and calls for more transparent reporting of open source repositories used in the pre training data to address the data leaking problem.", "Pub Date": "2023-07-14"}