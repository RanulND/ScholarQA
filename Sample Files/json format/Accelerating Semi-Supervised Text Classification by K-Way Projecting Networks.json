{"Title": "Accelerating Semi-Supervised Text Classification by K-Way Projecting Networks", "Doi": "10.1109/ACCESS.2023.3249214", "Authors": ["q. chen", "h. yang", "p. peng", "l. li"], "Key Words": ["knowledge distillation", "semi-supervised learning", "text classification", "projecting networks"], "Abstract": "the state of the art semi supervised learning framework has greatly shown its potential in making deep and complex language models such as bert highly effective for text classification tasks when labeled data is limited. however the large size and low inference speed of such models may hinder their application on resources limited or real time use cases. in this paper we propose a new approach in semi supervised learning framework to distill large complex teacher model into a fairly lightweight student model which has the ability of acquiring knowledge from different layers of teacher with the usage of  $k$  way projecting networks. across four english datasets in text classification benchmarks and one dataset collected from an chinese online course our experiment shows that this student model achieves comparable results with the state of the art transformer based semi supervised text classification methods while using only 0.156mb parameters and having an inference speed 785 times faster than the teacher model.", "Pub Date": "2023-03-02"}