{"Title": "How capable are state-of-the-art language models to cope with sarcasm?", "Doi": "10.1109/CSCS59211.2023.00069", "Authors": ["a. . -c. b\u0192\u00e9roiu", "\u2248\u00fb. tr\u0192\u00e9u\u2248\u00fcan-matu"], "Key Words": ["natural language processing", "sarcasm detection", "deep learning", "transformers", "large language models"], "Abstract": "sarcasm detection has established itself as one of the more difficult natural language processing tasks due to the complex nature of sarcasm. this paper aims to benchmark the performance of state of the art models like bert roberta albert and gpt-3 when faced with this task. the dataset selected is mustard which has increased in popularity in recent years especially for multimodal tasks and is one of the most qualitative and data rich dataset. an untuned gpt-3 based model was selected as the baseline and all the other models were fine tuned using the textual data present in mustard mainly the context and utterance information. the best performer was found to be the gpt-3 fine tuned model with an f1 score of 77. this is in line with the reported feats of gpt-3 based models that have popularized in recent months and reaffirms the superiority of gpt 3. future avenues of research are then presented and explored and the conclusions are drawn.", "Pub Date": "2023-08-17"}