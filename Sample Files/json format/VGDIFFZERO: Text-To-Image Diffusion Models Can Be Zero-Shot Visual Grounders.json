{"Title": "VGDIFFZERO: Text-To-Image Diffusion Models Can Be Zero-Shot Visual Grounders", "Doi": "10.1109/ICASSP48485.2024.10445945", "Authors": ["x. liu", "s. huang", "y. kang", "h. chen", "d. wang"], "Key Words": ["visual grounding", "diffusion models", "zero-shot learning", "vision-language models"], "Abstract": "large scale text to image diffusion models have shown impressive capabilities for generative tasks by leveraging strong vision language alignment from pre training. however most vision language discriminative tasks require extensive fine tuning on carefully labeled datasets to acquire such alignment with great cost in time and computing resources. in this work we explore directly applying a pre trained generative diffusion model to the challenging discriminative task of visual grounding without any fine tuning and additional training dataset. specifically we propose vgdiffzero a simple yet effective zero shot visual grounding framework based on text to image diffusion models. we also design a comprehensive region scoring method considering both global and local contexts of each isolated proposal. extensive experiments on refcoco refcoco+ and refcocog show that vgdiffzero achieves strong performance on zero shot visual grounding. our code is available at https //github.com xuyang liu16/vgdiffzero.", "Pub Date": "2024-03-18"}