{"Title": "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple-Choice Perspective", "Doi": "10.1109/ACCESS.2023.3343123", "Authors": ["j. wang", "p. yang", "r. gan", "y. zhang", "j. zhang", "t. sakai"], "Key Words": ["multi-task learning", "natural language understanding", "zero-shot learning"], "Abstract": "zero shot learning is an approach where models generalize to unseen tasks without direct training on them. we introduce the unified multiple choice  unimc  framework which is format independent compatible with various formats and applicable to tasks like text classification and sentiment analysis. furthermore we design a two stage tuning method initially training on multiple choice formats to develop format agnostic capabilities and subsequently enabling direct predictions on unseen tasks for zero shot learning. our methodology avoids issues in large scale models like flan enhancing generalization and reducing parameters. in experiments unimc shows state of the art  sota  performance across out of domain and in domain benchmarks with only 235m parameters far fewer than previous methods. moreover the unimc chinese model excels beyond human performance on benchmarks like eprstmt and chid fc underscoring its generalization capacity across languages. additionally ablation experiments demonstrate the effectiveness of our design. the code and model weights are available at https //github.com idea ccnl fengshenbang lm tree/main fengshen/examples unimc.", "Pub Date": "2023-12-22"}