{"Title": "Gradual Syntactic Label Replacement for Language Model Pre-Training", "Doi": "10.1109/TASLP.2023.3331096", "Authors": ["y. wang", "y. zhang", "p. li", "y. liu"], "Key Words": ["language model pre-training", "syntactic label replacement", "curriculum learning", "data-centric"], "Abstract": "pre training serves as a foundation of recent nlp models where language modeling tasks are performed over large texts. typical models like bert and gpt take the corpus as a whole and treat each word equally for language modeling. however recent works show that the naturally existing frequency bias in the raw corpus may limit the power of the language model. in this article we propose a multi stage training strategy that gradually increases the training vocabulary by modifying the training data. specifically we leverage the syntactic structure as a bridge for infrequent words and replace them with the corresponding syntactic labels then we recover their original lexical surface for further training. such strategy results in an easy to hard curriculum learning process where the model learns the most common words and some basic syntax concepts before recognizing a large number of uncommon words via their specific usages and the previously learned category knowledge. experimental results show that such a method can improve the performance of both discriminative and generative pre trained language models on benchmarks and various downstream tasks.", "Pub Date": "2023-11-27"}