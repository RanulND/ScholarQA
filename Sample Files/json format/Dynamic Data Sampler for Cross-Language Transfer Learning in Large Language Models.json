{"Title": "Dynamic Data Sampler for Cross-Language Transfer Learning in Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10446640", "Authors": ["y. li", "y. feng", "w. zhou", "z. zhao", "l. shen", "c. hou", "x. hou"], "Key Words": ["large language model", "cross-language", "knowledge transfer"], "Abstract": "large language models  large language model  have gained significant attention in the field of natural language processing  nlp  due to their wide range of applications. however training large language model for languages other than english poses significant challenges due to the difficulty in acquiring large scale corpus and the requisite computing resources. in this paper we propose chatflow a cross language transfer based large language model to address these challenges and train large chinese language models in a cost effective manner. we employ a mix of chinese english and parallel corpus to continuously train the llama2 model aiming to align cross language representations and facilitate the knowledge transfer specifically to the chinese language model. in addition we use a dynamic data sampler to progressively transition the model from unsupervised pre training to supervised fine tuning. experimental results demonstrate that our approach accelerates model convergence and achieves superior performance. we evaluate chatflow on popular chinese and english benchmarks the results indicate that it outperforms other chinese models post trained on llama 2 7b.", "Pub Date": "2024-03-18"}