{"Title": "Dhati: a Fine-tuned Large Language Model for evaluating Subjectivity in Arabic Textual Data", "Doi": "10.1109/PAIS60821.2023.10322022", "Authors": ["a. nehar", "s. bellaouar", "s. souffi", "m. bouameur"], "Key Words": ["arabic sentiment classification", "subjectivity", "large language models", "xlm-roberta", "transformers"], "Abstract": "despite being a linguistically rich and morphologically complex language arabic remains an under resourced language. the scarcity of large annotated datasets creates a challenge to providing accurate tools for many natural language processing  nlp  tasks such as subjectivity and sentiment analysis. the efficacy of text classification has been significantly enhanced for various languages including english and french due to the notable progress made in deep learning  dl  and transformers leading to the development of large language models. the aforementioned models have undergone pre training using extensive datasets followed by a process of fine tuning for targeted downstream tasks. in this paper we provide a tool which we call \"dhati\" for the evaluation of subjectivity in arabic textual data by fine tuning a large language model  xlm roberta  on the arabic sentiment tweets dataset  astd . then for comparison purposes we provide a parallel approach where we translate the arabic text into english and use two existing fine tuned models. the findings indicate that the dhati model has superior performance compared to the parallel approach as it achieves an accuracy rate of 82% in the arabic subjectivity classification task using the astd benchmark.", "Pub Date": "2023-11-22"}