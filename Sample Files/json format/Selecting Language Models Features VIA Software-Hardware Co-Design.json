{"Title": "Selecting Language Models Features VIA Software-Hardware Co-Design", "Doi": "10.1109/ICASSP49357.2023.10097191", "Authors": ["v. pandelea", "e. ragusa", "p. gastaldo", "e. cambria"], "Key Words": ["evaluation methodologies", "opinion mining / sentiment analysis", "language models", "edge computing"], "Abstract": "the availability of new datasets and deep learning techniques have led to a surge of effort directed towards the creation of new models that can exploit the large amount of data. however little attention has been given to the development of models that are not only accurate but also suitable for user specific use or geared towards resource constrained devices. fine tuning deep models on edge devices is impractical and often user customization stands on the sub optimal feature extractor classifier paradigm. here we propose a method to fully utilize the intermediate outputs of the popular large pre trained models in natural language processing when used as frozen feature extractors and further close the gap between their fine tuning and more computationally efficient solutions. we reach this goal exploiting the concept of software hardware co design and propose a methodical procedure inspired by neural architecture search to select the most desirable model taking into consideration application constraints.", "Pub Date": "2023-05-05"}