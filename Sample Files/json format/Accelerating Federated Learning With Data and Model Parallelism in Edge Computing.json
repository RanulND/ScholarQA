{"Title": "Accelerating Federated Learning With Data and Model Parallelism in Edge Computing", "Doi": "10.1109/TNET.2023.3299851", "Authors": ["y. liao", "y. xu", "h. xu", "z. yao", "l. wang", "c. qiao"], "Key Words": ["edge computing", "federated learning", "spilt learning", "system heterogeneity"], "Abstract": "recently edge ai has been launched to mine and discover valuable knowledge at network edge. federated learning as an emerging technique for edge ai has been widely deployed to collaboratively train models on many end devices in data parallel fashion. to alleviate the computation communication burden on the resource constrained workers  e.g. end devices  and protect user privacy spilt federated learning  sfl  which integrates both data parallelism and model parallelism in edge computing  ec  is becoming a practical and popular approach for model training over distributed data. however apart from the resource limitation sfl still faces two other critical challenges in ec i.e. system heterogeneity and context dynamics. to overcome these challenges we present an efficient sfl method named adasfl which controls both local updating frequency and batch size to better accelerate model training. we theoretically analyze the model convergence rate and obtain a convergence upper bound regarding local updating frequency given a fixed batch size. upon this we develop a control algorithm to determine adaptive local updating frequency and diverse batch sizes for heterogeneous workers to enhance the training efficiency. the experimental results show that adasfl can reduce the completion time by about 43% and the network traffic consumption by about 31% for achieving the similar test accuracy compared to the baselines.", "Pub Date": "2024-02-19"}