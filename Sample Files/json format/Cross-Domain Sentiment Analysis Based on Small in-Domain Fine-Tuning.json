{"Title": "Cross-Domain Sentiment Analysis Based on Small in-Domain Fine-Tuning", "Doi": "10.1109/ACCESS.2023.3269720", "Authors": ["a. v. kotelnikova", "s. v. vychegzhanin", "e. v. kotelnikov"], "Key Words": ["bert", "cross-domain models", "neural language models", "sentiment analysis"], "Abstract": "significant progress has been made in sentiment analysis over the past few years especially due to the application of deep neural language models. however there is a problem of transferability of trained models from one domain to another especially for less studied languages such as russian. we propose an approach to build cross domain sentiment analysis models based on a two stage procedure  first we fine tune a pre trained rubert language model on a combined non domain corpus and then fine tune this model on a small domain corpus. we conducted large scale experiments with 30 sentiment annotated corpora across 12 domains. in order to increase the representativeness of news texts with high quality annotation we created a novel runews corpus containing 1823 news articles annotated by sentiment. the results show that fine tuning the model using a small number  about several hundred  of annotated domain texts can significantly improve the performance of sentiment analysis for a new domain  on average by 4.6 p.p. . we also obtained the state of the art results for 7 out of 14 test corpora.", "Pub Date": "2023-05-01"}