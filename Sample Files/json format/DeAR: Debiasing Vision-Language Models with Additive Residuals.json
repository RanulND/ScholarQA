{"Title": "DeAR: Debiasing Vision-Language Models with Additive Residuals", "Doi": "10.1109/CVPR52729.2023.00659", "Authors": ["a. seth", "m. hemani", "c. agarwal"], "Key Words": ["transparency", "fairness", "accountability", "privacy", "ethics in vision"], "Abstract": "large pre trained vision language models  vlms  reduce the time for developing predictive models for various vision grounded language downstream tasks by providing rich adaptable image and text representations. however these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data. these biases manifest as the skewed similarity between the representations for specific text concepts and images of people of different identity groups and therefore limit the usefulness of such models in real world high stakes applications. in this work we present dear debiasing with additive residuals  a novel debiasing method that learns additive residual image representations to offset the original representations ensuring fair output representations. in doing so it reduces the ability of the representations to distinguish between the different identity groups. further we observe that the current fairness tests are performed on limited face image datasets that fail to indicate why a specific text concept should should not apply to them. to bridge this gap and better evaluate dearwe introduce the protected attribute tag association  pata dataset   a new context based bias benchmarking dataset for evaluating the fairness of large pre trained vlms. additionally pataprovides visual context for a diverse human population in different scenarios with both positive and negative connotations. experimental results for fairness and zero shot performance preservation using multiple datasets demonstrate the efficacy of our framework. the dataset is released here.", "Pub Date": "2023-08-22"}