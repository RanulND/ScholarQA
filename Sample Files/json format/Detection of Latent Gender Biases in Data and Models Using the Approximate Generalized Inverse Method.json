{"Title": "Detection of Latent Gender Biases in Data and Models Using the Approximate Generalized Inverse Method", "Doi": "10.1109/ICSC59802.2024.00036", "Authors": ["t. nakanishi"], "Key Words": ["gender bias", "approximate inverse model explanation", "language model", "bias detection", "embedding representation"], "Abstract": "gender bias creates inequalities in roles expectations and opportunities between males and females. when such biases are incorporated into artificial intelligence models the corresponding technological solutions and products can further entrench the social biases. herein a new method for investigating the extent to which latent biases in text based training data affect a language model is presented. potential gender bias is identified by deriving values assigned to male female words via inverse operations from embedded expressions to the original words using the approximate inverse model explanation  aime . in particular aime constructs approximate generalized inverse operators for black box models. a biased embedded representation used in machine learning models as an internal representation of word sentence vectors likely introduces bias into the overall prediction results of such models. the openai text embedding ada 002 large language model which provides embedded expressions is employed to determine the gender bias included in the proposed method. experimental results show that the openai textembedding ada 002 model is partially gender biased owing to the training text data. these results are expected to  i  contribute to the development of effective measures preventing gender bias during the design and training of language models  ii  promote the identification and mitigation of gender bias in future language models and  iii  provide insights into the effect of language models and their limitations from technical social and cultural perspectives.", "Pub Date": "2024-03-22"}