{"Title": "Performance Analysis of LoRA Finetuning Llama-2", "Doi": "10.1109/IEMENTech60402.2023.10423400", "Authors": ["a. pathak", "o. shree", "m. agarwal", "s. d. sarkar", "a. tiwary"], "Key Words": ["llms", "llama-2", "lora", "gpt-4", "hugging face"], "Abstract": "artificial intelligence  artificial intelliegence  has emerged as a transformative force particularly in the realm of large language models  large language model  which have long been in existence but recently gained substantial impact in our daily lives. our research endeavors focused on the exploration and open sourcing of llama 2 a significant large language model through fine tuning with the low rank adaptation  lora  technique. the satisfactory results obtained from the lora fine tuning of llama 2 have laid a foundation for further research in this domain. positioned as the premier open sourced model rivaling even gpt 4 llama 2 holds great promise for diverse research applications. its status as a transformer model coupled with refined hyperparameter tuning positions llama 2 as a pivotal tool for research and practical applications in the foreseeable future. leveraging a text summarization dataset our study demonstrated the enhanced performance of finetuned llama 2 suggesting its potential for broader applications and furthering its significance in cutting edge research. the dataset was imported from the hugging face space on which we tried to finetune the model. thus the fine tuned llama 2 is truly a force to be reckoned with.", "Pub Date": "2024-02-09"}