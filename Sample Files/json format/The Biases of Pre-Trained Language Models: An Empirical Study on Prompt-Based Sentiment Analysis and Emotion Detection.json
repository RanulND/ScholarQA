{"Title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection", "Doi": "10.1109/TAFFC.2022.3204972", "Authors": ["r. mao", "q. liu", "k. he", "w. li", "e. cambria"], "Key Words": ["emotion detection", "pre-trained language model", "prompt", "sentiment analysis"], "Abstract": "thanks to the breakthrough of large scale pre trained language model  plm  technology prompt based classification tasks e.g. sentiment analysis and emotion detection have raised increasing attention. such tasks are formalized as masked language prediction tasks which are in line with the pre training objects of most language models. thus one can use a plm to infer the masked words in a downstream task then obtaining label predictions with manually defined label word mapping templates. prompt based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. however there still remain many unclear issues related to the mechanisms of plms and prompt based classification. we conduct a systematic empirical study on prompt based sentiment analysis and emotion detection to study the biases of plms towards affective computing. we find that plms are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes emotional label word selections prompt templates and positions and the word forms of emotion lexicons.", "Pub Date": "2023-09-18"}