{"Title": "Lossless Neural Network Model Compression Through Exponent Sharing", "Doi": "10.1109/TVLSI.2023.3307607", "Authors": ["p. kashikar", "o. sentieys", "s. sinha"], "Key Words": ["convolutional neural network (cnn)", "fpga", "generalized matrix multiplication (gemm)", "model compression", "vivado hls"], "Abstract": "artificial intelligence  ai  on the edge has emerged as an important research area in the last decade to deploy different applications in the domains of computer vision and natural language processing on tiny devices. these devices have limited on chip memory and are battery powered. on the other hand neural network  nn  models require large memory to store model parameters and intermediate activation values. thus it is critical to make the models smaller so that their on chip memory requirements are reduced. various existing techniques like quantization and weight sharing reduce model sizes at the expense of some loss in accuracy. we propose a lossless technique of model size reduction by focusing on the sharing of exponents in weights which is different from the sharing of weights. we present results based on generalized matrix multiplication  gemm  in nn models. our method achieves at least a 20% reduction in memory when using bfloat16 and around 10% reduction when using ieee single precision floating point for models in general with a very small impact  up to 10% on the processor and less than 1% on fpga  on the execution time with no loss in accuracy. on specific models from hls4ml about 20% reduction in memory is observed in single precision with little execution overhead.", "Pub Date": "2023-10-17"}