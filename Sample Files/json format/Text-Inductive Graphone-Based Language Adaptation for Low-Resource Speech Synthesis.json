{"Title": "Text-Inductive Graphone-Based Language Adaptation for Low-Resource Speech Synthesis", "Doi": "10.1109/TASLP.2024.3369537", "Authors": ["t. saeki", "s. maiti", "x. li", "s. watanabe", "s. takamichi", "h. saruwatari"], "Key Words": ["multilingual text-to-speech", "low-resource adaptation", "adaptation of masked language model", "graphone"], "Abstract": "neural text to speech  tts  systems have made significant progress in generating natural synthetic speech. however neural tts requires large amounts of paired training data which limits its applicability to a small number of resource rich languages. previous work on low resource tts has addressed the data hungriness based on transfer learning from a multilingual model to low resource languages but it still relies heavily on the availability of paired data for the target languages. in this paper we propose a text inductive language adaptation framework for low resource tts to address the cost of collecting the paired data for low resource languages. to inject textual knowledge during transfer learning our framework employs a two stage adaptation scheme that utilizes both text only and supervised data for the target language. in the text based adaptation stage we update the language aware embedding layer with a masked language model objective using text only data for the target language. in the supervised adaptation stage the entire tts model is updated using paired data for the target language. we also propose a graphone based multilingual training method that jointly uses graphemes and international phonetic alphabet symbols  referred to as graphones  for resource rich languages while using only graphemes for low resource languages. this approach facilitates the transfer of pronunciation knowledge from resource rich to low resource languages. through extensive evaluations we demonstrate that 1  our framework with text based adaptation outperforms the previous supervised transfer learning approach and 2  the proposed graphone based training method further improves the performance of both multilingual tts and low resource language adaptation. with only 5 minutes of paired data for fine tuning our method achieved highly intelligible synthetic speech with the character error rates of around 6 % for a target language.", "Pub Date": "2024-03-14"}