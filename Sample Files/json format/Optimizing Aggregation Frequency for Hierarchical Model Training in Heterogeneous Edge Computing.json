{"Title": "Optimizing Aggregation Frequency for Hierarchical Model Training in Heterogeneous Edge Computing", "Doi": "10.1109/TMC.2022.3149584", "Authors": ["l. yang", "y. gan", "j. cao", "z. wang"], "Key Words": ["edge intelligence", "distributed machine learning", "aggregation frequency"], "Abstract": "federated learning  fl  has been widely used for distributed machine learning in edge computing. in fl the model parameters are iteratively aggregated from the clients to a central server which is inclined to be the communication bottleneck and single point of failure. to solve these drawbacks hierarchical model training frameworks like hierarchical federated learning  hfl  and e tree learning have been proposed. one of the most challenging problems in the hierarchical model training framework is optimizing the aggregation frequencies of the edge devices at various levels. because in an edge computing environment heterogeneity in the resource can introduce synchronization delays caused by waiting for slow workers and significantly impact the training performance. this paper tackles the problem with weak synchronization where edge devices on the same level have different frequencies on local updates and or model aggregations. existing works based on weak synchronization lack solutions to quantitatively determine the aggregation frequencies of each edge device. thus we propose a resource based aggregation frequency controlling method termed raf which determines the optimal aggregation frequencies of edge devices to minimize the loss function according to heterogeneous resources. our proposed method can alleviate the waiting time and fully utilize the resources of the edge devices. besides raf dynamically adjusts the aggregation frequencies at different phases during the model training to achieve fast convergence speed and high accuracy. we evaluated the performance of raf via extensive experiments with real datasets on our self developed edge computing testbed. evaluation results demonstrate that raf outperforms the benchmark approaches in terms of learning accuracy and convergence speed.", "Pub Date": "2023-06-05"}