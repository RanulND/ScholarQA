{"Title": "DNN Surgery: Accelerating DNN Inference on the Edge Through Layer Partitioning", "Doi": "10.1109/TCC.2023.3258982", "Authors": ["h. liang", "q. sang", "c. hu", "d. cheng", "x. zhou", "d. wang", "w. bao", "y. wang"], "Key Words": ["computation offloading", "deep neural networks", "edge computing", "inference acceleration", "layer partitioning"], "Abstract": "recent advances in deep neural networks have substantially improved the accuracy and speed of various intelligent applications. nevertheless one obstacle is that dnn inference imposes a heavy computation burden on end devices but offloading inference tasks to the cloud causes a large volume of data transmission. motivated by the fact that the data size of some intermediate dnn layers is significantly smaller than that of raw input data we designed the dnn surgery which allows partitioned dnn to be processed at both the edge and cloud while limiting the data transmission. the challenge is twofold   1  network dynamics substantially influence the performance of dnn partition and  2  state of the art dnns are characterized by a directed acyclic graph rather than a chain so that partition is incredibly complicated. to solve the issues we design a dynamic adaptive dnn surgery dads  scheme which optimally partitions the dnn under different network conditions. we also study the partition problem under the cost constrained system where the resource of the cloud for inference is limited. then a real world prototype based on the selif driving car video dataset is implemented showing that compared with current approaches dnn surgery can improve latency up to 6.45 times and improve throughput up to 8.31 times. we further evaluate dnn surgery through two case studies where we use dnn surgery to support an indoor intrusion detection application and a campus traffic monitor application and dnn surgery shows consistently high throughput and low latency.", "Pub Date": "2023-09-05"}