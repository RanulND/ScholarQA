{"Title": "A Survey of Knowledge Enhanced Pre-Trained Language Models", "Doi": "10.1109/TKDE.2023.3310002", "Authors": ["l. hu", "z. liu", "z. zhao", "l. hou", "l. nie", "j. li"], "Key Words": ["knowledge enhanced pre-trained language models", "natural language generation", "natural language processing", "natural language understanding", "pre-trained language models"], "Abstract": "pre trained language models  plms  which are trained on large text corpus via self supervised learning method have yielded promising performance on various tasks in natural language processing  nlp . however though plms with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine tuning stage they still have some limitations such as poor reasoning ability due to the lack of external knowledge. research has been dedicated to incorporating knowledge into plms to tackle these issues. in this paper we present a comprehensive review of knowledge enhanced pre trained language models  ke plms  to provide a clear insight into this thriving field. we introduce appropriate taxonomies respectively for natural language understanding  nlu  and natural language generation  nlg  to highlight these two main tasks of nlp. for nlu we divide the types of knowledge into four categories  linguistic knowledge text knowledge knowledge graph  kg  and rule knowledge. the ke plms for nlg are categorized into kg based and retrieval based methods. finally we point out some promising future directions of ke plms.", "Pub Date": "2024-03-07"}