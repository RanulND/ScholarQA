{"Title": "Domain Adaptation and Summary Distillation for Unsupervised Query Focused Summarization", "Doi": "10.1109/TKDE.2023.3296441", "Authors": ["j. du", "y. gao"], "Key Words": ["abstractive summarization", "domain adaptation", "query-focused summarization", "summary distillation", "unsupervised learning"], "Abstract": "text summarizing is the task of reducing a document length while maintaining its essential information. in the age of information explosion how to obtain the content that users needed from a large volume of information becomes particularly significant. under such circumstances query focused abstractive summarization  qfs  becomes more dominant since it is able to focus on user needs while delivering fluent concise succinct paraphrased summaries. however unlike generic summarization which has achieved remarkable progress driven by a substantial amount of parallel data the qfs struggles due to a deficiency of parallel corpus. therefore in this paper we leverage a typical large generic summarization dataset to facilitate the pressing demands on unsupervised qfs. the large scale query free benchmark is automatically transformed into a query focused dataset  query cnndm  while preserving its informative summaries. we propose a simple yet effective unsupervised method called domain adaptation and summary distillation method  dasd . in the model to achieve the domain adaptation for unsupervised qfs we design a query aware gap sentence generation  q gsg  strategy to equip the model with the capability of learning target textual knowledge and obtaining a good initialization at the target domain. as instance specific regularization we train a teacher model with the query cnndm to generate pseudo labels for summary distillation. experimental results indicate that our dasd model achieves state of the art performance on two benchmark datasets debatepedia and wikiref in a zero shot setting and shows good generalization to the abstractive few shot qfs.", "Pub Date": "2024-02-05"}