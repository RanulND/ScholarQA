{"Title": "Few-shot Learning with Prompting Methods", "Doi": "10.1109/IPRIA59240.2023.10147172", "Authors": ["m. bahrami", "m. mansoorizadeh", "h. khotanlou"], "Key Words": ["few-shot learning", "natural language processing", "prompting", "prompt-based learning", "pre-trained language model"], "Abstract": "today in natural language processing labeled data is important however getting adequate amount of data is a challenging step. there are many tasks for which it is difficult to obtain the required training data. for example in machine translation we need to prepare a lot of data in the target language so that the work performance is acceptable. we may not be able to collect useful data in the target language. hence we need to use few shot learning. recently a method called prompting has evolved in which text inputs are converted into text with a new structure using a certain format which has a blank space. given the prompted text a pre trained language model replaces the space with the best word. prompting can help us in the field of few shot learning  even in cases where there is no data i.e. zero shot learning. recent works use large language models such as gpt-2 and gpt 3 with the prompting method performed tasks such as machine translation. these efforts do not use any labeled training data. but these types of models with a massive number of parameters require powerful hardware. pattern exploiting training  pet  and iterative pattern exploiting training  ipet  were introduced which perform few shot learning using prompting and smaller pre trained language models such as bert and roberta. for example for the yahoo text classification dataset using ipet and roberta and ten labeled datasets 70% accuracy has been reached. this paper reviews research works in few shot learning with a new paradigm in natural language processing which we dub prompt based learning or in short prompting.", "Pub Date": "2023-06-14"}