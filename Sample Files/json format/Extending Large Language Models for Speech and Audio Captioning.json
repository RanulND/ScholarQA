{"Title": "Extending Large Language Models for Speech and Audio Captioning", "Doi": "10.1109/ICASSP48485.2024.10446343", "Authors": ["c. tang", "w. yu", "g. sun", "x. chen", "t. tan", "w. li", "l. lu", "z. ma", "c. zhang"], "Key Words": ["multimodal large language model", "automatic speech recognition", "audio captioning", "dual encoders", "q-former"], "Abstract": "multimodal large language models  large language model  have shown promising visual perception abilities by connecting with image encoders but their performance on auditory tasks has not yet been widely investigated. meanwhile automatic speech recognition  asr  and automatic audio captioning  aac  are often achieved with separate systems resulting in incomplete auditory perception abilities. to fill in these gaps in this paper we present the first study that achieves both asr and aac by connecting an large language model with auditory encoders. a dual auditory encoder structure is proposed integrating the whisper encoder for speech and the beats encoder for audio events with a high temporal resolution by using a q former at the window level. experiments for asr and aac are performed correspondingly on the widely used librispeech gigaspeech wavcaps audiocaps and clotho datasets and yield promising results. in particular state of the art results are achieved on gigaspeech audiocaps and clotho. our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events which is a step towards more complete machine auditory perception.", "Pub Date": "2024-03-18"}