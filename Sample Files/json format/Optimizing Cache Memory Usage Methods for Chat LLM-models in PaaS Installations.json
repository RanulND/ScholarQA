{"Title": "Optimizing Cache Memory Usage Methods for Chat LLM-models in PaaS Installations", "Doi": "10.1109/ElCon61730.2024.10468250", "Authors": ["m. m. rovnyagin", "d. m. sinelnikov", "a. a. eroshev", "t. a. rovnyagina", "a. v. tikhomirov"], "Key Words": ["chatgpt", "large language model", "transformer-type neural network", "context embedding", "memory allocation"], "Abstract": "recently llm models have become widespread in industry. they are used as the basis for voice assistants troubleshooting systems chatbots and much more. the work of llm is based on the architecture of transformer type neural networks where text is supplied as input  for example text chat  and the model estimating the character by character probability returns the result also in the form of text. in this case the model does not retrain. and the context itself is constantly accumulating. this paper proposes two ways to reduce the memory allocated for storing the test chat context. the first method is to periodically launch additional training in order to embed the chat context into the core of the model itself. the article discusses the pros and cons of this approach. the second method is to save in the text chat cache only with those users where this context has already been formed. the article describes the layout for conducting the experiment provides the results of the experimental study and describes the method for assessing the \u201a\u00e4\u00famaturity\u201a\u00e4\u00f9 of the chat correspondence context.", "Pub Date": "2024-03-20"}