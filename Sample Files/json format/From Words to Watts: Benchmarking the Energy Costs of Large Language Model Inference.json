{"Title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference", "Doi": "10.1109/HPEC58863.2023.10363447", "Authors": ["s. samsi", "d. zhao", "j. mcdonald", "b. li", "a. michaleas", "m. jones", "w. bergeron", "j. kepner", "d. tiwari", "v. gadepally"], "Key Words": ["large language models", "natural language processing", "inference", "green ai", "llm", "nlp", "deep learning", "distributed computing", "energy", "sustainability"], "Abstract": "large language models  llms  have exploded in popularity due to their new generative capabilities that go far beyond prior state of the art. these technologies are increasingly being leveraged in various domains such as law finance and medicine. however these models carry significant computational challenges especially the compute and energy costs required for inference. inference energy costs already receive less attention than the energy costs of training llms despite how often these large models are called on to conduct inference in reality  e.g. chatgpt . as these state of the art llms see increasing usage and deployment in various domains a better understanding of their resource utilization is crucial for cost savings scaling performance efficient hardware usage and optimal inference strategies. in this paper we describe experiments conducted to study the computational and energy utilization of inference with llms. we benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of llama a recent state of the art llm developed by meta ai on two generations of popular gpus  nvidia v100 & a100  and two datasets  alpaca and gsm8k  to reflect the diverse set of tasks benchmarks for llms in research and practice. we present the results of multi node multi gpu inference using model sharding across up to 32 gpus. to our knowledge our work is the one of the first to study llm inference performance from the perspective of computational and energy resources at this scale.", "Pub Date": "2023-12-25"}