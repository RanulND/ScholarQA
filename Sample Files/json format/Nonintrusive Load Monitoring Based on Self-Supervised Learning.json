{"Title": "Nonintrusive Load Monitoring Based on Self-Supervised Learning", "Authors": ["s. chen", "b. zhao", "m. zhong", "w. luan", "y. yu"], "Pub Date": "2023-03-03", "Abstract": "deep learning models for nonintrusive load monitoring  nilm  tend to require a large amount of labeled data for training. however it is difficult to generalize the trained models to unseen sites due to different load characteristics and operating patterns of appliances between datasets. for addressing such problems self supervised learning  ssl  is proposed in this article where labeled appliance level data from the target dataset or house are not required. initially only the aggregate power readings from target dataset are required to pretrain a general network via a self supervised pretext task to map aggregate power sequences to derived representatives. then supervised downstream tasks are carried out for each appliance category to fine tune the pretrained network where the features learned in the pretext task are transferred. utilizing labeled source datasets enables the downstream tasks to learn how each load is disaggregated by mapping the aggregate to labels. finally the fine tuned network is applied to load disaggregation for the target sites. for validation multiple experimental cases are designed based on three publicly accessible redd u.k. dale and refit datasets. besides the state of the art neural networks are employed to perform nilm task in the experiments. based on the nilm results in various cases ssl generally outperforms zero shot learning in improving load disaggregation performance without any submetering data from the target datasets.", "Doi": "10.1109/TIM.2023.3246504", "Key Words": ["deep neural network (dnn)", "nonintrusive load monitoring (nilm)", "self-supervised learning (ssl)", "sequence-to-point learning"]}