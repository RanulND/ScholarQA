{"Title": "SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly", "Doi": "10.1109/CGO57630.2024.10444788", "Authors": ["j. armengol-estap\u221a\u00a9", "j. woodruff", "c. cummins", "m. f. p. o'boyle"], "Key Words": ["decompilation", "neural decompilation", "transformer", "language models", "type inference"], "Abstract": "decompilation is a well studied area with numerous high quality tools available. these are frequently used for security tasks and to port legacy code. however they regularly generate difficult to read programs and require a large amount of engineering effort to support new programming languages and isas. recent interest in neural approaches has produced portable tools that generate readable code. nevertheless to date such techniques are usually restricted to synthetic programs without optimization and no models have evaluated their portability. furthermore while the code generated may be more readable it is usually incorrect. this paper presents slade a small language model decompiler based on a sequence to sequence transformer trained over real world code and augmented with a type inference engine. we utilize a novel tokenizer dropout free regularization and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. unlike standard approaches slade can infer out of context types and unlike neural approaches it generates correct code. we evaluate slade on over 4000 exebench functions on two isas and at two optimization levels. slade is up to 6\u221a\u00f3 more accurate than ghidra a state of the art industrial strength decompiler and up to 4\u221a\u00f3 more accurate than the large language model chatgpt and generates significantly more readable code than both.", "Pub Date": "2024-02-28"}