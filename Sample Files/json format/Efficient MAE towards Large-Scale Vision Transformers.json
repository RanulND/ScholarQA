{"Title": "Efficient MAE towards Large-Scale Vision Transformers", "Doi": "10.1109/WACV57701.2024.00066", "Authors": ["q. han", "g. zhang", "j. huang", "p. gao", "z. wei", "s. lu"], "Key Words": ["algorithms", "image recognition and understanding", "algorithms", "machine learning architectures", "formulations", "and algorithms"], "Abstract": "masked autoencoder  mae  has demonstrated superb pre training efficiency for vision transformer thanks to its partial input paradigm and high mask ratio  0.75 . however mae often suffers from severe performance drop under higher mask ratios which hinders its potential toward larger scale vision transformers. in this work we identify that the performance drop is largely attributed to the over dominance of difficult reconstruction targets as higher mask ratios lead to more sparse visible patches and fewer visual clues for reconstruction. to mitigate this issue we design efficient mae that introduces a novel difficulty flatten loss and a decoder masking strategy enabling a higher mask ratio for more efficient pre training. the difficulty flatten loss provides balanced supervision on reconstruction targets of different difficulties mitigating the performance drop under higher mask ratios effectively. additionally the decoder masking strategy discards the most difficult reconstruction targets which further alleviates the optimization difficulty and accelerates the pre training clearly. our proposed efficient mae introduces 27% and 30% pre training runtime accelerations for the vit large and vit huge models provides valuable insights into mae\u201a\u00e4\u00f4s optimization and paves the way for larger scale vision transformer pre training. code and pre trained models will be released.", "Pub Date": "2024-04-09"}