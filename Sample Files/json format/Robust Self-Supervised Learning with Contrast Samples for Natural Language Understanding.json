{"Title": "Robust Self-Supervised Learning with Contrast Samples for Natural Language Understanding", "Doi": "10.1109/ICASSP48485.2024.10448238", "Authors": ["j. liu", "x. han", "c. deng", "j. feng"], "Key Words": ["natural language processing", "large language models", "robustness", "contrastive learning"], "Abstract": "to improve the robustness of pre trained language models  plms  previous studies have focused more on how to efficiently obtain adversarial samples with similar semantics but less attention has been paid to the perturbed samples that change the gold label. therefore to fully perceive the effects of these different types of small perturbations on robustness we propose a robust self supervised learning  rosa  method which incorporates different types of perturbed samples and the robustness improvements into a unified framework. subsequently to implement rosa a perturbed sample generation strategy supported by the large language models  llms  is proposed which adaptively controls the generation process based on the fine grained similarity information among the training samples. the experimental results demonstrate the remarkable performance of our rosa.", "Pub Date": "2024-03-18"}