{"Title": "DNN Partitioning for Inference Throughput Acceleration at the Edge", "Doi": "10.1109/ACCESS.2023.3244497", "Authors": ["t. feltin", "l. march\u221a\u2265", "j. -a. cordero-fuertes", "f. brockners", "t. h. clausen"], "Key Words": ["distributed artificial intelligence", "edge computing", "scheduling and task partitioning"], "Abstract": "deep neural network  dnn  inference on streaming data requires computing resources to satisfy inference throughput requirements. however latency and privacy sensitive deep learning applications cannot afford to offload computation to remote clouds because of the implied transmission cost and lack of trust in third party cloud providers. among solutions to increase performance while keeping computation on a constrained environment hardware acceleration can be onerous and model optimization requires extensive design efforts while hindering accuracy. dnn partitioning is a third complementary approach and consists of distributing the inference workload over several available edge devices taking into account the edge network properties and the dnn structure with the objective of maximizing the inference throughput  number of inferences per second . this paper introduces a method to predict inference and transmission latencies for multi threaded distributed dnn deployments and defines an optimization process to maximize the inference throughput. a branch and bound solver is then presented and analyzed to quantify the achieved performance and complexity. this analysis has led to the definition of the acceleration region which describes deterministic conditions on the dnn and network properties under which dnn partitioning is beneficial. finally experimental results confirm the simulations and show inference throughput improvements in sample edge deployments.", "Pub Date": "2023-06-02"}