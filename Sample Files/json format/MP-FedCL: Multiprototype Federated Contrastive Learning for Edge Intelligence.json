{"Title": "MP-FedCL: Multiprototype Federated Contrastive Learning for Edge Intelligence", "Doi": "10.1109/JIOT.2023.3320250", "Authors": ["y. qiao", "m. s. munir", "a. adhikary", "h. q. le", "a. d. raha", "c. zhang", "c. s. hong"], "Key Words": ["communication efficiency", "contrastive learning", "edge intelligence", "federated learning (fl)", "global prototype pool", "label and feature not independent and identically distributed (non-iid)", "multiprototype"], "Abstract": "federated learning assisted edge intelligence enables privacy protection in modern intelligent services. however not independent and identically distributed  non iid  distribution among edge clients can impair the local model performance. the existing single prototype based strategy represents a class by using the mean of the feature space. however feature spaces are usually not clustered and a single prototype may not represent a class well. motivated by this this article proposes a multiprototype federated contrastive learning approach  mp fedcl  which demonstrates the effectiveness of using a multiprototype strategy over a single prototype under non iid settings including both label and feature skewness. specifically a multiprototype computation strategy based on k means is first proposed to capture different embedding representations for each class space using multiple prototypes  $ k$  centroids  to represent a class in the embedding space. in each global round the computed multiple prototypes and their respective model parameters are sent to the edge server for aggregation into a global prototype pool which is then sent back to all clients to guide their local training. finally local training for each client minimizes their own supervised learning tasks and learns from shared prototypes in the global prototype pool through supervised contrastive learning which encourages them to learn knowledge related to their own class from others and reduces the absorption of unrelated knowledge in each global iteration. experimental results on mnist digit 5 office 10 and domainnet show that our method outperforms multiple baselines with an average test accuracy improvement of about 4.6% and 10.4% under feature and label non iid distributions respectively.", "Pub Date": "2024-02-21"}