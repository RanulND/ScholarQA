{"Title": "Primer: Fast Private Transformer Inference on Encrypted Data", "Doi": "10.1109/DAC56929.2023.10247719", "Authors": ["m. zheng", "q. lou", "l. jiang"], "Key Words": ["fully homomorphic encryption", "multi-party computation", "transformer", "cryptographic protocol", "private inference"], "Abstract": "it is increasingly important to enable privacy preserving inference for cloud services based on transformers. post quantum cryptographic techniques e.g. fully homomorphic encryption  fhe  and multi party computation  mpc  are popular methods to support private transformer inference. however existing works still suffer from prohibitively computational and communicational overhead. in this work we present primer to enable a fast and accurate transformer over encrypted data for natural language processing tasks. in particular primer is constructed by a hybrid cryptographic protocol optimized for attention based transformer models as well as techniques including computation merge and tokens first ciphertext packing. comprehensive experiments on encrypted language modeling show that primer achieves state of the art accuracy and reduces the inference latency by 90.6% \u201a\u00e0\u00ba 97.5% over previous methods.", "Pub Date": "2023-09-15"}