{"Title": "ChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?", "Doi": "10.1109/APSEC60848.2023.00085", "Authors": ["m. fu", "c. k. tantithamthavorn", "v. nguyen", "t. le"], "Key Words": ["chatgpt", "large language models", "cybersecurity", "software vulnerability", "software security"], "Abstract": "large language models  llms  like chatgpt  i.e. gpt 3.5 turbo and gpt 4  exhibited remarkable advancement in a range of software engineering tasks associated with source code such as code review and code generation. in this paper we undertake a comprehensive study by instructing chatgpt for four prevalent vulnerability tasks  function and line level vulnerability prediction vulnerability classification severity estimation and vulnerability repair. we compare chatgpt with state of the art language models designed for software vulnerability purposes. through an empirical assessment employing extensive real world datasets featuring over 190000 c c++ functions we found that chatgpt achieves limited performance trailing behind other language models in vulnerability contexts by a significant margin. the experimental outcomes highlight the challenging nature of vulnerability prediction tasks requiring domain specific expertise. despite chatgpt substantial model scale exceeding that of source code pre trained language models  e.g. codebert  by a factor of 14000 the process of fine tuning remains imperative for chatgpt to generalize for vulnerability prediction tasks. we publish the studied dataset experimental prompts for chatgpt and experimental results at https //github.com awsm research chatgpt4vul.", "Pub Date": "2024-04-02"}