{"Title": "Dynamic Convolutional Neural Networks as Efficient Pre-Trained Audio Models", "Doi": "10.1109/TASLP.2024.3376984", "Authors": ["f. schmid", "k. koutini", "g. widmer"], "Key Words": ["dynamic convolutional neural networks", "dynamic convolution", "dynamic relu", "coordinate attention", "audio spectrogram transformer", "audio classification", "pre-trained audio models", "knowledge distillation"], "Abstract": "the introduction of large scale audio datasets such as audioset paved the way for transformers to conquer the audio domain and replace cnns as the state of the art neural network architecture for many tasks. audio spectrogram transformers are excellent at exploiting large datasets creating powerful pre trained models that surpass cnns when fine tuned on downstream tasks. however current popular audio spectrogram transformers are demanding in terms of computational complexity compared to cnns. recently we have shown that by employing transformer to cnn knowledge distillation efficient cnns can catch up with and even outperform transformers on large datasets. in this work we extend this line of research and increase the capacity of efficient cnns by introducing dynamic cnn blocks constructed of dynamic convolutions a dynamic relu activation function and coordinate attention. we show that these dynamic cnns outperform traditional efficient cnns such as mobilenets in terms of the performance\u201a\u00e4\u00eccomplexity trade off at the task of audio tagging on the large scale audioset. our experiments further indicate that the proposed dynamic cnns achieve competitive performance with transformer based models for end to end fine tuning on downstream tasks while being much more computationally efficient.", "Pub Date": "2024-04-08"}