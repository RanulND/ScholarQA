{"Title": "Freeze the Backbones: a Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-Training", "Doi": "10.1109/ICASSP48485.2024.10447326", "Authors": ["j. qin", "c. liu", "s. cheng", "y. guo", "r. arcucci"], "Key Words": ["vision-language pre-training", "selfsupervised learning", "medical visual representation learning"], "Abstract": "modern healthcare often utilises radiographic images alongside textual reports for diagnostics encouraging the use of vision language self supervised learning  vl ssl  with large pre trained models to learn versatile medical vision representations. however most existing vl ssl frameworks are trained end to end which is computation heavy and can lose vital prior information embedded in pre trained encoders. to address both issues we introduce the backbone agnostic adaptor framework which preserves medical knowledge in pre trained image and text encoders by keeping them frozen and employs a lightweight adaptor module for cross modal learning. experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre training approaches. notably when fine tuned with just 1% of data adaptor outperforms several transformer based methods trained on full datasets in medical image segmentation.", "Pub Date": "2024-03-18"}