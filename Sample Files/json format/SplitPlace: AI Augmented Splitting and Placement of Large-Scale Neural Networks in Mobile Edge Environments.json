{"Title": "SplitPlace: AI Augmented Splitting and Placement of Large-Scale Neural Networks in Mobile Edge Environments", "Doi": "10.1109/TMC.2022.3177569", "Authors": ["s. tuli", "g. casale", "n. r. jennings"], "Key Words": ["mobile edge computing", "neural network splitting", "container orchestration", "artificial intelligence", "qos optimization"], "Abstract": "in recent years deep learning models have become ubiquitous in industry and academia alike. deep neural networks can solve some of the most complex pattern recognition problems today but come with the price of massive compute and memory requirements. this makes the problem of deploying such large scale neural networks challenging in resource constrained mobile edge computing platforms specifically in mission critical domains like surveillance and healthcare. to solve this a promising solution is to split resource hungry neural networks into lightweight disjoint smaller components for pipelined distributed processing. at present there are two main approaches to do this  semantic and layer wise splitting. the former partitions a neural network into parallel disjoint models that produce a part of the result whereas the latter partitions into sequential models that produce intermediate results. however there is no intelligent algorithm that decides which splitting strategy to use and places such modular splits to edge nodes for optimal performance. to combat this this work proposes a novel ai driven online policy splitplace that uses multi armed bandits to intelligently decide between layer and semantic splitting strategies based on the input task service deadline demands. splitplace places such neural network split fragments on mobile edge devices using decision aware reinforcement learning for efficient and scalable computing. moreover splitplace fine tunes its placement engine to adapt to volatile environments. our experiments on physical mobile edge environments with real world workloads show that splitplace can significantly improve the state of the art in terms of average response time deadline violation rate inference accuracy and total reward by up to 46 69 3 and 12 percent respectively.", "Pub Date": "2023-08-04"}