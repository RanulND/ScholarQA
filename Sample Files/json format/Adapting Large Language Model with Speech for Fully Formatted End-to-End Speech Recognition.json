{"Title": "Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition", "Doi": "10.1109/ICASSP48485.2024.10448204", "Authors": ["s. ling", "y. hu", "s. qian", "g. ye", "y. qian", "y. gong", "e. lin", "m. zeng"], "Key Words": ["pretrained lm", "llm", "fully formatted e2e asr transcription"], "Abstract": "most end to end  e2e  speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. pretrained large language models  large language model  have the potential to improve the performance of e2e asr. however integrating a pretrained language model into an e2e speech recognition model has shown limited benefits due to the mismatches between text based large language model and those used in e2e asr. in this paper we explore an alternative approach by adapting a pretrained large language model to speech. our experiments on fully formatted e2e asr transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained large language model to produce more readable asr transcriptions. our model which is based on the pretrained large language models with either an encoder decoder or decoder only structure surpasses strong asr models such as whisper1 in terms of recognition error rate considering formats like punctuation and capitalization as well.", "Pub Date": "2024-03-18"}