{"Title": "Empowering Vision-Language Models for Reasoning Ability through Large Language Models", "Doi": "10.1109/ICASSP48485.2024.10446407", "Authors": ["y. yang", "x. zhang", "j. xu", "w. han"], "Key Words": ["vision-language model", "large language model", "reasoning"], "Abstract": "vision language models  vlm  have shown excellent performance in vision language tasks. however they sometimes lack sufficient reasoning ability. in contrast large language models  large language model  have emerged with powerful reasoning capabilities. therefore we propose a framework called tree which transfers the reasoning ability of the large language model to the vlm in learning free settings. tree is a three stage framework  observation thinking and re thinking. the observation stage requires the vlm to obtain overall visual information about the image. then the thinking stage combines the visual information and task description as the prompt for the large language model allowing it to present the thinking process  namely rationale . lastly the re thinking stage learns useful information from the rationale and then predicts the final result using the vlm. we are the first to explore enhancing the vlm\u201a\u00e4\u00f4s reasoning ability without any training finetuning or access to the large language model\u201a\u00e4\u00f4s parameters which we refer to as a plug in mode leading to the model agnostic feature. experiments show that tree performed well on general visual questionanswering  visual question answering  tasks and outperformed kosmos-1 on the challenging raven iq test dataset by 6%. furthermore with additional lightweight finetuning using a smaller amount of parameters tree achieved a high accuracy of 81.7% on gqa and 67.3% on vqav2.", "Pub Date": "2024-03-18"}