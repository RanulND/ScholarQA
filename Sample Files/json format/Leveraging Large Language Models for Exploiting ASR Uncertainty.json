{"Title": "Leveraging Large Language Models for Exploiting ASR Uncertainty", "Doi": "10.1109/ICASSP48485.2024.10446132", "Authors": ["p. dighe", "y. su", "s. zheng", "y. liu", "v. garg", "x. niu", "a. tewfik"], "Key Words": ["large language models", "prompting", "lora finetuning", "speech recognition", "intent detection", "keyword spotting"], "Abstract": "while large language models excel in a variety of natural language processing  nlp  tasks to perform well on spoken language understanding  slu  tasks they must either rely on off the shelf automatic speech recognition  asr  systems for transcription or be equipped with an in built speech modality. this work focuses on the former scenario where large language model\u201a\u00e4\u00f4s accuracy on slu tasks is constrained by the accuracy of a fixed asr system on the spoken input. specifically we tackle speech intent classification task where a high word error rate can limit the large language model\u201a\u00e4\u00f4s ability to understand the spoken intent. instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs we seek to answer how far we can go without substantially changing the underlying asr and large language model which can potentially be shared by multiple unrelated tasks. to this end we propose prompting the large language model with an n best list of asr hypotheses instead of only the error prone 1 best hypothesis. we explore prompt engineering to explain the concept of n best lists to the large language model  followed by the finetuning of low rank adapters  on the downstream tasks. our approach using n best lists proves to be effective on a device directed speech detection task as well as on a keyword spotting task where systems using n best list prompts outperform those using 1 best asr hypothesis  thus paving the way for an efficient method to exploit asr uncertainty via large language model for speech based applications.", "Pub Date": "2024-03-18"}