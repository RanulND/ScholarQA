{"Title": "Discriminative Speech Recognition Rescoring With Pre-Trained Language Models", "Doi": "10.1109/ASRU57964.2023.10389787", "Authors": ["p. g. shivakumar", "j. kolehmainen", "y. gu", "a. gandhe", "a. rastrow", "i. bulyko"], "Key Words": ["minimum word error rate", "discriminative training", "gpt-2", "bert", "asr rescoring"], "Abstract": "second pass rescoring is a critical component of competitive automatic speech recognition  asr  systems. large language models have demonstrated their ability in using pre trained information for better rescoring of asr hypothesis. discriminative training directly optimizing the minimum word error rate  mwer  criterion typically improves rescoring. in this study we propose and explore several discriminative fine tuning schemes for pre trained lms. we propose two architectures based on different pooling strategies of output embeddings and compare with probability based mwer. we conduct detailed comparisons between pre trained causal and bidirectional lms in discriminative settings. experiments on librispeech demonstrate that all mwer training schemes are beneficial giving additional gains upto 8.5% wer. proposed pooling variants achieve lower latency while retaining most improvements. finally our study concludes that bidirectionality is better utilized with discriminative training.", "Pub Date": "2024-01-19"}