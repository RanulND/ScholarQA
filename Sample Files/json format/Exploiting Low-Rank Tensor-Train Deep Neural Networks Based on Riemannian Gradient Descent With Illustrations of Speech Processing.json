{"Title": "Exploiting Low-Rank Tensor-Train Deep Neural Networks Based on Riemannian Gradient Descent With Illustrations of Speech Processing", "Doi": "10.1109/TASLP.2022.3231714", "Authors": ["j. qi", "c. -h. h. yang", "p. -y. chen", "j. tejedor"], "Key Words": ["tensor-train network", "speech enhancement", "spoken command recognition", "riemannian gradient descent", "low-rank tensor-train decomposition", "tensor-train deep neural network"], "Abstract": "this work focuses on designing low complexity hybrid tensor networks by considering trade offs between the model complexity and practical performance. firstly we exploit a low rank tensor train deep neural network  tt dnn  to build an end to end deep learning pipeline namely lr tt dnn. secondly a hybrid model combining lr tt dnn with a convolutional neural network  cnn  which is denoted as cnn+ lr tt dnn  is set up to boost the performance. instead of randomly assigning large tt ranks for tt dnn we leverage riemannian gradient descent to determine a tt dnn associated with small tt ranks. furthermore cnn+ lr tt dnn  consists of convolutional layers at the bottom for feature extraction and several tt layers at the top to solve regression and classification problems. we separately assess the lr tt dnn and cnn+ lr tt dnn  models on speech enhancement and spoken command recognition tasks. our empirical evidence demonstrates that the lr tt dnn and cnn+ lr tt dnn  models with fewer model parameters can outperform the tt dnn and cnn+ tt dnn  counterparts.", "Pub Date": "2023-01-04"}