{"Title": "Bit-Line Computing for CNN Accelerators Co-Design in Edge AI Inference", "Doi": "10.1109/TETC.2023.3237914", "Authors": ["m. rios", "f. ponzina", "a. levisse", "g. ansaloni", "d. atienza"], "Key Words": ["edge artificial intelligence", "in-memory computing", "hardware/software co-design", "convolutional neural networks", "low-power software optimization"], "Abstract": "by supporting the access of multiple memory words at the same time bit line computing  bc  architectures allow the parallel execution of bit wise operations in memory. at the array periphery arithmetic operations are then derived with little additional overhead. such a paradigm opens novel opportunities for artificial intelligence  artificial intelliegence  at the edge thanks to the massive parallelism inherent in memory arrays and the extreme energy efficiency of computing in situ hence avoiding data transfers. previous works have shown that bc brings disruptive efficiency gains when targeting artificial intelliegence workloads a key metric in the context of emerging edge artificial intelliegence scenarios. this manuscript builds on these findings by proposing an end to end framework that leverages bc specific optimizations to enable high parallelism and aggressive compression of artificial intelliegence models. our approach is supported by a novel hardware module performing real time decoding as well as new algorithms to enable bc friendly model compression. our hardware software approach results in a 91% energy savings  for a 1% accuracy degradation constraint  regarding state of the art bc computing approaches.", "Pub Date": "2023-06-06"}