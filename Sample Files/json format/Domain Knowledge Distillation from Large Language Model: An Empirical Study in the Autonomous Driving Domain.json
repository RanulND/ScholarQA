{"Title": "Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain", "Doi": "10.1109/ITSC57777.2023.10422308", "Authors": ["y. tang", "a. a. b. da costa", "x. zhang", "i. patrick", "s. khastgir", "p. jennings"], "Key Words": ["large language model", "domain ontology distillation", "autonomous driving"], "Abstract": "engineering knowledge based  or expert  systems require extensive manual effort and domain knowledge. as large language models  large language model  are trained using an enormous amount of cross domain knowledge it becomes possible to automate such engineering processes. this paper presents an empirical automation and semi automation framework for domain knowledge distillation using prompt engineering and the large language model chatgpt. we assess the framework empirically in the autonomous driving domain and present our key observations. in our implementation we construct the domain knowledge ontology by \u201a\u00e4\u00fachatting\u201a\u00e4\u00f9 with chatgpt. the key finding is that while fully automated domain ontology construction is possible human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. we therefore also develop a web based distillation assistant enabling supervision and flexible intervention at runtime. we hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge based systems across application domains.", "Pub Date": "2024-02-13"}