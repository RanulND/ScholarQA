{"Title": "Drinking From a Firehose: Continual Learning With Web-Scale Natural Language", "Doi": "10.1109/TPAMI.2022.3218265", "Authors": ["h. hu", "o. sener", "f. sha", "v. koltun"], "Key Words": ["continual learning", "lifelong learning", "personalized language modelling", "online multi-task learning", "web-scale datasets"], "Abstract": "continual learning systems will interact with humans with each other and with the physical world through time \u201a\u00e4\u00ec and continue to learn and adapt as they do. an important open problem for continual learning is a large scale benchmark which enables realistic evaluation of algorithms. in this paper we study a natural setting for continual learning on a massive scale. we introduce the problem of personalized online language learning  poll  which involves fitting personalized language models to a population of users that evolves over time. to facilitate research on poll we collect massive datasets of twitter posts. these datasets firehose10 m and firehose100 m comprise 100 million tweets posted by one million users over six years. enabled by the firehose datasets we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. based on this analysis we develop a simple algorithm for continual gradient descent  congrad  that outperforms prior continual learning methods on the firehose datasets as well as earlier benchmarks. collectively the poll problem setting the firehose datasets and the congrad algorithm enable a complete benchmark for reproducible research on web scale continual learning.", "Pub Date": "2023-04-03"}