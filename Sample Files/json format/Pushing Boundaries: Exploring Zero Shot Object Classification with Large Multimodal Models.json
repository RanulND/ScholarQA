{"Title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models", "Doi": "10.1109/SNAMS60348.2023.10375440", "Authors": ["a. islam", "m. r. biswas", "w. zaghouani", "s. b. belhaouari", "z. shah"], "Key Words": ["large language models", "large multimodal models", "prompt engineering", "classification"], "Abstract": "the synergy of language and vision models has given rise to large language and vision assistant models  llvas  designed to engage users in rich conversational experiences intertwined with image based queries. these comprehensive multimodal models seamlessly integrate vision encoders with large language models  large language model  expanding their applications in general purpose language and visual comprehension. the advent of large multimodal models  lmms  heralds a new era in artificial intelligence  artificial intelliegence  assistance extending the horizons of artificial intelliegence utilization. this paper takes a unique perspective on lmms exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. we also investigate the llvas zero shot learning capabilities. our study includes a benchmarking analysis across four diverse datasets  mnist cats vs. dogs hymnoptera  ants vs. bees  and an unconventional dataset comprising pox vs. non pox skin images. the results of our experiments demonstrate the model remarkable performance achieving classification accuracies of 85% 100% 77% and 79% for the respective datasets without any fine tuning. to bolster our analysis we assess the model performance post fine tuning for specific tasks. in one instance fine tuning is conducted over a dataset comprising images of faces of children with and without autism. prior to fine tuning the model demonstrated a test accuracy of 55% which significantly improved to 83% post fine tuning. these results coupled with our prior findings underscore the transformative potential of llvas and their versatile applications in real world scenarios.", "Pub Date": "2024-01-02"}