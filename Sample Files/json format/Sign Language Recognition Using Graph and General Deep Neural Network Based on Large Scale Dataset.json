{"Title": "Sign Language Recognition Using Graph and General Deep Neural Network Based on Large Scale Dataset", "Doi": "10.1109/ACCESS.2024.3372425", "Authors": ["a. s. m. miah", "m. a. m. hasan", "s. nishimura", "j. shin"], "Key Words": ["sign language recognition (slr)", "american sign language (asl)", "large scale dataset", "hand pose", "graph convolutional network (gcn)", "graph convolutional with attention and residual connection (gcar)", "deep learning network"], "Abstract": "sign language recognition  slr  represents a revolutionary technology aiming to establish communication between hearing impaired and non hearing impaired communities surpassing traditional interpreter based approaches. existing efforts in automatic sign recognition predominantly rely on hand skeleton joint information steering clear of image pixels to address challenges like partial occlusion and redundant backgrounds. many researchers have been working to develop automatic sign recognition using hand skeleton joint information instead of image pixels to overcome partial occlusion and redundant background problems. however body motion and facial expression play an essential role in increasing the inner gesture variance in expressing sign language emotion besides hand information for large scale sign word datasets. recently some researchers have been working to develop muti gesture based slr recognition systems but their performance accuracy and efficiency are unsatisfactory for real time deployment. addressing these limitations we propose a novel approach a two stream multistage graph convolution with attention and residual connection  gcar  designed to extract spatial temporal contextual information. the multistage gcar system incorporating a channel attention module dynamically enhances attention levels particularly for non connected skeleton points during specific events within spatial temporal features. the methodology involves capturing joint skeleton points and motion offering a comprehensive understanding of a person\u201a\u00e4\u00f4s entire body movement during sign language gestures and feeding this information into two streams. in the first stream joint key features undergo processing through sep tcn graph convolution deep learning layer and a channel attention module across multiple stages generating intricate spatial temporal features in sign language gestures. simultaneously the joint motion is processed in the second stream mirroring the steps of the first branch. the fusion of these two features yields the final feature vector which is then fed into the classification module. the model excels in capturing discriminative structural displacements and short range dependencies by leveraging unified joint features projected onto a high dimensional space. owing to the effectiveness of these features the proposed method achieved significant accuracies  90.31% 94.10% 99.75% and 34.41% for the wlasl psl msl and asllvd large scale datasets respectively with 0.69 million parameters. the high performance accuracy coupled with stable computational complexity demonstrates the superiority of the proposed model. this innovative approach is anticipated to redefine the landscape of sign language recognition setting a new standard in the field.", "Pub Date": "2024-03-08"}