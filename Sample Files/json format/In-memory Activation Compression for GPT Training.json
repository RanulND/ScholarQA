{"Title": "In-memory Activation Compression for GPT Training", "Doi": "10.1109/AICAS57966.2023.10168658", "Authors": ["s. lee", "g. yun", "h. -j. lee"], "Key Words": ["compression", "memory", "language model"], "Abstract": "recently a large number of parameters in transformer based language models have caused memory short ages during training. although solutions such as mixed precision and model parallelism have been proposed they have the limitation of inducing communication overhead and requiring modification of the model by a programmer. to address this issue we propose a scheme that compresses activation data in memory enabling the reduction of memory usage during training in a user transparent manner. the compression algorithm gathers activation data into a block and compresses it using base delta compression for the exponent and bit plane zero compression for the sign and mantissa. then the important bits are arranged in order and lsb truncation is applied to fit the target size. the proposed compression algorithm achieves a compression ratio of 2.09 for the sign 2.04 for the exponent and 1.21 for the mantissa. a compression ratio of 3.2 is obtained by applying up to the truncation and we confirm the convergence of gpt-2 training with the compression.", "Pub Date": "2023-07-07"}