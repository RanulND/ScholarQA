{"Title": "JoinER-BART: Joint Entity and Relation Extraction With Constrained Decoding, Representation Reuse and Fusion", "Doi": "10.1109/TASLP.2023.3310879", "Authors": ["h. chang", "h. xu", "j. van genabith", "d. xiong", "h. zan"], "Key Words": ["joint entity and relation extraction", "pre-trained bart", "sequence-to-sequence model"], "Abstract": "joint entity and relation extraction  jere  is an important research direction in information extraction  ie . given the surprising performance with fine tuning of pre trained bert in a wide range of nlp tasks nowadays most studies for jere are based on the bert model. rather than predicting a simple tag for each word these approaches are usually forced to design complex tagging schemes as they may have to extract entity relation pairs which may overlap with others from the same sequence of word representations in a sentence. recently sequence to sequence  seq2seq  pre trained bart models show better performance than bert models in many nlp tasks. importantly a seq2seq bart model can simply generate sequences of  many  entity relation triplets with its decoder rather than just tag input words. in this article we present a new generative jere framework based on pre trained bart. different from the basic seq2seq bart architecture  1  our framework employs a constrained classifier which only predicts either a token of the input sentence or a relation in each decoding step and 2  we reuse representations from the pre trained bart encoder in the classifier instead of a newly trained weight matrix as this better utilizes the knowledge of the pre trained model and context aware representations for classification and empirically leads to better performance. in our experiments on the widely studied nyt and webnlg datasets we show that our approach outperforms previous studies and establishes a new state of the art  92.91 and 91.37 f1 respectively in exact match evaluation .", "Pub Date": "2023-10-23"}