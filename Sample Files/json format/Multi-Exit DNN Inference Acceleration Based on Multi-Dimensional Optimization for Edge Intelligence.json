{"Title": "Multi-Exit DNN Inference Acceleration Based on Multi-Dimensional Optimization for Edge Intelligence", "Doi": "10.1109/TMC.2022.3172402", "Authors": ["f. dong", "h. wang", "d. shen", "z. huang", "q. he", "j. zhang", "l. wen", "t. zhang"], "Key Words": ["edge intelligence", "exit selection", "inference acceleration", "model partition", "multi -exit dnn", "resource allocation"], "Abstract": "edge intelligence as a prospective paradigm for accelerating dnn inference is mostly implemented by model partitioning which inevitably incurs the large transmission overhead of dnn\u201a\u00e4\u00f4s intermediate data. a popular solution introduces multi exit dnns to reduce latency by enabling early exits. however existing work ignores the correlation between exit settings and synergistic inference causing incoordination of device to edge. to address this issue this paper first investigates the bottlenecks of executing multi exit dnns in edge computing and builds a novel model for inference acceleration with exit selection model partition and resource allocation. to tackle the intractable coupling subproblems we propose a multi exit dnn inference acceleration framework based on multi dimensional optimization  mamo . in mamo the exit selection subproblem is first extracted from the original problem. then bidirectional dynamic programming is employed to determine the optimal exit setting for an arbitrary multi exit dnn. finally based on the optimal exit setting a drl based policy is developed to learn joint decisions of model partition and resource allocation. we deploy mamo on a real world testbed and evaluate its performance in various scenarios. extensive experiments show that it can adapt to heterogeneous tasks and dynamic networks and accelerate dnn inference by up to $13.7\\times$13.7\u221a\u00f3 compared with the state of the art.", "Pub Date": "2023-08-04"}