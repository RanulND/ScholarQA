{
    "index": 7,
    "Title": "Learning Video Representations from Large Language Models",
    "Doi": "10.1109/CVPR52729.2023.00637",
    "Authors": [
        "yue zhao",
        "ishan misra",
        "philipp kr\u00e4henb\u00fchl",
        "rohit girdhar"
    ],
    "Key Words": [
        "visualization",
        "computer vision",
        "computational modeling",
        "benchmark testing",
        "data models",
        "pattern recognition",
        "behavioral sciences",
        "video: action and event understanding"
    ],
    "Abstract": "we introduce lavila a new approach to learning video language representations by leveraging large language models  large language model . we repurpose pre trained large language model to be conditioned on visual input and finetune them to create automatic video narrators. our auto generated narrations offer a number of advantages including dense coverage of long videos better temporal synchronization of the visual information and text and much higher diversity of text. the video language embedding learned contrastively with these narrations outperforms the previous state of the art on multiple first person and third person video tasks both in zero shot and finetuned setups. most notably lavilaobtains an absolute gain of 10.1% on egtea classification and 5.9% epic kitchens 100 multi instance retrieval benchmarks. furthermore lavilatrained with only half the narrations from the ego4d dataset outperforms models trained on the full set and shows positive scaling behavior on increasing pre training data and model size.",
    "Pub Date": "2023-06-24"
}