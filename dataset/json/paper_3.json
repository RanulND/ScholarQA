{
    "Title": "Automated Program Repair in the Era of Large Pre-trained Language Models",
    "Doi": "10.1109/ICSE48619.2023.00129",
    "Authors": [
        "chunqiu steven xia",
        "yuxiang wei",
        "lingming zhang"
    ],
    "Key Words": [
        "codes",
        "computer bugs",
        "maintenance engineering",
        "software",
        "distance measurement",
        "task analysis",
        "faces",
        "automated program repair",
        "machine learning"
    ],
    "Abstract": "automated program repair (apr) aims to help developers automatically patch software bugs. however, current state-of-the-art traditional and learning-based apr techniques face the problem of limited patch variety, failing to fix complicated bugs. this is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). large pre-trained language models (llms), trained using billions of text/code tokens, can potentially help avoid this issue. very recently, researchers have directly leveraged llms for apr without relying on any bug-fixing datasets. meanwhile, such existing work either failed to include state-of-the-art llms or was not evaluated on realistic datasets. thus, the true power of modern llms on the important apr problem is yet to be revealed. in this work, we perform the first extensive study on directly applying llms for apr. we select 9 recent state-of-the-art llms, including both generative and infilling models, ranging from 125m to 20b in size. we designed 3 different repair settings to evaluate the different ways we can use llms to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. we apply the llms under these repair settings on 5 datasets across 3 different languages and compare different llms in the number of bugs fixed, generation speed and compilation rate. we also compare the llms against recent state-of-the-art apr tools. our study demonstrates that directly applying state-of-the-art llms can already substantially outperform all existing apr techniques on all our datasets. among the studied llms, the scaling effect exists for apr where larger models tend to achieve better performance. also, we show for the first time that suffix code after the buggy line (adopted in infilling-style apr) is important in not only generating more fixes but more patches with higher compilation rate. besides patch generation, the llms consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. lastly, we show that llm-based apr can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
    "Pub Date": "2023-05-20"
}