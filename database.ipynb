{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, \n",
    "# as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "import json\n",
    "import os\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk size, overlap and separators\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=10,\n",
    "    separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"allenai-specter\") # this is specialised for scientific articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON files from a folder\n",
    "folder_path = 'dataset/json'\n",
    "# store json files in a list\n",
    "json_files = [f\"{folder_path}/{file}\" for file in os.listdir(folder_path) if file.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty vector store\n",
    "vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_texts(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(json_file):\n",
    "    global vectorstore\n",
    "    \n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        # Extract relevant text data\n",
    "        title = data.get('Title', '')\n",
    "        authors = ', '.join(data.get('Authors', []))\n",
    "        abstract = data.get('Abstract', '')\n",
    "        keywords = ', '.join(data.get('Keywords', []))\n",
    "        pubdate = data.get('Pub Date', '')\n",
    "        # Concatenate text data\n",
    "        text = f\"{abstract} \"\n",
    "        meta = f\"Title: {title} | Authors: {authors} | Keywords: {keywords} | Pubdate:{pubdate}\"\n",
    "        # Split the text into documents\n",
    "        docs = text_splitter.split_text(text)\n",
    "        \n",
    "        # Load or create the embedding model\n",
    "        embedding_model = load_embedding_model(model_path='allenai-specter') #specialised for scientific research\n",
    "        \n",
    "        # If vectorstore is None, initialize it with the first set of embeddings\n",
    "        if vectorstore is None:\n",
    "            vectorstore = create_embeddings(docs, embedding_model)\n",
    "        else:\n",
    "            # Update the vectorstore with new embeddings\n",
    "            vectorstore.add_texts(docs, metadatas=[{}] * len(docs), embeddings_model=embedding_model)\n",
    "            \n",
    "        # Save the updated vectorstore\n",
    "        vectorstore.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_files is a list containing paths to JSON files\n",
    "for json_file in json_files:\n",
    "    process_json_file(json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scholarqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
