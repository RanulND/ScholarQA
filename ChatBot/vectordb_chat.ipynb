{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../Models/llama-7b-chat-q4/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.56 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n",
      "/Users/ranul/Projects/ScholarQA/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name ../Models/embedding-allenai-specter. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from Scripts.vectordb_chat import chat_with_vectordb\n",
    "retrieval_chain = chat_with_vectordb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2189.30 ms\n",
      "llama_print_timings:      sample time =      12.39 ms /   120 runs   (    0.10 ms per token,  9685.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28550.80 ms /   444 tokens (   64.30 ms per token,    15.55 tokens per second)\n",
      "llama_print_timings:        eval time =   11755.49 ms /   119 runs   (   98.79 ms per token,    10.12 tokens per second)\n",
      "llama_print_timings:       total time =   40548.13 ms /   563 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is a large language model', 'result': ' A large language model is a type of artificial intelligence model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-generated text. Large language models can be used for a variety of tasks, such as text generation, language translation, and language understanding. They have also been used to create automated program separators, which can generate code from natural language specifications of programmer intent.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a large language model\"\n",
    "answer = retrieval_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'query': 'What is a large language model', 'result': ' A large language model is a type of artificial intelligence model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-generated text. Large language models can be used for a variety of tasks, such as text generation, language translation, and language understanding. They have also been used to create automated program separators, which can generate code from natural language specifications of programmer intent.'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2189.30 ms\n",
      "llama_print_timings:      sample time =      29.19 ms /   279 runs   (    0.10 ms per token,  9559.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25326.50 ms /   398 tokens (   63.63 ms per token,    15.71 tokens per second)\n",
      "llama_print_timings:        eval time =   28700.61 ms /   278 runs   (  103.24 ms per token,     9.69 tokens per second)\n",
      "llama_print_timings:       total time =   54546.70 ms /   676 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the examples for large language models?', 'result': ' Large language models refer to artificial intelligence models that are trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. Some examples of large language models include:\\n* GPT-3: A popular large language model developed by Meta AI that has been used for a variety of tasks, such as text generation, language translation, and language understanding.\\n* Codex: A large language model developed by Google that can generate code from natural language specifications of programmer intent.\\n* Coogle: A large language model developed by Meta AI that can generate text from natural language inputs.\\n* BERT: A large language model developed by Google that has been used for a variety of natural language processing tasks, such as sentiment analysis, question answering, and text classification.\\n* RoBERTa: A variant of BERT that was specifically designed for text classification tasks and has achieved state-of-the-art results in many benchmarks.\\n* Longformer: A large language model that is specifically designed for long-form text generation tasks, such as generating articles or stories from prompts.\\n* ELECTRA: A large language model that is trained on a combination of text and image data to generate text that is visually coherent with the accompanying images.\\n\\n        </Answer>'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the examples for large language models?\"\n",
    "answer  = retrieval_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'query': 'What are the examples for large language models?', 'result': ' Large language models refer to artificial intelligence models that are trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. Some examples of large language models include:\\n* GPT-3: A popular large language model developed by Meta AI that has been used for a variety of tasks, such as text generation, language translation, and language understanding.\\n* Codex: A large language model developed by Google that can generate code from natural language specifications of programmer intent.\\n* Coogle: A large language model developed by Meta AI that can generate text from natural language inputs.\\n* BERT: A large language model developed by Google that has been used for a variety of natural language processing tasks, such as sentiment analysis, question answering, and text classification.\\n* RoBERTa: A variant of BERT that was specifically designed for text classification tasks and has achieved state-of-the-art results in many benchmarks.\\n* Longformer: A large language model that is specifically designed for long-form text generation tasks, such as generating articles or stories from prompts.\\n* ELECTRA: A large language model that is trained on a combination of text and image data to generate text that is visually coherent with the accompanying images.\\n\\n        </Answer>'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2189.30 ms\n",
      "llama_print_timings:      sample time =       0.53 ms /     5 runs   (    0.11 ms per token,  9433.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51161.16 ms /   788 tokens (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_print_timings:        eval time =     398.74 ms /     4 runs   (   99.68 ms per token,    10.03 tokens per second)\n",
      "llama_print_timings:       total time =   51676.85 ms /   792 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are we talking about?', 'result': '\\n\\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are we talking about?\"\n",
    "answer = retrieval_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'query': 'What are we talking about?', 'result': ' We are talking about large language models, which are artificial intelligence models that are trained on a large corpus of text data in order to generate language outputs that are coherent and natural-sounding. Examples of large language models include GPT-3, Codex, and Coogle Language Model, among others.'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2189.30 ms\n",
      "llama_print_timings:      sample time =      30.89 ms /   288 runs   (    0.11 ms per token,  9323.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47910.89 ms /   759 tokens (   63.12 ms per token,    15.84 tokens per second)\n",
      "llama_print_timings:        eval time =   30626.78 ms /   287 runs   (  106.71 ms per token,     9.37 tokens per second)\n",
      "llama_print_timings:       total time =   79146.02 ms /  1046 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the capabilities of large language models?', 'result': '1. Generate code from natural language specifications of programmer intent: Large language models such as GPT-3, Codex, and Coogle can generate code from natural language specifications of programmer intent, which can reduce the cost of specializing large language models using end-to-end finetuning and serve highly specialized large language models to end users.\\n    2. Outperform recent state-of-the-art automated program separation tools: Directly applying state-of-the-art large language models can already substantially outperform all existing automated program separation techniques on all datasets studied. Among the studied large language models, the scaling effect exists for automated program separation, where larger models tend to achieve better performance.\\n    3. Improve productivity: Large language models such as GPT-3, Codex, and Coogle have the potential to improve productivity by providing an automated artificial intelligence pair programmer for every programmer in the world.\\n    4. Generate text from natural language inputs: Large language models can generate text from natural language inputs, which can be useful in various applications such as chatbots, virtual assistants, and content generation.\\n    5. Be further substantially boosted: Large language model-based automated program separation can be further substantially boosted via increasing the sample size and incorporating fix template information.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the capabilities of large language models?\"\n",
    "answer = retrieval_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'query': 'What are the capabilities of large language models?', 'result': ' Large language models have various capabilities.  They can generate code from natural language specifications of programmer intent, serving as an automated AI pair programmer for every programmer in the world. They can also substantially outperform recent state-of-the-art automated program separation techniques. Large pre-trained models such as GPT-3, Codex, and Coogle Language Model are now capable of generating code from natural language specifications of programmer intent. However, we view these developments with a mixture of optimism and caution, recognizing both the potential benefits and limitations of these models. Directly applying these models can already outperform existing automated program separation techniques, but incorporating fix template information or increasing the sample size can further boost their performance.'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
