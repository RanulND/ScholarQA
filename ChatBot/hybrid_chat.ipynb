{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from Scripts.prompt import get_prompt_template_1\n",
    "from Scripts.chatbot_utils import load_llm,load_retriever\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../Models/llama-7b-chat-q4/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.56 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n",
      "/Users/ranul/Projects/ScholarQA/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name ../Models/embedding-allenai-specter. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# locainf the llm\n",
    "llm = load_llm()\n",
    "prompt = get_prompt_template_1()\n",
    "db = load_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_1 = db.get_relevant_documents(\"What are the capabilities of large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_2 = [\"generate action sequences\", 'score next actions', 'generate code from natural language specifications\\\n",
    "of programmer intent', \"improve productivity by providing an automated AI pair programmer for every program\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What are the capabilities of large language model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranul/Projects/ScholarQA/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =    3204.64 ms\n",
      "llama_print_timings:      sample time =      52.37 ms /   618 runs   (    0.08 ms per token, 11800.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66114.10 ms /  1008 tokens (   65.59 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:        eval time =   64164.90 ms /   618 runs   (  103.83 ms per token,     9.63 tokens per second)\n",
      "llama_print_timings:       total time =  131540.44 ms /  1626 tokens\n"
     ]
    }
   ],
   "source": [
    "hybrid_answer = chain.run(question=question,\n",
    "                              candidate_1=candidate_1,\n",
    "                              candidate_2=candidate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Based on the provided answer candidates, the capabilities of large language models can be summarized as follows:\\n1. Generating code from natural language specifications of programmer intent: Large language models such as GPT-3, Codex, and Coogle can generate code from natural language specifications of programmer intent, which can improve productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\n2. Automated program repair: Large language models can be used for automated program repair, which can substantially outperform recent state-of-the-art automated program separation techniques (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n3. Increasing sample size and incorporating fix template information can further boost automated program repair (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n4. Improving productivity: Large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\n5. Analytical models: Large language models can be used to build analytical models of code, which can help in understanding the semantics of code and in generating code from natural language specifications (Jigsaw, 2022).\\n6. Natural languages: Large language models can handle natural languages, which can be useful in generating code from natural language specifications (Jigsaw, 2022).\\n7. Building analysis: Large language models can be used for building analysis, which can help in understanding the distance between different parts of a building and in generating code for automated program repair (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n8. Syntactics: Large language models can handle syntactics, which can be useful in generating code from natural language specifications (Jigsaw, 2022).\\n9. Program synthesis: Large language models can be used for program synthesis, which can help in generating code from natural language specifications of programmer intent (Jigsaw, 2022).\\n10. Machine learning: Large language models can be used for machine learning, which can help in improving productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\nIn conclusion, large language models have various capabilities, including generating code from natural language specifications, automated program repair, improving productivity, building analysis, syntactics, program synthesis, and machine learning. These capabilities can be used in various applications, such as software development, maintenance engineering, and automated program repair.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'  Based on the provided answer candidates, the capabilities of large language models can be summarized as follows:\\n\n",
    "1. Generating code from natural language specifications of programmer intent: Large language models such as GPT-3, Codex, and Coogle can generate code from natural language specifications of programmer intent, which can improve productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\n\n",
    "2. Automated program repair: Large language models can be used for automated program repair, which can substantially outperform recent state-of-the-art automated program separation techniques (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n\n",
    "3. Increasing sample size and incorporating fix template information can further boost automated program repair (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n\n",
    "4. Improving productivity: Large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\n\n",
    "5. Analytical models: Large language models can be used to build analytical models of code, which can help in understanding the semantics of code and in generating code from natural language specifications (Jigsaw, 2022).\\n\n",
    "6. Natural languages: Large language models can handle natural languages, which can be useful in generating code from natural language specifications (Jigsaw, 2022).\\n\n",
    "7. Building analysis: Large language models can be used for building analysis, which can help in understanding the distance between different parts of a building and in generating code for automated program repair (Automated Program Repair in the Era of Large Pre-trained Language Models, 2023).\\n\n",
    "8. Syntactics: Large language models can handle syntactics, which can be useful in generating code from natural language specifications (Jigsaw, 2022).\\n\n",
    "9. Program synthesis: Large language models can be used for program synthesis, which can help in generating code from natural language specifications of programmer intent (Jigsaw, 2022).\\n\n",
    "10. Machine learning: Large language models can be used for machine learning, which can help in improving productivity by providing an automated AI pair programmer for every programmer in the world (Jigsaw, 2022).\\n\n",
    "In conclusion, large language models have various capabilities, including generating code from natural language specifications, automated program repair, improving productivity, building analysis, syntactics, program synthesis, and machine learning. These capabilities can be used in various applications, such as software development, maintenance engineering, and automated program repair.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
