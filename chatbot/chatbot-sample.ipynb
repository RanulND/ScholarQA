{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting replicate\n",
      "  Downloading replicate-0.25.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting httpx<1,>=0.21.0 (from replicate)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: packaging in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from replicate) (23.2)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from replicate) (2.6.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from replicate) (4.10.0)\n",
      "Requirement already satisfied: anyio in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/ranul/Projects/ScholarQA/.venv/lib/python3.12/site-packages (from pydantic>1.10.7->replicate) (2.16.3)\n",
      "Downloading replicate-0.25.1-py3-none-any.whl (39 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m836.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m974.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: httpcore, httpx, replicate\n",
      "Successfully installed httpcore-1.0.4 httpx-0.27.0 replicate-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "# REPLICATE_API_TOKEN=\"r8_4RRVc6sGnM2Mg0VmcFfzjhn1HWTbfjf02oztc\"\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  \"Document(page_content='. 3  it eliminates the need to specialize large language model using end to end finetuning and serve highly specialized large language model to end users thereby reducing cost.'), \\\n",
    " Document(page_content='. we also compare the large language model against recent state of the art automated program sepair tools. our study demonstrates that directly applying state of the art large language \\\n",
    "     model can already substantially outperform all existing automated program sepair techniques on all our datasets. among the studied large language model the scaling effect exists for automated program sepair \\\n",
    "         where larger models tend to achieve better performance'),\\\n",
    " Document(page_content='large pre trained language models such as gpt-3  codex  and coogle language model  are now capable of generating code from natural language specifications of programmer intent. \\\n",
    "     we view these developments with a mixture of optimism and caution. on the optimistic side such large language models have the potential to improve productivity by providing an automated artificial \\\n",
    "         intelliegence pair programmer for every programmer in the world', ),\\\n",
    " Document(page_content='. our experiments demonstrate that while the approach has promise  the large language model could collectively repair 100/% of our synthetically generated and hand crafted scenarios\\\n",
    "     a qualitative evaluation of the model performance over a corpus of historical real world examples highlights challenges in generating functionally correct code.')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(page_content='. 3  it eliminates the need to specialize large language model using end to end finetuning and serve highly specialized large language model to end users thereby reducing cost.'),  Document(page_content='. we also compare the large language model against recent state of the art automated program sepair tools. our study demonstrates that directly applying state of the art large language      model can already substantially outperform all existing automated program sepair techniques on all our datasets. among the studied large language model the scaling effect exists for automated program sepair          where larger models tend to achieve better performance'), Document(page_content='large pre trained language models such as gpt-3  codex  and coogle language model  are now capable of generating code from natural language specifications of programmer intent.      we view these developments with a mixture of optimism and caution. on the optimistic side such large language models have the potential to improve productivity by providing an automated artificial          intelliegence pair programmer for every programmer in the world', ), Document(page_content='. our experiments demonstrate that while the approach has promise  the large language model could collectively repair 100/% of our synthetically generated and hand crafted scenarios     a qualitative evaluation of the model performance over a corpus of historical real world examples highlights challenges in generating functionally correct code.')\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dialog = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. There will be two contexts as context_1 and context_2, you have the freedom to compare those two and provide the user with the most accurate and comprehensive answer \\\n",
    "which can be a combination of boith the context If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\\n\"\n",
    "\n",
    "context_1 = \"generate action sequences, score next actions, generate code from natural language specifications\\\n",
    "of programmer intent, improve productivity by providing an automated AI pair programmer for every program\"\n",
    "context_2 = y\n",
    "\n",
    "context_dialog += \"context_1 : \" + context_1\n",
    "context_dialog += \"context_2 : \" + context_2\n",
    "\n",
    "prompt_input = \"what are the capabilities of large language models?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Dialogs\n",
    "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. There will be two contexts as context_1 and context_2, you have the freedom to compare those two and provide the user with the most accurate and comprehensive answer \\\n",
    "which can be a combination of boith the context If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\\n\n",
    "\n",
    "-------\n",
    "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. You have the freedom to go through the provided context as context_1 and provide the user with the most accurate and comprehensive answer. \\\n",
    "If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\\n\n",
    "\n",
    "-------\n",
    "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using all the knowledge you can find.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = replicate.run(\n",
    "    \"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n",
    "    input={\n",
    "        \"prompt\": f\"{context_dialog} {prompt_input} Assistant: \",\n",
    "        \"temperature\":0.1, \n",
    "        \"top_p\":0.9, \n",
    "        \"max_length\":1024, \n",
    "        \"repetition_penalty\":1\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"\"\n",
    "for word in output:\n",
    "    out += word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, large language models have the capability to generate action sequences, score next actions, generate code from natural language specifications of programmer intent, and improve productivity by providing an automated AI pair programmer for every programmer in the world. Additionally, recent state-of-the-art automated program sepair tools have been compared to large language models, demonstrating that directly applying state-of-the-art large language models can already substantially outperform all existing automated program sepair techniques on all datasets. However, there are also challenges in generating functionally correct code, and further evaluation is needed to fully understand the capabilities of large language models in this regard.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Prediction.output_iterator at 0x147b027a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: \n",
    "    1) generate the entire patch function, \n",
    "    2) fill in a chunk of code given the prefix and suffix \n",
    "    3) output a single line fix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG + VDB Answer\n",
    "'Based on the provided documents, large language models such as GPT-3, Codex, and Coogle Language Model have several capabilities:  \n",
    "1. Generating code from natural language specifications of programmer intent. \n",
    "2. Improving productivity by providing an automated AI pair programmer for every programmer. \n",
    "3. Substantially outperforming all existing automated program sepair techniques on all datasets. \n",
    "4. Demonstrating a scaling effect for automated program sepair, where larger models tend to achieve better performance. \n",
    "However, the approach also has challenges in generating functionally correct code, as demonstrated in a qualitative evaluation of the model performance over a corpus of historical real-world examples.'\n",
    "\n",
    "'Based on the provided context, large language models have the capability to generate action sequences, score next actions, generate code from natural language specifications of programmer intent, and improve productivity by providing an automated AI pair programmer for every programmer in the world. Additionally, recent state-of-the-art automated program sepair tools have been compared to large language models, demonstrating that directly applying state-of-the-art large language models can already substantially outperform all existing automated program sepair techniques on all datasets. However, there are also challenges in generating functionally correct code, and further evaluation is needed to fully understand the capabilities of large language models in this regard.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG Only Answer\n",
    "No answer generate :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VDB only Answer\n",
    "\n",
    "'Based on the provided context, large language models such as GPT-3, Codex, and Coogle Language Model have the capability to generate code from natural language specifications of programmer intent. They have the potential to improve productivity by providing an automated artificial intelligence pair programmer for every programmer in the world. However, our experiments also demonstrate that the approach has challenges in generating functionally correct code, and a qualitative evaluation of the model performance over a corpus of historical real-world examples highlights these challenges.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Only\n",
    "'Large language models, such as transformer-based models like BERT, RoBERTa, and XLNet, have several capabilities that make them useful for a wide range of natural language processing tasks. Some of their key capabilities include:\\n\n",
    "1. Language Understanding: Large language models can understand natural language text and can perform tasks such as language translation, sentiment analysis, and text summarization.\\n\n",
    "2. Text Generation: Large language models can generate text that is coherent and contextually appropriate. They can be used for tasks such as text completion, language translation, and content creation.\\n\n",
    "3. Question Answering: Large language models can answer questions based on the information they have been trained on. They can be used for tasks such as customer service, trivia games, and information retrieval.\\n\n",
    "4. Dialogue Systems: Large language models can be used to build dialogue systems that can engage in conversation with humans. They can be used for tasks such as virtual assistants, chatbots, and voice assistants.\\n\n",
    "5. Text Classification: Large language models can classify text into categories such as spam/not spam, positive/negative sentiment, and topic categories.\\n\n",
    "6. Named Entity Recognition: Large language models can identify and classify named entities in text, such as people, organizations, and locations.\\n\n",
    "7. Part-of-Speech Tagging: Large language models can assign part-of-speech tags to words in text, such as noun, verb, adjective, etc.\\n\n",
    "8. Dependency Parsing: Large language models can analyze the grammatical structure of sentences and identify the relationships between words, such as subject-verb-object relationships.\\n\n",
    "9. Machine Translation: Large language models can be used to translate text from one language to another.\\n\n",
    "10. Summarization: Large language models can summarize long documents, extracting the most important information and condensing it into a shorter form.\\n\n",
    "These are just a few examples of the capabilities of large language models. They are highly versatile and can be used for a wide range of tasks, making them a powerful tool for natural language processing.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KG expected answer \n",
    "generate action sequences\n",
    "score next actions\n",
    "generate code from natural language specifications of programmer intent, \n",
    "improve productivity by providing an automated AI pair programmer for every program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain -> qa chain\n",
    "# conversational retrieval chains\n",
    "# History saving and using"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
